{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIDS W261 Machine Learning At Scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Christopher Llop | christopher.llop@ischool.berkeley.edu <br>\n",
    "Week 1 | Submission Date:\n",
    "\n",
    "<span style=\"color:red\">[Placeholder for introduction to assignment]</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:silver\"><b>HW1.0.0.</b> Define big data. Provide an example of a big data problem in your domain of expertise. </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\"><b>Answer:</b></span> In short, big data refers to problems using large or complex data sets that cannot quickly and easily be processed by a single machine using \"traditional\" methods of data-processing. There are several reasons why data could be not easy to process by traditional means. \n",
    "\n",
    "Many people talk about the 3 (or 4) V's: Velocity, Volume, Variety and Varacity. These lead to data challenges of processing, storage, or throughput that can be addressed by some \"big data\" techniques.\n",
    "\n",
    "In my current domain of economic and litigation consulting, most of our data processing can be handled by traditional means. However, we are starting to run into situations that push the boundaries of what we can do by traditional means. For example:\n",
    "- Quickly processing 104,000 Analyst Report text documents\n",
    "- Analyzing multiple TB of credit card transaction records at stake in litigation\n",
    "- Modeling complicated relationships between nodes in the electric system at an hourly level over decades\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:silver\"><b>HW1.0.1.</b> In 500 words (English or pseudo code or a combination) describe how to estimate the bias, the variance, the error for a test dataset T when using polynomial regression models of degree 1, 2,3, 4,5 are considered. How would you select a model?</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\"><b>Answer:</b></span>\n",
    "\n",
    "<b>Bias:</b> We can estimate the bais by taking bootstrap samples of our data (resampling), building many of each  polynomial model, and useing these models to calculate average predicted values for each type of model. By comparing average predicted values of data to the actual values, we can determine how off our model is on average. If the average prediction values are very different from the actual values, bais is high. If they are similar, bais is low.\n",
    "$$E[H(x^*)] - f(x^*)$$\n",
    "\n",
    "<b>Variance:</b> We can estimate the variances through a similar bootstrapping process. For each polynomial model, we can measure how consistent the predictions are from one bootstrap to the next. If the predictions for an example are all similar (tightly clustered), the variance is low. If they are dispersed, the variance is high. We can use the expected value formula for variance for the calculation: \n",
    "$$E[(h(x^*) - E[h(x^*)])^2]$$\n",
    "\n",
    "<b>Error:</b> The error can be evaluated by running our predictions on a held out test set, then evaluating how far off we are using a loss function such as squared prediction error. Because the model was not trained on this held out set, it will (likely) not be impacted by overfitting.  We can then compare the error of each polynomial and select the model with the least error on the held out test set. As shown in lecture and class readings, a similar result can be obtained by minimzing the $bais^2 + variance$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:silver\"><b>HW1.1.</b></style> Read through the provided control script (pNaiveBayes.sh) and all of its comments. When you are comfortable with their purpose and function, respond to the remaining homework questions below. A simple cell in the notebook with a print statmement with  a \"done\" string will suffice here. (dont forget to include the Question Number and the quesition in the cell as a multiline comment!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# HW 1.1: Read through pNaiveBayes and all comments to \n",
    "#         become comfortable with the code.\n",
    "\n",
    "def HW1_1():\n",
    "    print \"done\"\n",
    "\n",
    "HW1_1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:silver\"><b>HW1.2.</b> Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh will determine the number of occurrences of a single, user-specified word. Examine the word “assistance” and report your results.</span>\n",
    "\n",
    "   <span style=\"color:silver\">To do so, make sure that:</span>\n",
    "   - <span style=\"color:silver\">mapper.py counts all occurrences of a single word, and</span>\n",
    "   - <span style=\"color:silver\">reducer.py collates the counts of the single word.</span>\n",
    "   \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "\n",
    "# Get input parameters\n",
    "findword = sys.argv[2]\n",
    "findfile = sys.argv[1]\n",
    "term_hits = {}\n",
    "\n",
    "with open (findfile, \"r\") as myfile:\n",
    "    for full_email in myfile:\n",
    "        try:\n",
    "            # Spam classification\n",
    "            is_spam = re.findall(\"\\t([0-1])\\t\",full_email)[0]\n",
    "\n",
    "            # Parse out email body for processing. Find body using \"tab spam/ham tab\"\n",
    "            # use regex to strip out non alpha-numeric. \"don't\" will become \"dont\" which is fine.\n",
    "            keyword = re.findall(\"\\t[0-1]\\t\",full_email)[0]\n",
    "            email_id, is_spam_tabbed, email_body = full_email.partition(keyword)\n",
    "            email_body = re.sub('[^A-Za-z0-9\\s]+', '', email_body)\n",
    "            email_len = len(email_body.split())\n",
    "\n",
    "            # Number of hits for search word. Using a dictionary now will allow us to use\n",
    "            #    a similar Mapper output format when using multiple search terms later \n",
    "            #    in this problem set.\n",
    "            term_hits[findword] = len(re.findall(findword,email_body))\n",
    "            \n",
    "            # Print as tuple with unique splitter \"|||\"\n",
    "            print \"{} ||| {} ||| {}\".format(is_spam, email_len, term_hits)\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import ast\n",
    "\n",
    "# Get input parameters - list of file names\n",
    "filelist = sys.argv[1:]\n",
    "\n",
    "term_sum = {}\n",
    "\n",
    "# Open each map result\n",
    "for thisfile in filelist:\n",
    "    with open (thisfile, \"r\") as openfile:\n",
    "        # Process each emaul one at a time\n",
    "        for processed_email in openfile:\n",
    "            # Read data for email\n",
    "            processed_email = processed_email.split(\" ||| \")\n",
    "            count_dict = ast.literal_eval(processed_email[2])\n",
    "            \n",
    "            # Fold (sum) together the results from mapping\n",
    "            for key, value in count_dict.iteritems():\n",
    "                term_sum[key] = term_sum.get(key,0) + value\n",
    "\n",
    "# Print results\n",
    "for key, value in term_sum.iteritems():\n",
    "    print \"The word count for '{}' is {}\".format(key, value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use chmod for permissions - Only need to do this once per assignment since we're just overriding the file\n",
    "!chmod a+x mapper.py\n",
    "!chmod a+x reducer.py\n",
    "!chmod a+x pNaiveBayes.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: The word count for 'assistance' is 10\n",
      "\n",
      "Check Result: 10\n"
     ]
    }
   ],
   "source": [
    "# HW 1.2: Create map/reduce pair that determins occurances of a single word\n",
    "def HW1_2(word=\"assistance\"):\n",
    "    import re\n",
    "    \n",
    "    # Run pNaiveBayes.sh\n",
    "    !./pNaiveBayes.sh 10 {word}\n",
    "\n",
    "    # Print the output file contents to screen\n",
    "    with open (\"enronemail_1h.txt.output\", \"r\") as openfile:\n",
    "        print \"Result:\", openfile.read()\n",
    "\n",
    "    # Crosscheck results (data is small enough to use RE in python)\n",
    "    with open (\"enronemail_1h.txt\", \"r\") as myfile:\n",
    "        print \"Check Result:\", len(re.findall(word,myfile.read()))\n",
    "        \n",
    "HW1_2(word = \"assistance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:silver\"><b>HW1.3. </b>Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh\n",
    "   will classify the email messages by a single, user-specified word. Examine the word “assistance” and report your results. To do so, make sure that</span>\n",
    "   \n",
    "   - <span style=\"color:silver\">mapper.py is same as in part (2), and [Note - later instructions removed this criteria]</span>\n",
    "   - <span style=\"color:silver\">reducer.py performs a single word Naive Bayes classification.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "\n",
    "# Get input parameters\n",
    "findword = sys.argv[2]\n",
    "findfile = sys.argv[1]\n",
    "\n",
    "with open (findfile, \"r\") as myfile:\n",
    "    for full_email in myfile:\n",
    "        try:\n",
    "            # Empty dictionary\n",
    "            term_hits = {}\n",
    "            \n",
    "            # Spam classification\n",
    "            is_spam = re.findall(\"\\t([0-1])\\t\",full_email)[0]\n",
    "\n",
    "            # Parse out email body for processing. Find body using \"tab spam/ham tab\"\n",
    "            # use regex to strip out non alpha-numeric. \"don't\" will become \"dont\" which is fine for classifying.\n",
    "            keyword = re.findall(\"\\t[0-1]\\t\",full_email)[0]\n",
    "            email_id, is_spam_tabbed, email_body = full_email.partition(keyword)\n",
    "            email_body = re.sub('[^A-Za-z0-9\\s]+', '', email_body)\n",
    "            # Must process search query and email bodies the same\n",
    "            findword = re.sub('[^A-Za-z0-9\\s]+', '', findword)\n",
    "            email_len = len(email_body.split())\n",
    "\n",
    "            # Build counts of term words. Note, we need to get a count of distinct words\n",
    "            # regardless, so we might as well count all words here in preparation for the next problem 1.3.\n",
    "            # In an actual industry application, we would not store counts for all words since that would\n",
    "            # take up more space than required to solve the problem. Here, the focus is on learning MapReduce\n",
    "            # and Naive Bayes as opposed to this concern.\n",
    "            for word in list(set(email_body.split())):\n",
    "                term_hits[word] = len(re.findall(word,email_body))\n",
    "            \n",
    "            # Print as tuple with unique splitter \"|||\"\n",
    "            print \"{} ||| {} ||| {} ||| {} ||| {}\".format(re.findall(r'\\w+', findword), email_id, is_spam, email_len, term_hits)\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import ast\n",
    "import math\n",
    "\n",
    "filelist = sys.argv[1:]\n",
    "\n",
    "spam_term_counts = {}\n",
    "ham_term_counts = {}\n",
    "word_prob_given_spam = {}\n",
    "word_prob_given_ham = {}\n",
    "spam_count = 0\n",
    "ham_count = 0\n",
    "spam_len = 0\n",
    "ham_len = 0\n",
    "distinct_term_list = []\n",
    "\n",
    "# Open each file and build Multinomial Naive Bayes model\n",
    "for thisfile in filelist:\n",
    "    with open (thisfile, \"r\") as openfile:\n",
    "        for processed_email in openfile:\n",
    "            # Read in tuples created by mapper\n",
    "            processed_email = processed_email.split(\" ||| \")\n",
    "            search_terms = ast.literal_eval(processed_email[0])\n",
    "            email_id = processed_email[1]\n",
    "            is_spam = int(processed_email[2])\n",
    "            email_len = int(processed_email[3])\n",
    "            count_dict = ast.literal_eval(processed_email[4])\n",
    "            \n",
    "            # Build counts for spam and ham definitions.\n",
    "            if is_spam:\n",
    "                for key, value in count_dict.iteritems():\n",
    "                    spam_term_counts[key] = spam_term_counts.get(key, 0) + value\n",
    "                spam_count += 1\n",
    "                spam_len += email_len\n",
    "            else:\n",
    "                for key, value in count_dict.iteritems():\n",
    "                    ham_term_counts[key] = ham_term_counts.get(key, 0) + value\n",
    "                ham_count += 1\n",
    "                ham_len += email_len\n",
    "                \n",
    "            distinct_term_list = list(set(distinct_term_list + count_dict.keys()))\n",
    "\n",
    "# Calculate our priors based on the overall ratio of spam to ham\n",
    "spam_prior = float(spam_count) / (spam_count + ham_count)\n",
    "ham_prior = 1 - spam_prior\n",
    "spam_prior = math.log10(spam_prior)\n",
    "ham_prior = math.log10(ham_prior)\n",
    "\n",
    "# Calculate our conditional probabilites for the search term using MNB formula\n",
    "#     term_given_spam = (spam_term_count + 1 for smoothing) / (total count of spam words + total distinct vocab size)\n",
    "distinct_term_count = len(distinct_term_list)\n",
    "\n",
    "for term in search_terms:\n",
    "    word_prob_given_spam[term] = math.log10((spam_term_counts.get(term,0) + 1.0) / (float(spam_len) + distinct_term_count))\n",
    "    word_prob_given_ham[term] = math.log10((ham_term_counts.get(term,0) + 1.0) / (float(ham_len) + distinct_term_count))\n",
    "\n",
    "# Open each file and predict. Note - prediction is embaressingly parallel and could\n",
    "#   be done effectively via Mapping. However, the assignment asks us to solve the problem using\n",
    "#   the provided pNaiveBayes.sh.\n",
    "accuracy = []\n",
    "for thisfile in filelist:\n",
    "    with open (thisfile, \"r\") as openfile:\n",
    "        for processed_email in openfile:\n",
    "            # Defaults\n",
    "            pred_spam = 0\n",
    "            spam_prediction = spam_prior\n",
    "            ham_prediction = ham_prior\n",
    "\n",
    "            # Read in tuples created by mapper\n",
    "            processed_email = processed_email.split(\" ||| \")\n",
    "            email_id = processed_email[1]\n",
    "            is_spam = int(processed_email[2])\n",
    "            count_dict = ast.literal_eval(processed_email[4])\n",
    "            \n",
    "            # Read in counts to use in prediction\n",
    "            for term in word_prob_given_spam.keys():\n",
    "                # Calculate the probability for each class\n",
    "                spam_prediction += (word_prob_given_spam[term] * count_dict.get(term, 0))\n",
    "                ham_prediction += (word_prob_given_ham[term] * count_dict.get(term, 0))\n",
    "                \n",
    "            # Pick the higher probability\n",
    "            if spam_prediction > ham_prediction: \n",
    "                pred_spam = 1\n",
    "            \n",
    "            # Store accuracy in a list\n",
    "            accuracy.append(1*(pred_spam==is_spam))\n",
    "\n",
    "            # Print predictions to results file\n",
    "            print '{}\\t{}\\t{}'.format(email_id, is_spam, pred_spam)\n",
    "# Print accuracy\n",
    "sys.stderr.write(\"Spam Probs: {}\\n\".format(word_prob_given_spam))\n",
    "sys.stderr.write(\"Ham Probs: {}\\n\".format(word_prob_given_ham))\n",
    "sys.stderr.write(\"Accuracy = {:.2f}\\n\".format(float(sum(accuracy))/len(accuracy)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spam Probs: {'assistance': -3.4264028097516}\r\n",
      "Ham Probs: {'assistance': -3.799891684656865}\r\n",
      "Accuracy = 0.60\r\n"
     ]
    }
   ],
   "source": [
    "# HW 1.3: Create multinomial bayes map/reduce pair that predicts spam/ham using a single word\n",
    "def HW1_3(terms=\"assistance\"):\n",
    "    # Run pNaiveBayes.sh\n",
    "    !./pNaiveBayes.sh 10 \"{terms}\"\n",
    "        \n",
    "HW1_3(terms = \"assistance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:silver\"><b>HW1.4.</b> Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh\n",
    "   will classify the email messages by a list of one or more user-specified words. Examine the words “assistance”, “valium”, and “enlargementWithATypo” and report your results\n",
    "   To do so, make sure that</span>\n",
    " \n",
    "\n",
    "   - <span style=\"color:silver\">mapper.py counts all occurrences of a list of words, and</span>\n",
    "   - <span style=\"color:silver\">reducer.py performs the multiple-word Naive Bayes classification via the chosen list.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spam Probs: {'assistance': -3.4264028097516, 'enlargementWithATypo': -4.380645319190925, 'valium': -3.778585327862962}\r\n",
      "Ham Probs: {'assistance': -3.799891684656865, 'enlargementWithATypo': -4.277012939376528, 'valium': -4.277012939376528}\r\n",
      "Accuracy = 0.63\r\n"
     ]
    }
   ],
   "source": [
    "# HW 1.4: Our function for HW 1.3 can also classify multiple words, as requested by problem 1.4\n",
    "def HW1_4(words = \"assistance valium enlargementWithATypo\"):\n",
    "    # Run pNaiveBayes.sh\n",
    "    !./pNaiveBayes.sh 10 \"{words}\"\n",
    "            \n",
    "HW1_4(words = \"assistance valium enlargementWithATypo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:silver\"><b>HW1.5.</b> Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh will classify the email messages by all words present. To do so, make sure that</span>\n",
    "\n",
    "   - <span style=\"color:silver\">mapper.py counts all occurrences of all words, and</span>\n",
    "   - <span style=\"color:silver\">reducer.py performs a word-distribution-wide Naive Bayes classification.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "\n",
    "# Get input parameters\n",
    "findword = sys.argv[2]\n",
    "findfile = sys.argv[1]\n",
    "\n",
    "star_switch = 0\n",
    "if findword == \"*\":\n",
    "    star_switch = 1\n",
    "\n",
    "with open (findfile, \"r\") as myfile:\n",
    "    for full_email in myfile:\n",
    "        try:\n",
    "            # Empty dictionary\n",
    "            term_hits = {}\n",
    "\n",
    "            # Spam classification\n",
    "            is_spam = re.findall(\"\\t([0-1])\\t\",full_email)[0]\n",
    "\n",
    "            # Parse out email body for processing. Find body using \"tab spam/ham tab\"\n",
    "            # use regex to strip out non alpha-numeric. \"don't\" will become \"dont\" which is fine for classifying.\n",
    "            keyword = re.findall(\"\\t[0-1]\\t\",full_email)[0]\n",
    "            email_id, is_spam_tabbed, email_body = full_email.partition(keyword)\n",
    "            email_body = re.sub('[^A-Za-z0-9\\s]+', '', email_body)\n",
    "            # Must process search query and email bodies the same\n",
    "            # This is a good place to add logic to use all terms in body as search terms when \"*\" appears\n",
    "            if star_switch == 1:\n",
    "                findword = \" \".join(list(set(email_body.split())))\n",
    "            else:\n",
    "                findword = re.sub('[^A-Za-z0-9\\s]+', '', findword)\n",
    "            email_len = len(email_body.split())\n",
    "\n",
    "            # Build counts of term words.\n",
    "            for word in list(set(email_body.split())):\n",
    "                term_hits[word] = len(re.findall(word,email_body))\n",
    "\n",
    "            # Print as tuple with unique splitter \"|||\"\n",
    "            print \"{} ||| {} ||| {} ||| {} ||| {}\".format(re.findall(r'\\w+', findword), email_id, is_spam, email_len, term_hits)\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import ast\n",
    "import math\n",
    "\n",
    "filelist = sys.argv[1:]\n",
    "\n",
    "spam_term_counts = {}\n",
    "ham_term_counts = {}\n",
    "word_prob_given_spam = {}\n",
    "word_prob_given_ham = {}\n",
    "spam_count = 0\n",
    "ham_count = 0\n",
    "spam_len = 0\n",
    "ham_len = 0\n",
    "distinct_term_list = []\n",
    "\n",
    "# Open each file and build Multinomial Naive Bayes model\n",
    "for thisfile in filelist:\n",
    "    with open (thisfile, \"r\") as openfile:\n",
    "        for processed_email in openfile:\n",
    "            # Read in tuples created by mapper\n",
    "            processed_email = processed_email.split(\" ||| \")\n",
    "            search_terms = ast.literal_eval(processed_email[0])\n",
    "            email_id = processed_email[1]\n",
    "            is_spam = int(processed_email[2])\n",
    "            email_len = int(processed_email[3])\n",
    "            count_dict = ast.literal_eval(processed_email[4])\n",
    "            \n",
    "            # Build counts for spam and ham definitions.\n",
    "            if is_spam:\n",
    "                for key, value in count_dict.iteritems():\n",
    "                    spam_term_counts[key] = spam_term_counts.get(key, 0) + value\n",
    "                spam_count += 1\n",
    "                spam_len += email_len\n",
    "            else:\n",
    "                for key, value in count_dict.iteritems():\n",
    "                    ham_term_counts[key] = ham_term_counts.get(key, 0) + value\n",
    "                ham_count += 1\n",
    "                ham_len += email_len\n",
    "                \n",
    "            distinct_term_list = list(set(distinct_term_list + count_dict.keys()))\n",
    "\n",
    "# Calculate our priors based on the overall ratio of spam to ham\n",
    "spam_prior = float(spam_count) / (spam_count + ham_count)\n",
    "ham_prior = 1 - spam_prior\n",
    "spam_prior = math.log10(spam_prior)\n",
    "ham_prior = math.log10(ham_prior)\n",
    "\n",
    "# Calculate our conditional probabilites for the search term using MNB formula\n",
    "#     term_given_spam = (spam_term_count + 1 for smoothing) / (total count of spam words + total distinct vocab size)\n",
    "distinct_term_count = len(distinct_term_list)\n",
    "\n",
    "for term in distinct_term_list:\n",
    "    word_prob_given_spam[term] = math.log10((spam_term_counts.get(term,0) + 1.0) / (float(spam_len) + distinct_term_count))\n",
    "    word_prob_given_ham[term] = math.log10((ham_term_counts.get(term,0) + 1.0) / (float(ham_len) + distinct_term_count))\n",
    "\n",
    "# Open each file and predict. Note - prediction is embaressingly parallel and could\n",
    "#   be done effectively via Mapping. However, the assignment asks us to solve the problem using\n",
    "#   the provided pNaiveBayes.sh.\n",
    "accuracy = []\n",
    "for thisfile in filelist:\n",
    "    with open (thisfile, \"r\") as openfile:\n",
    "        for processed_email in openfile:\n",
    "            # Defaults\n",
    "            pred_spam = 0\n",
    "            spam_prediction = spam_prior\n",
    "            ham_prediction = ham_prior\n",
    "\n",
    "            # Read in tuples created by mapper\n",
    "            processed_email = processed_email.split(\" ||| \")\n",
    "            email_id = processed_email[1]\n",
    "            is_spam = int(processed_email[2])\n",
    "            count_dict = ast.literal_eval(processed_email[4])\n",
    "            \n",
    "            # Read in counts to use in prediction\n",
    "            for term in distinct_term_list:\n",
    "                # Calculate the probability for each class\n",
    "                spam_prediction += (word_prob_given_spam[term] * count_dict.get(term, 0))\n",
    "                ham_prediction += (word_prob_given_ham[term] * count_dict.get(term, 0))\n",
    "                \n",
    "            # Pick the higher probability\n",
    "            if spam_prediction > ham_prediction: \n",
    "                pred_spam = 1\n",
    "            \n",
    "            # Store accuracy in a list\n",
    "            accuracy.append(1*(pred_spam==is_spam))\n",
    "\n",
    "            # Print predictions to results file\n",
    "            print '{}\\t{}\\t{}'.format(email_id, is_spam, pred_spam)\n",
    "# Print accuracy\n",
    "sys.stderr.write(\"Accuracy = {:.2f}\\n\".format(float(sum(accuracy))/len(accuracy)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.96\r\n"
     ]
    }
   ],
   "source": [
    "# HW 1.5: Run on all words. \n",
    "def HW1_5():\n",
    "    # Run pNaiveBayes.sh\n",
    "    !./pNaiveBayes.sh 10 \"*\"\n",
    "            \n",
    "HW1_5()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "\n",
    "# Get input parameters\n",
    "findwords = sys.argv[2].split()\n",
    "findfile = sys.argv[1]\n",
    "term_hits = {}\n",
    "\n",
    "if findwords[0] == \"*\":\n",
    "    with open (findfile, \"r\") as myfile:\n",
    "        for full_email in myfile:\n",
    "            try:\n",
    "                # Spam classification\n",
    "                is_spam = re.findall(\"\\t([0-1])\\t\",full_email)[0]\n",
    "\n",
    "                # Parse out email body for processing. Find body using \"tab spam/ham tab\"\n",
    "                # use regex to strip out non alpha-numeric. \"don't\" will become \"dont\" which is fine.\n",
    "                keyword = re.findall(\"\\t[0-1]\\t\",full_email)[0]\n",
    "                email_id, is_spam_tabbed, email_body = full_email.partition(keyword)\n",
    "                email_body = re.sub('[^A-Za-z0-9\\s]+', '', email_body)\n",
    "                email_len = len(email_body.split())\n",
    "\n",
    "                # Number of hits for search word, stored in dictionary\n",
    "                for word in list(set(email_body.split())):\n",
    "                    term_hits[word] = len(re.findall(word,email_body))\n",
    "\n",
    "\n",
    "                # Print as tuple with unique splitter \"|||\"\n",
    "                print \"{} ||| {} ||| {}\".format(is_spam, email_len, term_hits)\n",
    "\n",
    "            except:\n",
    "                pass\n",
    "else:\n",
    "    print \"Invalid input\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import ast\n",
    "import math\n",
    "\n",
    "filelist = sys.argv[1:]\n",
    "\n",
    "spam_term_counts = {}\n",
    "ham_term_counts = {}\n",
    "spam_count = 0\n",
    "ham_count = 0\n",
    "spam_len = 0\n",
    "ham_len = 0\n",
    "\n",
    "# Open each file and build Multinomial Naive Bayes model\n",
    "for thisfile in filelist:\n",
    "    with open (thisfile, \"r\") as openfile:\n",
    "        for processed_email in openfile:\n",
    "            # Read in tuples created by mapper\n",
    "            processed_email = processed_email.split(\" ||| \")\n",
    "            is_spam = int(processed_email[0])\n",
    "            email_len = int(processed_email[1])\n",
    "            count_dict = ast.literal_eval(processed_email[2])\n",
    "            \n",
    "            # Build counts for spam and ham definitions.\n",
    "            if is_spam:\n",
    "                for key, value in count_dict.iteritems():\n",
    "                    spam_term_counts[key] = spam_term_counts.get(key, 0) + value\n",
    "                spam_count += 1\n",
    "                spam_len += email_len\n",
    "            else:\n",
    "                for key, value in count_dict.iteritems():\n",
    "                    ham_term_counts[key] = ham_term_counts.get(key, 0) + value\n",
    "                ham_count += 1\n",
    "                ham_len += email_len\n",
    "    \n",
    "# Calculate our priors based on the overall ratio of spam to ham\n",
    "spam_prior = float(spam_count) / (spam_count + ham_count)\n",
    "ham_prior = 1 - spam_prior\n",
    "\n",
    "# Get full vocabulary\n",
    "vocab_full = sorted(list(set(spam_term_counts.keys() + ham_term_counts.keys())))\n",
    "vocab_len = len(vocab_full)\n",
    "\n",
    "# Store probabilites for each term in the full vocabulary in dictionary. \n",
    "#    Calculate with MNB formulas.\n",
    "probs_given_spam = {}\n",
    "probs_given_ham = {}\n",
    "for key in vocab_full:\n",
    "    # Probability of term | Spam\n",
    "    spam_term_count = spam_term_counts.get(key, 0)\n",
    "    probs_given_spam[key] = (spam_term_count + 1.0) / (spam_len + vocab_len)\n",
    "    \n",
    "    # Probability of term | Ham\n",
    "    ham_term_count = ham_term_counts.get(key, 0)\n",
    "    probs_given_ham[key] = (ham_term_count + 1.0) / (ham_len + vocab_len)\n",
    "\n",
    "print spam_len\n",
    "print ham_len\n",
    "print vocab_len\n",
    "print\n",
    "print spam_prior\n",
    "print ham_prior\n",
    "print math.log(spam_prior)\n",
    "print math.log(ham_prior)\n",
    "\n",
    "# Open each file and predict. Note - prediction is embaressingly parallel and could\n",
    "#   be done effectively via Mapping. However, we were asked not to modify pNaiveBayes.sh\n",
    "#   as part of this assignment. As a result, I will predict here in the reducer although\n",
    "#   it is not the most efficient method.\n",
    "\n",
    "accuracy = []\n",
    "\n",
    "for thisfile in filelist:\n",
    "    with open (thisfile, \"r\") as openfile:\n",
    "        for processed_email in openfile:\n",
    "            # Defaults\n",
    "            term_count = 0\n",
    "            pred_spam = 0\n",
    "\n",
    "            # Read in tuples created by mapper\n",
    "            processed_email = processed_email.split(\" ||| \")\n",
    "            is_spam = int(processed_email[0])\n",
    "            count_dict = ast.literal_eval(processed_email[2])\n",
    "            \n",
    "            # We will predict using the sum of logs to avoid multiplying together very small\n",
    "            #   numbers, which would risk a floating point error.\n",
    "            spam_prediction = math.log(spam_prior)\n",
    "            ham_prediction = math.log(ham_prior)\n",
    "            for key, value in count_dict.iteritems():\n",
    "                spam_prediction += value*math.log(probs_given_spam[key])\n",
    "                ham_prediction += value*math.log(probs_given_ham[key])\n",
    "                #print key, value\n",
    "                #print spam_prediction\n",
    "                #print ham_prediction\n",
    "                        \n",
    "            # Pick the higher probability\n",
    "            if spam_prediction > ham_prediction: \n",
    "                pred_spam = 1\n",
    "                \n",
    "            #print \"Predict:\", pred_spam\n",
    "            #print \"Actual:\", is_spam\n",
    "            # Store accuracy in a list\n",
    "            accuracy.append(1*(pred_spam==is_spam))\n",
    "            print pred_spam\n",
    "            print accuracy\n",
    "\n",
    "# Print accuracy\n",
    "print \"Accuracy = {:.2f}\".format(float(sum(accuracy))/len(accuracy))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18284\n",
      "13184\n",
      "5740\n",
      "\n",
      "0.44\n",
      "0.56\n",
      "-0.82098055207\n",
      "-0.579818495253\n",
      "0\n",
      "[1]\n",
      "0\n",
      "[1, 1]\n",
      "0\n",
      "[1, 1, 1]\n",
      "0\n",
      "[1, 1, 1, 1]\n",
      "0\n",
      "[1, 1, 1, 1, 1]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1, 1]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0]\n",
      "0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0]\n",
      "Accuracy = 0.56\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# HW 1.5: TODO\n",
    "def HW1_5():\n",
    "    # Run pNaiveBayes.sh\n",
    "    !./pNaiveBayes.sh 1 \"*\"\n",
    "    \n",
    "    # Print the output file contents to screen\n",
    "    with open (\"enronemail_1h.txt.output\", \"r\") as openfile:\n",
    "        print openfile.read()\n",
    "        \n",
    "HW1_5()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "# This mapper requires each mapper to be given a chunk size of one individual email\n",
    "import sys\n",
    "import re\n",
    "\n",
    "findword = sys.argv[2]\n",
    "findfile = sys.argv[1]\n",
    "\n",
    "with open (findfile, \"r\") as myfile:\n",
    "    for content in myfile:\n",
    "        try:\n",
    "            # Spam classification\n",
    "            is_spam = re.findall(\"\\t([0-1])\\t\",content)[0]\n",
    "\n",
    "            # Parse out email body for processing. Find body using \"tab spam/ham tab\"\n",
    "            # use regex to strip out non alpha-numeric. \"don't\" will become \"dont\" which is fine.\n",
    "            keyword = re.findall(\"\\t[0-1]\\t\",content)[0]\n",
    "            email_id, is_spam_tabbed, email_body = content.partition(keyword)\n",
    "            email_body = re.sub('[^A-Za-z0-9\\s]+', '', email_body)\n",
    "            email_distinct = sorted(list(set(email_body.split())))\n",
    "\n",
    "            # Number of hits for search word\n",
    "            term_hits = len(re.findall(findword,email_body))\n",
    "            \n",
    "            # Print as triple with unique splitter \"|||\"\n",
    "            print \"{} ||| {} ||| {}\".format(is_spam, term_hits, email_distinct)\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import ast\n",
    "\n",
    "filelist = sys.argv[1:]\n",
    "\n",
    "spam_words = []\n",
    "ham_words = []\n",
    "spam_search_count = 0\n",
    "ham_search_count = 0\n",
    "spam_count = 0\n",
    "ham_count = 0\n",
    "\n",
    "# Open each file and build Multinomial Naive Bayes model\n",
    "for thisfile in filelist:\n",
    "    with open (thisfile, \"r\") as openfile:\n",
    "        for line in openfile:\n",
    "            # Read in triples created by mapper\n",
    "            line = line.split(\" ||| \")\n",
    "            is_spam = int(line[0])\n",
    "            searchterm_count = int(line[1])\n",
    "            distinct_words = ast.literal_eval(line[2])\n",
    "            \n",
    "            if is_spam:\n",
    "                spam_words += distinct_words\n",
    "                spam_search_count += searchterm_count\n",
    "                spam_count += 1\n",
    "            else:\n",
    "                ham_words += distinct_words\n",
    "                ham_search_count += searchterm_count\n",
    "                ham_count += 1\n",
    "    \n",
    "    # Find unique word counts\n",
    "    spam_words = sorted(list(set(spam_words)))\n",
    "    ham_words = sorted(list(set(ham_words)))\n",
    "    all_words = sorted(list(set(spam_words + ham_words)))\n",
    "\n",
    "    \n",
    "    spam_prior = spam_count / (spam_count + ham_count)\n",
    "    ham_prior = 1 - spam_prior\n",
    "    \n",
    "    search_given_spam = (spam_search_count + 1) / ()\n",
    "    search_given_ham = \n",
    "             \n",
    "#            print is_spam, searchterm_count, distinct_words\n",
    "\n",
    "print \"Spam words\", len(spam_words)\n",
    "print \"Ham words\", len(ham_words)\n",
    "\n",
    "\n",
    "print \"Spam words\", len(spam_words)\n",
    "print \"Ham words\", len(ham_words)\n",
    "\n",
    "# Open each file and predict. Note - prediction is embaressingly parallel and could\n",
    "#   be done effectively via Mapping. However, we were asked not to modify pNaiveBayes.sh\n",
    "#   as part of this assignment. As a result, I will predict here in the reducer although\n",
    "#   it is not the most efficient method.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '3', 'five']\n",
      "1 3 five\n"
     ]
    }
   ],
   "source": [
    "test = [\"1\",\"3\",\"five\"]\n",
    "print str(test)\n",
    "print \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "content = \"12313313\t1\tasdad13\t1\ta13\t0\ta\"\n",
    "print re.findall(\"\\t([0-1])\\t\",content)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12313313\n",
      "\t1\t\n",
      "asdad13\t1\ta13\t0\ta\n"
     ]
    }
   ],
   "source": [
    "content = \"12313313\t1\tasdad13\t1\ta13\t0\ta\"\n",
    "keyword = re.findall(\"\\t[0-1]\\t\",content)[0]\n",
    "email_id, is_spam, email_body = content.partition(keyword)\n",
    "print email_id\n",
    "print is_spam\n",
    "print email_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
