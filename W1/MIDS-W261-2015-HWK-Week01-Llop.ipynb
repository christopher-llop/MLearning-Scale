{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIDS W261 Machine Learning At Scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Christopher Llop | christopher.llop@ischool.berkeley.edu <br>\n",
    "Week 1 | Submission Date:\n",
    "\n",
    "<span style=\"color:red\">[Placeholder for introduction to assignment]</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:silver\"><b>HW1.0.0.</b> Define big data. Provide an example of a big data problem in your domain of expertise. </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\"><b>Answer:</b></span> In short, big data refers to problems using large or complex data sets that cannot quickly and easily be processed by a single machine using \"traditional\" methods of data-processing. There are several reasons why data could be not easy to process by traditional means. \n",
    "\n",
    "Many people talk about the 3 (or 4) V's: Velocity, Volume, Variety and Varacity. These lead to data challenges of processing, storage, or throughput that can be addressed by some \"big data\" techniques.\n",
    "\n",
    "In my current domain of economic and litigation consulting, most of our data processing can be handled by traditional means. However, we are starting to run into situations that push the boundaries of what we can do by traditional means. For example:\n",
    "- Quickly processing 104,000 Analyst Report text documents\n",
    "- Analyzing multiple TB of credit card transaction records at stake in litigation\n",
    "- Modeling complicated relationships between nodes in the electric system at an hourly level over decades\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:silver\"><b>HW1.0.1.</b> In 500 words (English or pseudo code or a combination) describe how to estimate the bias, the variance, the error for a test dataset T when using polynomial regression models of degree 1, 2,3, 4,5 are considered. How would you select a model?</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\"><b>Answer:</b></span>\n",
    "\n",
    "<b>Bias:</b> We can estimate the bais by taking bootstrap samples of our data (resampling), building many of each  polynomial model, and useing these models to calculate average predicted values for each type of model. By comparing average predicted values of data to the actual values, we can determine how off our model is on average. If the average prediction values are very different from the actual values, bais is high. If they are similar, bais is low.\n",
    "$$E[H(x^*)] - f(x^*)$$\n",
    "\n",
    "<b>Variance:</b> We can estimate the variances through a similar bootstrapping process. For each polynomial model, we can measure how consistent the predictions are from one bootstrap to the next. If the predictions for an example are all similar (tightly clustered), the variance is low. If they are dispersed, the variance is high. We can use the expected value formula for variance for the calculation: \n",
    "$$E[(h(x^*) - E[h(x^*)])^2]$$\n",
    "\n",
    "<b>Error:</b> The error can be evaluated by running our predictions on a held out test set, then evaluating how far off we are using a loss function such as squared prediction error. Because the model was not trained on this held out set, it will (likely) not be impacted by overfitting.  We can then compare the error of each polynomial and select the model with the least error on the held out test set. As shown in lecture and class readings, a similar result can be obtained by minimzing the $bais^2 + variance$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:silver\"><b>HW1.1.</b></style> Read through the provided control script (pNaiveBayes.sh) and all of its comments. When you are comfortable with their purpose and function, respond to the remaining homework questions below. A simple cell in the notebook with a print statmement with  a \"done\" string will suffice here. (dont forget to include the Question Number and the quesition in the cell as a multiline comment!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# HW 1.1: Read through pNaiveBayes and all comments to \n",
    "#         become comfortable with the code.\n",
    "\n",
    "def HW1_1():\n",
    "    print \"done\"\n",
    "\n",
    "HW1_1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:silver\"><b>HW1.2.</b> Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh will determine the number of occurrences of a single, user-specified word. Examine the word “assistance” and report your results.</span>\n",
    "\n",
    "   <span style=\"color:silver\">To do so, make sure that:</span>\n",
    "   - <span style=\"color:silver\">mapper.py counts all occurrences of a single word, and</span>\n",
    "   - <span style=\"color:silver\">reducer.py collates the counts of the single word.</span>\n",
    "   \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "\n",
    "# Get input parameters\n",
    "findwords = sys.argv[2].split()\n",
    "findfile = sys.argv[1]\n",
    "term_hits = {}\n",
    "\n",
    "with open (findfile, \"r\") as myfile:\n",
    "    for full_email in myfile:\n",
    "        try:\n",
    "            # Spam classification\n",
    "            is_spam = re.findall(\"\\t([0-1])\\t\",full_email)[0]\n",
    "\n",
    "            # Parse out email body for processing. Find body using \"tab spam/ham tab\"\n",
    "            # use regex to strip out non alpha-numeric. \"don't\" will become \"dont\" which is fine.\n",
    "            keyword = re.findall(\"\\t[0-1]\\t\",full_email)[0]\n",
    "            email_id, is_spam_tabbed, email_body = full_email.partition(keyword)\n",
    "            email_body = re.sub('[^A-Za-z0-9\\s]+', '', email_body)\n",
    "\n",
    "            # Number of hits for search word. Using a dictionary now will make this\n",
    "            #    code easier to apply to multiple search terms later in the problem set.\n",
    "            for word in findwords:\n",
    "                term_hits[word] = len(re.findall(word,email_body))\n",
    "            \n",
    "            # Print as tuple with unique splitter \"|||\"\n",
    "            print \"{} ||| {}\".format(is_spam, term_hits)\n",
    "\n",
    "        except:\n",
    "            print \"Processing Error\"\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import ast\n",
    "\n",
    "# Get input parameters - list of file names\n",
    "filelist = sys.argv[1:]\n",
    "\n",
    "count_dict = {}\n",
    "term_sum = {}\n",
    "\n",
    "# Open each map result\n",
    "for thisfile in filelist:\n",
    "    with open (thisfile, \"r\") as openfile:\n",
    "        for processed_email in openfile:\n",
    "            processed_email = processed_email.split(\" ||| \")\n",
    "            \n",
    "            count_dict = ast.literal_eval(processed_email[1])\n",
    "            # Fold (sum) together the results from mapping\n",
    "            for key, value in count_dict.iteritems():\n",
    "                term_sum[key] = term_sum.get(key,0) + value\n",
    "        \n",
    "for key, value in term_sum.iteritems():\n",
    "    print \"The word count for '{}' is {}\".format(key, value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use chmod for permissions\n",
    "!chmod a+x mapper.py\n",
    "!chmod a+x reducer.py\n",
    "!chmod a+x pNaiveBayes.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: The word count for 'assistance' is 10\n",
      "\n",
      "Check Result: 10\n"
     ]
    }
   ],
   "source": [
    "# HW 1.2: Create map/reduce pair that determins occurances of a single word\n",
    "def HW1_2():\n",
    "    # Run pNaiveBayes.sh\n",
    "    !./pNaiveBayes.sh 10 \"assistance\"\n",
    "\n",
    "    # Print the output file contents to screen\n",
    "    with open (\"enronemail_1h.txt.output\", \"r\") as openfile:\n",
    "        print \"Result:\", openfile.read()\n",
    "\n",
    "    # Crosscheck results (data is small enough to use RE in python)\n",
    "    with open (\"enronemail_1h.txt\", \"r\") as myfile:\n",
    "        print \"Check Result:\", len(re.findall(\"assistance\",myfile.read()))\n",
    "        \n",
    "HW1_2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>HW1.3. </b>Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh\n",
    "   will classify the email messages by a single, user-specified word. Examine the word “assistance” and report your results. To do so, make sure that\n",
    "   \n",
    "   - mapper.py is same as in part (2), and\n",
    "   - reducer.py performs a single word Naive Bayes classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "filelist = sys.argv[1:]\n",
    "\n",
    "spam_search_count = 0\n",
    "ham_search_count = 0\n",
    "spam_count = 0\n",
    "ham_count = 0\n",
    "\n",
    "# Open each file and build Multinomial Naive Bayes model\n",
    "for thisfile in filelist:\n",
    "    with open (thisfile, \"r\") as openfile:\n",
    "        for processed_email in openfile:\n",
    "            # Read in triples created by mapper\n",
    "            processed_email = processed_email.split(\" ||| \")\n",
    "            is_spam = int(processed_email[0])\n",
    "            searchterm_count = int(processed_email[1])\n",
    "            \n",
    "            if is_spam:\n",
    "                spam_search_count += searchterm_count\n",
    "                spam_count += 1\n",
    "            else:\n",
    "                ham_search_count += searchterm_count\n",
    "                ham_count += 1\n",
    "    \n",
    "    spam_prior = spam_count / (spam_count + ham_count)\n",
    "    ham_prior = 1 - spam_prior\n",
    "    \n",
    "    # Note: the denominator is fixed at either 0+1 or 1+1 depending on if the\n",
    "    #       search term is present. This is because we are limiting the language size to 1\n",
    "    search_given_spam = (spam_search_count + 1) / ( + 1)\n",
    "    search_given_ham = \n",
    "             \n",
    "#            print is_spam, searchterm_count, distinct_words\n",
    "\n",
    "print \"Spam words\", len(spam_words)\n",
    "print \"Ham words\", len(ham_words)\n",
    "\n",
    "\n",
    "print \"Spam words\", len(spam_words)\n",
    "print \"Ham words\", len(ham_words)\n",
    "\n",
    "# Open each file and predict. Note - prediction is embaressingly parallel and could\n",
    "#   be done effectively via Mapping. However, we were asked not to modify pNaiveBayes.sh\n",
    "#   as part of this assignment. As a result, I will predict here in the reducer although\n",
    "#   it is not the most efficient method.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use chmod for permissions\n",
    "!chmod a+x mapper.py\n",
    "!chmod a+x reducer.py\n",
    "!chmod a+x pNaiveBayes.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run pNaiveBayes.sh\n",
    "!./pNaiveBayes.sh 50 \"assistance\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>HW1.4.</b> Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh\n",
    "   will classify the email messages by a list of one or more user-specified words. Examine the words “assistance”, “valium”, and “enlargementWithATypo” and report your results\n",
    "   To do so, make sure that\n",
    "\n",
    "   - mapper.py counts all occurrences of a list of words, and\n",
    "   - reducer.py performs the multiple-word Naive Bayes classification via the chosen list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>HW1.5.</b> Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh will classify the email messages by all words present. To do so, make sure that\n",
    "\n",
    "   - mapper.py counts all occurrences of all words, and\n",
    "   - reducer.py performs a word-distribution-wide Naive Bayes classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "# This mapper requires each mapper to be given a chunk size of one individual email\n",
    "import sys\n",
    "import re\n",
    "\n",
    "findword = sys.argv[2]\n",
    "findfile = sys.argv[1]\n",
    "\n",
    "with open (findfile, \"r\") as myfile:\n",
    "    for content in myfile:\n",
    "        try:\n",
    "            # Spam classification\n",
    "            is_spam = re.findall(\"\\t([0-1])\\t\",content)[0]\n",
    "\n",
    "            # Parse out email body for processing. Find body using \"tab spam/ham tab\"\n",
    "            # use regex to strip out non alpha-numeric. \"don't\" will become \"dont\" which is fine.\n",
    "            keyword = re.findall(\"\\t[0-1]\\t\",content)[0]\n",
    "            email_id, is_spam_tabbed, email_body = content.partition(keyword)\n",
    "            email_body = re.sub('[^A-Za-z0-9\\s]+', '', email_body)\n",
    "            email_distinct = sorted(list(set(email_body.split())))\n",
    "\n",
    "            # Number of hits for search word\n",
    "            term_hits = len(re.findall(findword,email_body))\n",
    "            \n",
    "            # Print as triple with unique splitter \"|||\"\n",
    "            print \"{} ||| {} ||| {}\".format(is_spam, term_hits, email_distinct)\n",
    "\n",
    "        except:\n",
    "            print \"Processing Error\"\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import ast\n",
    "\n",
    "filelist = sys.argv[1:]\n",
    "\n",
    "spam_words = []\n",
    "ham_words = []\n",
    "spam_search_count = 0\n",
    "ham_search_count = 0\n",
    "spam_count = 0\n",
    "ham_count = 0\n",
    "\n",
    "# Open each file and build Multinomial Naive Bayes model\n",
    "for thisfile in filelist:\n",
    "    with open (thisfile, \"r\") as openfile:\n",
    "        for line in openfile:\n",
    "            # Read in triples created by mapper\n",
    "            line = line.split(\" ||| \")\n",
    "            is_spam = int(line[0])\n",
    "            searchterm_count = int(line[1])\n",
    "            distinct_words = ast.literal_eval(line[2])\n",
    "            \n",
    "            if is_spam:\n",
    "                spam_words += distinct_words\n",
    "                spam_search_count += searchterm_count\n",
    "                spam_count += 1\n",
    "            else:\n",
    "                ham_words += distinct_words\n",
    "                ham_search_count += searchterm_count\n",
    "                ham_count += 1\n",
    "    \n",
    "    # Find unique word counts\n",
    "    spam_words = sorted(list(set(spam_words)))\n",
    "    ham_words = sorted(list(set(ham_words)))\n",
    "    all_words = sorted(list(set(spam_words + ham_words)))\n",
    "\n",
    "    \n",
    "    spam_prior = spam_count / (spam_count + ham_count)\n",
    "    ham_prior = 1 - spam_prior\n",
    "    \n",
    "    search_given_spam = (spam_search_count + 1) / ()\n",
    "    search_given_ham = \n",
    "             \n",
    "#            print is_spam, searchterm_count, distinct_words\n",
    "\n",
    "print \"Spam words\", len(spam_words)\n",
    "print \"Ham words\", len(ham_words)\n",
    "\n",
    "\n",
    "print \"Spam words\", len(spam_words)\n",
    "print \"Ham words\", len(ham_words)\n",
    "\n",
    "# Open each file and predict. Note - prediction is embaressingly parallel and could\n",
    "#   be done effectively via Mapping. However, we were asked not to modify pNaiveBayes.sh\n",
    "#   as part of this assignment. As a result, I will predict here in the reducer although\n",
    "#   it is not the most efficient method.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "content = \"12313313\t1\tasdad13\t1\ta13\t0\ta\"\n",
    "print re.findall(\"\\t([0-1])\\t\",content)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12313313\n",
      "\t1\t\n",
      "asdad13\t1\ta13\t0\ta\n"
     ]
    }
   ],
   "source": [
    "content = \"12313313\t1\tasdad13\t1\ta13\t0\ta\"\n",
    "keyword = re.findall(\"\\t[0-1]\\t\",content)[0]\n",
    "email_id, is_spam, email_body = content.partition(keyword)\n",
    "print email_id\n",
    "print is_spam\n",
    "print email_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
