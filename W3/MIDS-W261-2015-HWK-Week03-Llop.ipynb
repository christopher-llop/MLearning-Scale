{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIDS W261 Machine Learning At Scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Christopher Llop | christopher.llop@ischool.berkeley.edu <br>\n",
    "Week 3 | Submission Date: 9/22/2015\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>HW3.0.</b>\n",
    "\n",
    "What is a merge sort? Where is it used in Hadoop?\n",
    "\n",
    "\n",
    "\n",
    "How is  a combiner function in the context of Hadoop? \n",
    "\n",
    "\n",
    "\n",
    "Give an example where it can be used and justify why it should be used in the context of this problem.\n",
    "\n",
    "\n",
    "\n",
    "What is the Hadoop shuffle?\n",
    "\n",
    "\n",
    "\n",
    "What is the Apriori algorithm? Describe an example use in your domain of expertise. \n",
    "\n",
    "\n",
    "\n",
    "Define confidence and lift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:silver\"><b>HW3.1. </b></span>\n",
    "\n",
    "<span style=\"color:silver\">Product Recommendations: The action or practice of selling additional products or services to existing customers is called cross-selling. Giving product recommendation is one of the examples of cross-selling that are frequently used by online retailers. One simple method to give product recommendations is to recommend products that are frequently browsed together by the customers.</span>\n",
    "\n",
    "<span style=\"color:silver\">Suppose we want to recommend new products to the customer based on the products they have already browsed on the online website. Write a program using the A-priori algorithm to find products which are frequently browsed together. Fix the support to s = 100  (i.e. product pairs need to occur together at least 100 times to be considered frequent) and find itemsets of size 2 and 3. (Note - Jake told us not to do this via the Google Group).</span>\n",
    "\n",
    "<span style=\"color:silver\">Use the online browsing behavior dataset at: </span>\n",
    "\n",
    "https://www.dropbox.com/s/zlfyiwa70poqg74/ProductPurchaseData.txt?dl=0\n",
    "\n",
    "<span style=\"color:silver\">Each line in this dataset represents a browsing session of a customer. On each line, each string <br>\n",
    "of 8 characters represents the id of an item browsed during that session. The items are separated <br>\n",
    "by spaces.</span>\n",
    "\n",
    "<span style=\"color:silver\">Do some exploratory data analysis of this dataset. </span>\n",
    "\n",
    "<span style=\"color:silver\">Report your findings such as number of unique products; largest basket, etc. using Hadoop Map-Reduce.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\"><b>Answer:</b></span>\n",
    "We were asked to preform some EDA. I've decided to look at the number of unique products, the largest basket, the freqency of each basket size, and the frequency of each product. The code below solves each of these problems. There are two MapReduce runs. The first calculates the number of unique products and the product frequency. The second finds the largest basket (it turns out there is a tie) and counts the frequency of each basket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of unique products and product frequency can be solved together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "item_inventory = {}\n",
    "\n",
    "for line in sys.stdin:\n",
    "    for item in line.rstrip('\\n').split():\n",
    "        item_inventory[item] = item_inventory.get(item, 0) + 1\n",
    "            \n",
    "for item, inventory in item_inventory.iteritems():\n",
    "    print \"{}\\t{}\".format(item, inventory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "unique_item_count = 0\n",
    "current_item_count = 0\n",
    "current_item = \"\"\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.rstrip('\\n').split()\n",
    "    if current_item == line[0]:\n",
    "        # If same item, add to count\n",
    "        current_item_count += int(line[1])\n",
    "    else:\n",
    "        # If new item, print, increment unique, restart count\n",
    "        if unique_item_count > 0:\n",
    "            print current_item, current_item_count\n",
    "        unique_item_count += 1\n",
    "        current_item_count = int(line[1])\n",
    "        current_item = line[0]\n",
    "        \n",
    "# Print final row of counts\n",
    "print current_item, current_item_count\n",
    "\n",
    "# Finally, print the number of unique items (will be on last row of reducer output)\n",
    "print unique_item_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use chmod for permissions\n",
    "!chmod a+x mapper.py\n",
    "!chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Move files and make directory\n",
    "!hadoop fs -mkdir ./W261/In/HW3\n",
    "!hdfs dfs -put ./ProductPurchaseData.txt ./W261/In/HW3/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# HW3.1_a: Execute a job using Hadoop Streaming to generate 10,000 random integers and sort them.\n",
    "def HW3_1a():\n",
    "    !hadoop jar /usr/local/Cellar/hadoop/2.6.0/libexec/share/hadoop/tools/lib/hadoop-streaming-2.6.0.jar \\\n",
    "    -Dmapreduce.job.maps=10 \\\n",
    "    -Dmapreduce.job.reduces=1 \\\n",
    "    -files ./mapper.py,./reducer.py \\\n",
    "    -mapper ./mapper.py  \\\n",
    "    -reducer ./reducer.py \\\n",
    "    -input ./W261/In/HW3/ProductPurchaseData.txt -output ./W261/Out/HW3_1_a    \n",
    "    \n",
    "#HW3_1a()\n",
    "\n",
    "# Note - to clean up this python notebook, we ran HW3_1a and are replacing with its output\n",
    "print \"The first 5 results in the reducer output are:\"\n",
    "!hadoop fs -cat ./W261/Out/HW3_1_a/part-00000 | head -n5\n",
    "\n",
    "print\n",
    "print \"The total number of unique items is:\"\n",
    "!hadoop fs -cat ./W261/Out/HW3_1_a/part-00000 | tail -n1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Largest basket and frequency of basket counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "basket_inventory = {}\n",
    "\n",
    "for basket in sys.stdin:\n",
    "#    for basket in line:\n",
    "    basket = basket.rstrip('\\n')\n",
    "    basket_inventory[basket] = basket_inventory.get(basket, 0) + 1\n",
    "            \n",
    "# Note - this code assumes we can fit the ENTIRE document in memory. This isnt' best practice.\n",
    "#   I should really updated this code to check memory and emit whenever memory hits a certain point.\n",
    "for basket, inventory in basket_inventory.iteritems():\n",
    "    print \"{}\\t{}\".format(basket, inventory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "unique_basket_count = 0\n",
    "current_basket_count = 0\n",
    "current_basket = \"\"\n",
    "largest_basket = []\n",
    "largest_basket_size = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.rstrip('\\n').split('\\t')\n",
    "    if current_basket == line[0]:\n",
    "        # If same item, add to count\n",
    "        current_basket_count += int(line[1])\n",
    "    else:\n",
    "        # If new item, print, increment unique, restart count\n",
    "        if unique_basket_count > 0:\n",
    "            print current_basket.rstrip('\\n'), current_basket_count\n",
    "        unique_basket_count += 1\n",
    "        current_basket_count = int(line[1])\n",
    "        current_basket = line[0]\n",
    "        \n",
    "    # Track the maximum basket size\n",
    "    if len(current_basket.split()) > largest_basket_size:\n",
    "        largest_basket_size = len(current_basket.split())\n",
    "        largest_basket = [current_basket]\n",
    "    elif len(current_basket.split()) == largest_basket_size:\n",
    "        largest_basket.append(current_basket)\n",
    "        \n",
    "print current_basket.rstrip('\\n'), current_basket_count\n",
    "print \"The largest basket(s) have {} items. There are {} such baskets: {}\".format(\n",
    "    largest_basket_size, len(largest_basket), largest_basket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# HW3.1_b: Execute a job using Hadoop Streaming to generate 10,000 random integers and sort them.\n",
    "def HW3_1b():\n",
    "    !hadoop jar /usr/local/Cellar/hadoop/2.6.0/libexec/share/hadoop/tools/lib/hadoop-streaming-2.6.0.jar \\\n",
    "    -Dmapreduce.job.maps=10 \\\n",
    "    -Dmapreduce.job.reduces=1 \\\n",
    "    -files ./mapper.py,./reducer.py \\\n",
    "    -mapper ./mapper.py  \\\n",
    "    -reducer ./reducer.py \\\n",
    "    -input ./W261/In/HW3/ProductPurchaseData.txt -output ./W261/Out/HW3_1_b    \n",
    "    \n",
    "#HW3_1b()\n",
    "\n",
    "# Note - to clean up this python notebook, we ran HW3_1b and are replacing with its output\n",
    "print \"The first 5 results in the reducer output are:\"\n",
    "!hadoop fs -cat ./W261/Out/HW3_1_b/part-00000 | head -n5\n",
    "\n",
    "print\n",
    "print \"Printing largest baskets....\"\n",
    "!hadoop fs -cat ./W261/Out/HW3_1_b/part-00000 | tail -n1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>HW3.2.</b> (Computationally prohibitive but then again Hadoop can handle this)\n",
    "\n",
    "Note: for this part the writeup will require a specific rule ordering but the program need not sort the output.\n",
    "\n",
    "List the top 5 rules with corresponding confidence scores in decreasing order of confidence score \n",
    "for frequent (100>count) itemsets of size 2. \n",
    "A rule is of the form: \n",
    "\n",
    "(item1) â‡’ item2.\n",
    "\n",
    "Fix the ordering of the rule lexicographically (left to right), \n",
    "and break ties in confidence (between rules, if any exist) \n",
    "by taking the first ones in lexicographically increasing order. \n",
    "Use Hadoop MapReduce to complete this part of the assignment; \n",
    "use a single mapper and single reducer; use a combiner if you think it will help and justify. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "# Formula is: I -> J = (I U J) / I\n",
    "\n",
    "# Pairs Approach | Note, I'm traveling to see a sick family member this weekend. While\n",
    "# stipes is faster and perhaps a better approach, I am making the design decision that, in \n",
    "# my case, time of the programmer is the resource that we need to take into account. You can \n",
    "# liken this to a business situation where a project cannot afford programing hours, but can\n",
    "# have a longer runtime :). In future weeks, I'll make different decisions.\n",
    "\n",
    "# To gain some efficiecny back, I'll write a quick combiner to help make things better.\n",
    "\n",
    "basket_inventory = {}\n",
    "        \n",
    "for basket in sys.stdin:\n",
    "    # Get unique items in basket\n",
    "    basket = list(set(basket.rstrip('\\n').split()))\n",
    "\n",
    "    # Output a single record for each item so that we can use order inversion in our\n",
    "    # reduce side frequency count.\n",
    "    for item1 in basket:\n",
    "        for item2 in basket:\n",
    "            if item1 == item2:\n",
    "                print \"* {}\\t1\".format(item1)\n",
    "            else:\n",
    "                print \"{} {}\\t1\".format(item1, item2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting combiner.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile combiner.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "#itempairs = [\"* 55\\t1\",\"* 55\\t1\",\"* 66\\t1\",\"23 45\\t1\",\"23 45\\t1\",\"23 95\\t1\"]\n",
    "currentpair = \"\"\n",
    "currentcount = 0\n",
    "\n",
    "for itempair in sys.stdin:\n",
    "#for itempair in itempairs:\n",
    "    itempair = itempair.rstrip('\\n')\n",
    "    # If multiple of one key in a row, sum\n",
    "    if itempair.split('\\t')[0] == currentpair:\n",
    "        currentcount += int(itempair.split('\\t')[1])\n",
    "    else:\n",
    "        # Otherwise, print and reset counters. Note - a combiner must print in the same format\n",
    "        # as a mapper.\n",
    "        if currentcount > 0:\n",
    "            print \"{}\\t{}\".format(currentpair, currentcount)\n",
    "        currentpair = itempair.split('\\t')[0]\n",
    "        currentcount = int(itempair.split('\\t')[1])\n",
    "print \"{}\\t{}\".format(currentpair, currentcount)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "#currentpair = \"\"\n",
    "#currentcount = 0\n",
    "\n",
    "\n",
    "#for line in sys.stdin:\n",
    "#    print line.rstrip('\\n')\n",
    "for itempair in sys.stdin:\n",
    "#    itempair = itempair.rstrip('\\n')\n",
    "#    # If multiple of one key in a row, sum\n",
    "#    if itempair.split('\\t')[0] == currentpair:\n",
    "#        currentcount += int(itempair.split('\\t')[1])\n",
    "#    else:\n",
    "#        # Otherwise, print and reset counters. Note - a combiner must print in the same format\n",
    "#        # as a mapper.\n",
    "#        if currentcount > 0:\n",
    "#            print \"{}\\t{}\".format(currentpair, currentcount)\n",
    "#        currentpair = itempair.split('\\t')[0]\n",
    "#        currentcount = int(itempair.split('\\t')[1])\n",
    "#print \"{}\\t{}\".format(currentpair, currentcount)\n",
    "\n",
    "    print itempair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/21 06:36:36 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/21 06:36:37 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "15/09/21 06:36:37 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "15/09/21 06:36:37 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "15/09/21 06:36:37 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "15/09/21 06:36:38 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "15/09/21 06:36:38 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1347217305_0001\n",
      "15/09/21 06:36:38 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "15/09/21 06:36:38 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "15/09/21 06:36:38 INFO mapreduce.Job: Running job: job_local1347217305_0001\n",
      "15/09/21 06:36:38 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "15/09/21 06:36:38 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "15/09/21 06:36:38 INFO mapred.LocalJobRunner: Starting task: attempt_local1347217305_0001_m_000000_0\n",
      "15/09/21 06:36:38 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "15/09/21 06:36:38 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "15/09/21 06:36:38 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/cjllop/W261/In/HW3/ProductPurchaseData.txt:0+3458517\n",
      "15/09/21 06:36:38 INFO mapred.MapTask: numReduceTasks: 1\n",
      "15/09/21 06:36:38 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "15/09/21 06:36:38 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "15/09/21 06:36:38 INFO mapred.MapTask: soft limit at 83886080\n",
      "15/09/21 06:36:38 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "15/09/21 06:36:38 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "15/09/21 06:36:38 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "15/09/21 06:36:38 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/cjllop/Code/MIDS/W261/HW/W3/././mapper.py]\n",
      "15/09/21 06:36:38 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "15/09/21 06:36:38 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "15/09/21 06:36:38 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "15/09/21 06:36:38 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "15/09/21 06:36:38 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "15/09/21 06:36:38 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "15/09/21 06:36:38 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "15/09/21 06:36:38 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "15/09/21 06:36:38 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "15/09/21 06:36:38 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "15/09/21 06:36:38 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "15/09/21 06:36:38 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "15/09/21 06:36:39 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/21 06:36:39 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/21 06:36:39 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/21 06:36:39 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/21 06:36:39 INFO mapreduce.Job: Job job_local1347217305_0001 running in uber mode : false\n",
      "15/09/21 06:36:39 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "15/09/21 06:36:39 INFO streaming.PipeMapRed: Records R/W=1389/1\n",
      "15/09/21 06:36:42 INFO streaming.PipeMapRed: R/W/S=10000/1739075/0 in:3333=10000/3 [rec/s] out:579692=1739078/3 [rec/s]\n",
      "15/09/21 06:36:43 INFO mapred.MapTask: Spilling map output\n",
      "15/09/21 06:36:43 INFO mapred.MapTask: bufstart = 0; bufend = 46124305; bufvoid = 104857600\n",
      "15/09/21 06:36:43 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 16773960(67095840); length = 9440437/6553600\n",
      "15/09/21 06:36:43 INFO mapred.MapTask: (EQUATOR) 55711281 kvi 13927816(55711264)\n",
      "15/09/21 06:36:44 INFO mapred.LocalJobRunner: Records R/W=1389/1 > map\n",
      "15/09/21 06:36:45 INFO mapreduce.Job:  map 40% reduce 0%\n",
      "15/09/21 06:36:47 INFO mapred.LocalJobRunner: Records R/W=1389/1 > map\n",
      "15/09/21 06:36:53 INFO mapred.LocalJobRunner: Records R/W=1389/1 > map\n",
      "15/09/21 06:36:54 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/cjllop/Code/MIDS/W261/HW/W3/././combiner.py]\n",
      "15/09/21 06:36:54 INFO Configuration.deprecation: mapred.skip.map.auto.incr.proc.count is deprecated. Instead, use mapreduce.map.skip.proc-count.auto-incr\n",
      "15/09/21 06:36:54 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/21 06:36:54 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/21 06:36:54 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/21 06:36:54 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/21 06:36:54 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/21 06:36:54 INFO streaming.PipeMapRed: Records R/W=40329/1\n",
      "15/09/21 06:36:54 INFO streaming.PipeMapRed: R/W/S=100000/3700/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/21 06:36:54 INFO streaming.PipeMapRed: R/W/S=200000/23847/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/21 06:36:55 INFO streaming.PipeMapRed: R/W/S=300000/64732/0 in:300000=300000/1 [rec/s] out:64732=64732/1 [rec/s]\n",
      "15/09/21 06:36:55 INFO streaming.PipeMapRed: R/W/S=400000/99882/0 in:400000=400000/1 [rec/s] out:99882=99882/1 [rec/s]\n",
      "15/09/21 06:36:56 INFO streaming.PipeMapRed: R/W/S=500000/131749/0 in:500000=500000/1 [rec/s] out:131749=131749/1 [rec/s]\n",
      "15/09/21 06:36:56 INFO streaming.PipeMapRed: R/W/S=600000/158689/0 in:300000=600000/2 [rec/s] out:79344=158689/2 [rec/s]\n",
      "15/09/21 06:36:56 INFO mapred.LocalJobRunner: Records R/W=40329/1 > map\n",
      "15/09/21 06:36:56 INFO streaming.PipeMapRed: R/W/S=700000/195478/0 in:350000=700000/2 [rec/s] out:97739=195478/2 [rec/s]\n",
      "15/09/21 06:36:56 INFO streaming.PipeMapRed: R/W/S=800000/228165/0 in:400000=800000/2 [rec/s] out:114082=228165/2 [rec/s]\n",
      "15/09/21 06:36:57 INFO streaming.PipeMapRed: R/W/S=900000/273145/0 in:300000=900000/3 [rec/s] out:91048=273145/3 [rec/s]\n",
      "15/09/21 06:36:57 INFO streaming.PipeMapRed: R/W/S=1000000/308284/0 in:333333=1000000/3 [rec/s] out:102761=308284/3 [rec/s]\n",
      "15/09/21 06:36:58 INFO streaming.PipeMapRed: R/W/S=1100000/349175/0 in:366666=1100000/3 [rec/s] out:116391=349175/3 [rec/s]\n",
      "15/09/21 06:36:58 INFO streaming.PipeMapRed: R/W/S=1200000/387599/0 in:300000=1200000/4 [rec/s] out:96899=387599/4 [rec/s]\n",
      "15/09/21 06:36:58 INFO streaming.PipeMapRed: R/W/S=1300000/424384/0 in:325000=1300000/4 [rec/s] out:106096=424384/4 [rec/s]\n",
      "15/09/21 06:36:59 INFO streaming.PipeMapRed: R/W/S=1400000/466095/0 in:350000=1400000/4 [rec/s] out:116523=466095/4 [rec/s]\n",
      "15/09/21 06:36:59 INFO mapred.LocalJobRunner: Records R/W=40329/1 > map\n",
      "15/09/21 06:36:59 INFO streaming.PipeMapRed: R/W/S=1500000/503699/0 in:300000=1500000/5 [rec/s] out:100739=503699/5 [rec/s]\n",
      "15/09/21 06:36:59 INFO streaming.PipeMapRed: R/W/S=1600000/542126/0 in:320000=1600000/5 [rec/s] out:108425=542126/5 [rec/s]\n",
      "15/09/21 06:37:00 INFO streaming.PipeMapRed: R/W/S=1700000/580543/0 in:283333=1700000/6 [rec/s] out:96757=580543/6 [rec/s]\n",
      "15/09/21 06:37:00 INFO streaming.PipeMapRed: R/W/S=1800000/612404/0 in:300000=1800000/6 [rec/s] out:102067=612404/6 [rec/s]\n",
      "15/09/21 06:37:01 INFO streaming.PipeMapRed: R/W/S=1900000/642637/0 in:316666=1900000/6 [rec/s] out:107106=642637/6 [rec/s]\n",
      "15/09/21 06:37:01 INFO streaming.PipeMapRed: R/W/S=2000000/685316/0 in:285714=2000000/7 [rec/s] out:97905=685341/7 [rec/s]\n",
      "15/09/21 06:37:01 INFO streaming.PipeMapRed: R/W/S=2100000/728510/0 in:300000=2100000/7 [rec/s] out:104072=728510/7 [rec/s]\n",
      "15/09/21 06:37:02 INFO streaming.PipeMapRed: R/W/S=2200000/767755/0 in:314285=2200000/7 [rec/s] out:109679=767755/7 [rec/s]\n",
      "15/09/21 06:37:02 INFO streaming.PipeMapRed: R/W/S=2300000/807004/0 in:287500=2300000/8 [rec/s] out:100875=807004/8 [rec/s]\n",
      "15/09/21 06:37:02 INFO mapred.LocalJobRunner: Records R/W=40329/1 > map\n",
      "15/09/21 06:37:02 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/09/21 06:37:02 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/09/21 06:37:02 INFO mapred.MapTask: Finished spill 0\n",
      "15/09/21 06:37:02 INFO mapred.MapTask: (RESET) equator 55711281 kv 13927816(55711264) kvi 11584304(46337216)\n",
      "15/09/21 06:37:02 INFO streaming.PipeMapRed: Records R/W=18924/2945989\n",
      "15/09/21 06:37:04 INFO mapred.MapTask: Spilling map output\n",
      "15/09/21 06:37:04 INFO mapred.MapTask: bufstart = 55711281; bufend = 101780183; bufvoid = 104857600\n",
      "15/09/21 06:37:04 INFO mapred.MapTask: kvstart = 13927816(55711264); kvend = 4473528(17894112); length = 9454289/6553600\n",
      "15/09/21 06:37:04 INFO mapred.MapTask: (EQUATOR) 6509559 kvi 1627384(6509536)\n",
      "15/09/21 06:37:05 INFO mapred.LocalJobRunner: Records R/W=18924/2945989 > map\n",
      "15/09/21 06:37:06 INFO mapreduce.Job:  map 66% reduce 0%\n",
      "15/09/21 06:37:08 INFO mapred.LocalJobRunner: Records R/W=18924/2945989 > map\n",
      "15/09/21 06:37:08 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/cjllop/Code/MIDS/W261/HW/W3/././combiner.py]\n",
      "15/09/21 06:37:08 INFO Configuration.deprecation: mapred.skip.reduce.auto.incr.proc.count is deprecated. Instead, use mapreduce.reduce.skip.proc-count.auto-incr\n",
      "15/09/21 06:37:08 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/21 06:37:08 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/21 06:37:08 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/21 06:37:08 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/21 06:37:08 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/21 06:37:08 INFO streaming.PipeMapRed: Records R/W=40329/1\n",
      "15/09/21 06:37:09 INFO streaming.PipeMapRed: R/W/S=100000/3663/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/21 06:37:09 INFO streaming.PipeMapRed: R/W/S=200000/19164/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/21 06:37:09 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "15/09/21 06:37:09 INFO streaming.PipeMapRed: R/W/S=300000/65802/0 in:300000=300000/1 [rec/s] out:65802=65802/1 [rec/s]\n",
      "15/09/21 06:37:10 INFO streaming.PipeMapRed: R/W/S=400000/109155/0 in:400000=400000/1 [rec/s] out:109155=109155/1 [rec/s]\n",
      "15/09/21 06:37:10 INFO streaming.PipeMapRed: R/W/S=500000/144301/0 in:500000=500000/1 [rec/s] out:144301=144301/1 [rec/s]\n",
      "15/09/21 06:37:10 INFO streaming.PipeMapRed: R/W/S=600000/182733/0 in:300000=600000/2 [rec/s] out:91366=182733/2 [rec/s]\n",
      "15/09/21 06:37:11 INFO streaming.PipeMapRed: R/W/S=700000/226087/0 in:350000=700000/2 [rec/s] out:113043=226087/2 [rec/s]\n",
      "15/09/21 06:37:11 INFO mapred.LocalJobRunner: Records R/W=40329/1 > map\n",
      "15/09/21 06:37:11 INFO streaming.PipeMapRed: R/W/S=800000/266974/0 in:400000=800000/2 [rec/s] out:133487=266974/2 [rec/s]\n",
      "15/09/21 06:37:12 INFO streaming.PipeMapRed: R/W/S=900000/313599/0 in:300000=900000/3 [rec/s] out:104533=313599/3 [rec/s]\n",
      "15/09/21 06:37:12 INFO streaming.PipeMapRed: R/W/S=1000000/356950/0 in:333333=1000000/3 [rec/s] out:118983=356950/3 [rec/s]\n",
      "15/09/21 06:37:12 INFO streaming.PipeMapRed: R/W/S=1100000/402064/0 in:275000=1100000/4 [rec/s] out:100516=402064/4 [rec/s]\n",
      "15/09/21 06:37:13 INFO streaming.PipeMapRed: R/W/S=1200000/455150/0 in:300000=1200000/4 [rec/s] out:113787=455150/4 [rec/s]\n",
      "15/09/21 06:37:13 INFO streaming.PipeMapRed: R/W/S=1300000/494830/0 in:260000=1300000/5 [rec/s] out:98971=494859/5 [rec/s]\n",
      "15/09/21 06:37:14 INFO streaming.PipeMapRed: R/W/S=1400000/549237/0 in:280000=1400000/5 [rec/s] out:109847=549237/5 [rec/s]\n",
      "15/09/21 06:37:14 INFO mapred.LocalJobRunner: Records R/W=40329/1 > map\n",
      "15/09/21 06:37:14 INFO streaming.PipeMapRed: R/W/S=1500000/588484/0 in:300000=1500000/5 [rec/s] out:117696=588484/5 [rec/s]\n",
      "15/09/21 06:37:15 INFO streaming.PipeMapRed: R/W/S=1600000/633477/0 in:266666=1600000/6 [rec/s] out:105579=633477/6 [rec/s]\n",
      "15/09/21 06:37:15 INFO streaming.PipeMapRed: R/W/S=1700000/678461/0 in:283333=1700000/6 [rec/s] out:113076=678461/6 [rec/s]\n",
      "15/09/21 06:37:15 INFO streaming.PipeMapRed: R/W/S=1800000/721810/0 in:257142=1800000/7 [rec/s] out:103115=721810/7 [rec/s]\n",
      "15/09/21 06:37:16 INFO streaming.PipeMapRed: R/W/S=1900000/760241/0 in:271428=1900000/7 [rec/s] out:108605=760241/7 [rec/s]\n",
      "15/09/21 06:37:16 INFO streaming.PipeMapRed: R/W/S=2000000/810965/0 in:285714=2000000/7 [rec/s] out:115852=810965/7 [rec/s]\n",
      "15/09/21 06:37:16 INFO streaming.PipeMapRed: R/W/S=2100000/859246/0 in:262500=2100000/8 [rec/s] out:107405=859246/8 [rec/s]\n",
      "15/09/21 06:37:17 INFO streaming.PipeMapRed: R/W/S=2200000/905879/0 in:275000=2200000/8 [rec/s] out:113234=905879/8 [rec/s]\n",
      "15/09/21 06:37:17 INFO mapred.LocalJobRunner: Records R/W=40329/1 > map\n",
      "15/09/21 06:37:17 INFO streaming.PipeMapRed: R/W/S=2300000/952518/0 in:287500=2300000/8 [rec/s] out:119064=952518/8 [rec/s]\n",
      "15/09/21 06:37:17 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/09/21 06:37:17 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/09/21 06:37:17 INFO mapred.MapTask: Finished spill 1\n",
      "15/09/21 06:37:18 INFO mapred.MapTask: (RESET) equator 6509559 kv 1627384(6509536) kvi 25501436(102005744)\n",
      "15/09/21 06:37:18 INFO streaming.PipeMapRed: Records R/W=31101/5308771\n",
      "15/09/21 06:37:18 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/09/21 06:37:18 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/09/21 06:37:18 INFO mapred.LocalJobRunner: Records R/W=40329/1 > map\n",
      "15/09/21 06:37:18 INFO mapred.MapTask: Starting flush of map output\n",
      "15/09/21 06:37:18 INFO mapred.MapTask: Spilling map output\n",
      "15/09/21 06:37:18 INFO mapred.MapTask: bufstart = 6509559; bufend = 20627585; bufvoid = 104857600\n",
      "15/09/21 06:37:18 INFO mapred.MapTask: kvstart = 1627384(6509536); kvend = 24941124(99764496); length = 2900661/6553600\n",
      "15/09/21 06:37:19 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/cjllop/Code/MIDS/W261/HW/W3/././combiner.py]\n",
      "15/09/21 06:37:19 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/21 06:37:19 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/21 06:37:19 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/21 06:37:19 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/21 06:37:19 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/21 06:37:19 INFO streaming.PipeMapRed: Records R/W=30247/1\n",
      "15/09/21 06:37:19 INFO streaming.PipeMapRed: R/W/S=100000/27916/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/21 06:37:20 INFO streaming.PipeMapRed: R/W/S=200000/81114/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/21 06:37:20 INFO mapred.LocalJobRunner: Records R/W=30247/1 > sort\n",
      "15/09/21 06:37:20 INFO streaming.PipeMapRed: R/W/S=300000/135135/0 in:300000=300000/1 [rec/s] out:135135=135135/1 [rec/s]\n",
      "15/09/21 06:37:21 INFO streaming.PipeMapRed: R/W/S=400000/194066/0 in:400000=400000/1 [rec/s] out:194066=194066/1 [rec/s]\n",
      "15/09/21 06:37:21 INFO streaming.PipeMapRed: R/W/S=500000/254651/0 in:250000=500000/2 [rec/s] out:127325=254651/2 [rec/s]\n",
      "15/09/21 06:37:21 INFO streaming.PipeMapRed: R/W/S=600000/311129/0 in:300000=600000/2 [rec/s] out:155564=311129/2 [rec/s]\n",
      "15/09/21 06:37:22 INFO streaming.PipeMapRed: R/W/S=700000/370070/0 in:350000=700000/2 [rec/s] out:185035=370070/2 [rec/s]\n",
      "15/09/21 06:37:22 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/09/21 06:37:22 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/09/21 06:37:22 INFO mapred.MapTask: Finished spill 2\n",
      "15/09/21 06:37:22 INFO mapred.Merger: Merging 3 sorted segments\n",
      "15/09/21 06:37:22 INFO mapred.Merger: Down to the last merge-pass, with 3 segments left of total size: 48404963 bytes\n",
      "15/09/21 06:37:22 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/cjllop/Code/MIDS/W261/HW/W3/././combiner.py]\n",
      "15/09/21 06:37:22 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/21 06:37:22 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/21 06:37:22 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/21 06:37:22 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/21 06:37:22 INFO streaming.PipeMapRed: Records R/W=9873/1\n",
      "15/09/21 06:37:22 INFO streaming.PipeMapRed: R/W/S=10000/1227/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/21 06:37:23 INFO streaming.PipeMapRed: R/W/S=100000/72086/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/21 06:37:23 INFO mapred.LocalJobRunner: Records R/W=9873/1 > sort > \n",
      "15/09/21 06:37:23 INFO mapreduce.Job:  map 69% reduce 0%\n",
      "15/09/21 06:37:23 INFO streaming.PipeMapRed: R/W/S=200000/152284/0 in:200000=200000/1 [rec/s] out:152298=152298/1 [rec/s]\n",
      "15/09/21 06:37:24 INFO streaming.PipeMapRed: R/W/S=300000/229794/0 in:300000=300000/1 [rec/s] out:229794=229794/1 [rec/s]\n",
      "15/09/21 06:37:24 INFO streaming.PipeMapRed: R/W/S=400000/313137/0 in:400000=400000/1 [rec/s] out:313137=313137/1 [rec/s]\n",
      "15/09/21 06:37:25 INFO streaming.PipeMapRed: R/W/S=500000/390779/0 in:250000=500000/2 [rec/s] out:195389=390779/2 [rec/s]\n",
      "15/09/21 06:37:26 INFO streaming.PipeMapRed: R/W/S=600000/469210/0 in:200000=600000/3 [rec/s] out:156403=469210/3 [rec/s]\n",
      "15/09/21 06:37:26 INFO streaming.PipeMapRed: R/W/S=700000/547680/0 in:233333=700000/3 [rec/s] out:182560=547680/3 [rec/s]\n",
      "15/09/21 06:37:26 INFO mapred.LocalJobRunner: Records R/W=9873/1 > sort > \n",
      "15/09/21 06:37:26 INFO mapreduce.Job:  map 78% reduce 0%\n",
      "15/09/21 06:37:26 INFO streaming.PipeMapRed: R/W/S=800000/629387/0 in:200000=800000/4 [rec/s] out:157346=629387/4 [rec/s]\n",
      "15/09/21 06:37:27 INFO streaming.PipeMapRed: R/W/S=900000/708690/0 in:225000=900000/4 [rec/s] out:177172=708690/4 [rec/s]\n",
      "15/09/21 06:37:27 INFO streaming.PipeMapRed: R/W/S=1000000/789638/0 in:200000=1000000/5 [rec/s] out:157927=789638/5 [rec/s]\n",
      "15/09/21 06:37:28 INFO streaming.PipeMapRed: R/W/S=1100000/872168/0 in:220000=1100000/5 [rec/s] out:174433=872168/5 [rec/s]\n",
      "15/09/21 06:37:28 INFO streaming.PipeMapRed: R/W/S=1200000/954783/0 in:200000=1200000/6 [rec/s] out:159130=954783/6 [rec/s]\n",
      "15/09/21 06:37:29 INFO streaming.PipeMapRed: R/W/S=1300000/1033911/0 in:216666=1300000/6 [rec/s] out:172321=1033929/6 [rec/s]\n",
      "15/09/21 06:37:29 INFO mapred.LocalJobRunner: Records R/W=9873/1 > sort > \n",
      "15/09/21 06:37:29 INFO mapreduce.Job:  map 87% reduce 0%\n",
      "15/09/21 06:37:29 INFO streaming.PipeMapRed: R/W/S=1400000/1111713/0 in:200000=1400000/7 [rec/s] out:158816=1111713/7 [rec/s]\n",
      "15/09/21 06:37:30 INFO streaming.PipeMapRed: R/W/S=1500000/1196723/0 in:214285=1500000/7 [rec/s] out:170961=1196727/7 [rec/s]\n",
      "15/09/21 06:37:30 INFO streaming.PipeMapRed: R/W/S=1600000/1275164/0 in:228571=1600000/7 [rec/s] out:182166=1275164/7 [rec/s]\n",
      "15/09/21 06:37:31 INFO streaming.PipeMapRed: R/W/S=1700000/1353588/0 in:212500=1700000/8 [rec/s] out:169198=1353588/8 [rec/s]\n",
      "15/09/21 06:37:31 INFO streaming.PipeMapRed: R/W/S=1800000/1438623/0 in:225000=1800000/8 [rec/s] out:179827=1438623/8 [rec/s]\n",
      "15/09/21 06:37:32 INFO streaming.PipeMapRed: R/W/S=1900000/1520419/0 in:211111=1900000/9 [rec/s] out:168935=1520419/9 [rec/s]\n",
      "15/09/21 06:37:32 INFO mapred.LocalJobRunner: Records R/W=9873/1 > sort > \n",
      "15/09/21 06:37:32 INFO mapreduce.Job:  map 96% reduce 0%\n",
      "15/09/21 06:37:32 INFO streaming.PipeMapRed: R/W/S=2000000/1598050/0 in:222222=2000000/9 [rec/s] out:177561=1598050/9 [rec/s]\n",
      "15/09/21 06:37:32 INFO streaming.PipeMapRed: Records R/W=2041952/1633521\n",
      "15/09/21 06:37:33 INFO streaming.PipeMapRed: R/W/S=2100000/1678194/0 in:210000=2100000/10 [rec/s] out:167819=1678194/10 [rec/s]\n",
      "15/09/21 06:37:33 INFO streaming.PipeMapRed: R/W/S=2200000/1759901/0 in:220000=2200000/10 [rec/s] out:175990=1759901/10 [rec/s]\n",
      "15/09/21 06:37:33 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/09/21 06:37:33 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/09/21 06:37:33 INFO mapred.Task: Task:attempt_local1347217305_0001_m_000000_0 is done. And is in the process of committing\n",
      "15/09/21 06:37:33 INFO mapred.LocalJobRunner: Records R/W=2041952/1633521 > sort\n",
      "15/09/21 06:37:33 INFO mapred.Task: Task 'attempt_local1347217305_0001_m_000000_0' done.\n",
      "15/09/21 06:37:33 INFO mapred.LocalJobRunner: Finishing task: attempt_local1347217305_0001_m_000000_0\n",
      "15/09/21 06:37:33 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "15/09/21 06:37:33 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "15/09/21 06:37:33 INFO mapred.LocalJobRunner: Starting task: attempt_local1347217305_0001_r_000000_0\n",
      "15/09/21 06:37:34 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "15/09/21 06:37:34 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "15/09/21 06:37:34 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@c4a3158\n",
      "15/09/21 06:37:34 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=371130368, maxSingleShuffleLimit=92782592, mergeThreshold=244946048, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "15/09/21 06:37:34 INFO reduce.EventFetcher: attempt_local1347217305_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "15/09/21 06:37:34 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1347217305_0001_m_000000_0 decomp: 38864678 len: 38864682 to MEMORY\n",
      "15/09/21 06:37:34 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "15/09/21 06:37:34 INFO reduce.InMemoryMapOutput: Read 38864678 bytes from map-output for attempt_local1347217305_0001_m_000000_0\n",
      "15/09/21 06:37:34 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 38864678, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->38864678\n",
      "15/09/21 06:37:34 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "15/09/21 06:37:34 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/21 06:37:34 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "15/09/21 06:37:34 INFO mapred.Merger: Merging 1 sorted segments\n",
      "15/09/21 06:37:34 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 38864665 bytes\n",
      "15/09/21 06:37:36 INFO reduce.MergeManagerImpl: Merged 1 segments, 38864678 bytes to disk to satisfy reduce memory limit\n",
      "15/09/21 06:37:36 INFO reduce.MergeManagerImpl: Merging 1 files, 38864682 bytes from disk\n",
      "15/09/21 06:37:36 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "15/09/21 06:37:36 INFO mapred.Merger: Merging 1 sorted segments\n",
      "15/09/21 06:37:36 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 38864665 bytes\n",
      "15/09/21 06:37:36 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/21 06:37:36 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/cjllop/Code/MIDS/W261/HW/W3/././reducer.py]\n",
      "15/09/21 06:37:36 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "15/09/21 06:37:36 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "15/09/21 06:37:37 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/21 06:37:37 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/21 06:37:37 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/21 06:37:37 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/21 06:37:37 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/21 06:37:37 INFO streaming.PipeMapRed: Records R/W=11092/1\n",
      "15/09/21 06:37:37 INFO streaming.PipeMapRed: R/W/S=100000/172351/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/21 06:37:37 INFO streaming.PipeMapRed: R/W/S=200000/370548/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/21 06:37:37 INFO streaming.PipeMapRed: R/W/S=300000/575368/0 in:300000=300000/1 [rec/s] out:575380=575380/1 [rec/s]\n",
      "15/09/21 06:37:38 INFO streaming.PipeMapRed: R/W/S=400000/771798/0 in:400000=400000/1 [rec/s] out:771817=771817/1 [rec/s]\n",
      "15/09/21 06:37:38 INFO streaming.PipeMapRed: R/W/S=500000/971821/0 in:500000=500000/1 [rec/s] out:971838=971838/1 [rec/s]\n",
      "15/09/21 06:37:38 INFO streaming.PipeMapRed: R/W/S=600000/1175229/0 in:600000=600000/1 [rec/s] out:1175246=1175246/1 [rec/s]\n",
      "15/09/21 06:37:38 INFO streaming.PipeMapRed: R/W/S=700000/1373637/0 in:700000=700000/1 [rec/s] out:1373655=1373655/1 [rec/s]\n",
      "15/09/21 06:37:38 INFO streaming.PipeMapRed: R/W/S=800000/1574216/0 in:800000=800000/1 [rec/s] out:1574224=1574224/1 [rec/s]\n",
      "15/09/21 06:37:38 INFO streaming.PipeMapRed: R/W/S=900000/1778546/0 in:900000=900000/1 [rec/s] out:1778567=1778567/1 [rec/s]\n",
      "15/09/21 06:37:38 INFO streaming.PipeMapRed: R/W/S=1000000/1974018/0 in:500000=1000000/2 [rec/s] out:987018=1974036/2 [rec/s]\n",
      "15/09/21 06:37:39 INFO streaming.PipeMapRed: R/W/S=1100000/2173457/0 in:550000=1100000/2 [rec/s] out:1086737=2173474/2 [rec/s]\n",
      "15/09/21 06:37:39 INFO streaming.PipeMapRed: R/W/S=1200000/2370776/0 in:600000=1200000/2 [rec/s] out:1185396=2370793/2 [rec/s]\n",
      "15/09/21 06:37:39 INFO streaming.PipeMapRed: R/W/S=1300000/2574601/0 in:650000=1300000/2 [rec/s] out:1287324=2574648/2 [rec/s]\n",
      "15/09/21 06:37:39 INFO streaming.PipeMapRed: R/W/S=1400000/2773815/0 in:700000=1400000/2 [rec/s] out:1386917=2773834/2 [rec/s]\n",
      "15/09/21 06:37:39 INFO streaming.PipeMapRed: R/W/S=1500000/2972388/0 in:750000=1500000/2 [rec/s] out:1486199=2972399/2 [rec/s]\n",
      "15/09/21 06:37:39 INFO streaming.PipeMapRed: R/W/S=1600000/3177049/0 in:800000=1600000/2 [rec/s] out:1588538=3177076/2 [rec/s]\n",
      "15/09/21 06:37:39 INFO streaming.PipeMapRed: R/W/S=1700000/3373925/0 in:566666=1700000/3 [rec/s] out:1124648=3373944/3 [rec/s]\n",
      "15/09/21 06:37:40 INFO mapred.LocalJobRunner: Records R/W=11092/1 > reduce\n",
      "15/09/21 06:37:40 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/09/21 06:37:40 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/09/21 06:37:40 INFO mapred.Task: Task:attempt_local1347217305_0001_r_000000_0 is done. And is in the process of committing\n",
      "15/09/21 06:37:40 INFO mapred.LocalJobRunner: Records R/W=11092/1 > reduce\n",
      "15/09/21 06:37:40 INFO mapred.Task: Task attempt_local1347217305_0001_r_000000_0 is allowed to commit now\n",
      "15/09/21 06:37:40 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1347217305_0001_r_000000_0' to hdfs://localhost:9000/user/cjllop/W261/Out/HW3_2/_temporary/0/task_local1347217305_0001_r_000000\n",
      "15/09/21 06:37:40 INFO mapred.LocalJobRunner: Records R/W=11092/1 > reduce\n",
      "15/09/21 06:37:40 INFO mapred.Task: Task 'attempt_local1347217305_0001_r_000000_0' done.\n",
      "15/09/21 06:37:40 INFO mapred.LocalJobRunner: Finishing task: attempt_local1347217305_0001_r_000000_0\n",
      "15/09/21 06:37:40 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "15/09/21 06:37:40 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "15/09/21 06:37:40 INFO mapreduce.Job: Job job_local1347217305_0001 completed successfully\n",
      "15/09/21 06:37:40 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=174749714\n",
      "\t\tFILE: Number of bytes written=214153400\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=6917034\n",
      "\t\tHDFS: Number of bytes written=38864676\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=31101\n",
      "\t\tMap output records=5448849\n",
      "\t\tMap output bytes=106311233\n",
      "\t\tMap output materialized bytes=38864682\n",
      "\t\tInput split bytes=121\n",
      "\t\tCombine input records=7652376\n",
      "\t\tCombine output records=3970309\n",
      "\t\tReduce input groups=1766782\n",
      "\t\tReduce shuffle bytes=38864682\n",
      "\t\tReduce input records=1766782\n",
      "\t\tReduce output records=3533564\n",
      "\t\tSpilled Records=5737091\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=90\n",
      "\t\tTotal committed heap usage (bytes)=464932864\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3458517\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=38864676\n",
      "15/09/21 06:37:40 INFO streaming.StreamJob: Output directory: ./W261/Out/HW3_2\n",
      "The first 5 results in the reducer output are:\n",
      "15/09/21 06:37:43 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "* DAI11153\t8\n",
      "\t\n",
      "* DAI11223\t155\n",
      "\t\n",
      "* DAI11238\t3\n",
      "\t\n",
      "* DAI11257\t1\n",
      "\t\n",
      "* DAI11261\t6\n",
      "\t\n",
      "* DAI11273\t1\n",
      "\t\n",
      "* DAI11290\t5\n",
      "\t\n",
      "* DAI11299\t2\n",
      "\t\n",
      "* DAI11375\t1\n",
      "\t\n",
      "* DAI11462\t8\n",
      "\t\n",
      "* DAI11541\t5\n",
      "\t\n",
      "* DAI11552\t8\n",
      "\t\n",
      "* DAI11555\t25\n",
      "\t\n",
      "* DAI11582\t1\n",
      "\t\n",
      "* DAI11613\t2\n",
      "\t\n",
      "* DAI11695\t5\n",
      "\t\n",
      "* DAI11707\t1\n",
      "\t\n",
      "* DAI11778\t117\n",
      "\t\n",
      "* DAI11927\t73\n",
      "\t\n",
      "* DAI11946\t1\n",
      "\t\n",
      "* DAI11995\t1\n",
      "\t\n",
      "* DAI12036\t6\n",
      "\t\n",
      "* DAI12139\t1\n",
      "\t\n",
      "* DAI12152\t1\n",
      "\t\n",
      "* DAI12275\t2\n",
      "\t\n",
      "* DAI12437\t16\n",
      "\t\n",
      "* DAI12448\t1\n",
      "\t\n",
      "* DAI12460\t1\n",
      "\t\n",
      "* DAI12521\t78\n",
      "\t\n",
      "* DAI12535\t1\n",
      "\t\n",
      "* DAI12558\t3\n",
      "\t\n",
      "* DAI12566\t3\n",
      "\t\n",
      "* DAI12680\t65\n",
      "\t\n",
      "* DAI12683\t2\n",
      "\t\n",
      "* DAI12698\t1\n",
      "\t\n",
      "* DAI12720\t49\n",
      "\t\n",
      "* DAI12740\t3\n",
      "\t\n",
      "* DAI12770\t3\n",
      "\t\n",
      "* DAI12821\t2\n",
      "\t\n",
      "* DAI12844\t2\n",
      "\t\n",
      "* DAI12853\t2\n",
      "\t\n",
      "* DAI12862\t21\n",
      "\t\n",
      "* DAI12870\t1\n",
      "\t\n",
      "* DAI12879\t1\n",
      "\t\n",
      "* DAI12900\t56\n",
      "\t\n",
      "* DAI12903\t1\n",
      "\t\n",
      "* DAI12919\t9\n",
      "\t\n",
      "* DAI12961\t7\n",
      "\t\n",
      "* DAI12976\t1\n",
      "\t\n",
      "* DAI13045\t1\n",
      "\t\n",
      "cat: Unable to write to output stream.\n"
     ]
    }
   ],
   "source": [
    "# HW3.2: TODO\n",
    "def HW3_2():\n",
    "    !hadoop jar /usr/local/Cellar/hadoop/2.6.0/libexec/share/hadoop/tools/lib/hadoop-streaming-2.6.0.jar \\\n",
    "    -Dmapreduce.job.maps=1 \\\n",
    "    -Dmapreduce.job.reduces=1 \\\n",
    "    -mapper ./mapper.py  \\\n",
    "    -combiner ./combiner.py \\\n",
    "    -reducer ./reducer.py \\\n",
    "    -input ./W261/In/HW3/ProductPurchaseData.txt -output ./W261/Out/HW3_2    \n",
    "#    -combiner ./combiner.py  \\\n",
    "HW3_2()\n",
    "\n",
    "# Note - to clean up this python notebook, we ran HW3_2 and are replacing with its output\n",
    "print \"The first 5 results in the reducer output are:\"\n",
    "!hadoop fs -cat ./W261/Out/HW3_2/part-00000 | head -n100\n",
    "#!hadoop fs -cat ./W261/Out/HW3_2/part-00000 | head -n5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/21 06:36:16 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/21 06:36:17 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted W261/Out/HW3_2\n"
     ]
    }
   ],
   "source": [
    "# This cell can be used to delete old output to allow re-run of any Hadoop script.\n",
    "#!hadoop fs -rm -r ./W261/Out/HW3_1_a\n",
    "#!hadoop fs -rm -r ./W261/Out/HW3_1_b\n",
    "!hadoop fs -rm -r ./W261/Out/HW3_2\n",
    "#!hadoop fs -rm -r ./W261/Out/HW2_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Formula is: I -> J = (I U J) / I\n",
    "\n",
    "# Pairs Approach | Note, I'm traveling to see a sick family member this weekend. While\n",
    "# stipes is faster and perhaps a better approach, I am making the design decision that, in \n",
    "# my case, time of the programmer is the resource that we need to take into account. You can \n",
    "# liken this to a business situation where a project cannot afford programing hours, but can\n",
    "# have a longer runtime :). In future weeks, I'll make different decisions.\n",
    "\n",
    "# To gain some efficiecny back, I'll write a quick combiner to help make things better.\n",
    "\n",
    "basket_inventory = {}\n",
    "x = 1\n",
    "with open(\"ProductPurchaseData.txt\") as infile:\n",
    "    for basket in infile:\n",
    "        # Get unique items in basket\n",
    "        basket = list(set(basket.rstrip('\\n').split()))\n",
    "\n",
    "        # Output a single record for each item so that we can use order inversion in our\n",
    "        # reduce side frequency count.\n",
    "        for item1 in basket:\n",
    "            for item2 in basket:\n",
    "                if item1 == item2:\n",
    "                    print \"*\\t{} 1\".format(item1)\n",
    "                else:\n",
    "                    print \"{}\\t{} 1\".format(item1, item2)\n",
    "                           \n",
    "        break\n",
    "        \n",
    "#        basket_inventory[basket] = basket_inventory.get(basket, 0) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>HW3.3</b>\n",
    "\n",
    "Benchmark your results using the pyFIM implementation of the Apriori algorithm\n",
    "(Apriori - Association Rule Induction / Frequent Item Set Mining implemented by Christian Borgelt). \n",
    "You can download pyFIM from here: \n",
    "\n",
    "http://www.borgelt.net/pyfim.html\n",
    "\n",
    "Comment on the results from both implementations (your Hadoop MapReduce of apriori versus pyFIM) \n",
    "in terms of results and execution times.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>HW3.4</b> (Conceptual Exercise)\n",
    "\n",
    "Suppose that you wished to perform the Apriori algorithm once again,\n",
    "though this time now with the goal of listing the top 5 rules with corresponding confidence scores \n",
    "in decreasing order of confidence score for itemsets of size 3 using Hadoop MapReduce.\n",
    "A rule is now of the form: \n",
    "\n",
    "(item1, item2) â‡’ item3 \n",
    "\n",
    "Recall that the Apriori algorithm is iterative for increasing itemset size,\n",
    "working off of the frequent itemsets of the previous size to explore \n",
    "ONLY the NECESSARY subset of a large combinatorial space. \n",
    "Describe how you might design a framework to perform this exercise.\n",
    "\n",
    "In particular, focus on the following:\n",
    "  â€” map-reduce steps required\n",
    "  - enumeration of item sets and filtering for frequent candidates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\"><b>Answer</b></span>:\n",
    "\n",
    "Invert sort:\n",
    "\n",
    "(*, a, b)\n",
    "(a, b, c)\n",
    "\n",
    "From this, can count all the a, b terms via the pre-sorted *, a, b\n",
    "\n",
    "Particianing question:\n",
    "One option: only one reducer.\n",
    "Another option: multiple passes through the MapReduce\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:silver\"><b>HW2.0.</b> What is a race condition in the context of parallel computation? Give an example.\n",
    "What is MapReduce?\n",
    "How does it differ from Hadoop?\n",
    "Which programming paradigm is Hadoop based on? Explain and give a simple example in code and show the code running.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\"><b>Answer:</b></span>\n",
    "\n",
    "A <b>race condition</b> is a condition where two threads must access the same data source. The programmer in this instance does not control which thread modifies the data first. As a result, it is possible that the ultimate end product of the code differs at random based on the order that the data source is accessed by threads.\n",
    "\n",
    "As an example, say the number \"3\" is stored on disk. Thread 1 wants to double the number, while Thread 2 wants to add 5 to the number. If Thread 1 acts first, the result is $(3 * 2) + 5 = 11$. If Thread 2 acts first, the result is $(3 + 5) * 2 = 16$. This sort of condition can cause all sorts of difficulties.\n",
    "\n",
    "<br>\n",
    "<b>MapReduce</b> is a problem solving framework/concept for embaressingly parallel data analysis. At its core, a problem is chunked and first processed in parallel by a number of mappers. Reducers then \"fold\" together the results of the mappers into a final output. <b>Hadoop</b> is a technical environment that, when combined with Hadoop File System allows a programmer to execute MapReduce jobs with ease. Hadoop programming is based on the MapReduce paradigm, which stems back to Functional Programming, or the idea of a programming language that can accept entire functions as an arguement.\n",
    "\n",
    "Hadoop is similar to functional programming because the \"mapper\" and \"reducer\" passed to Hadoop can be thought of as two functions being passed as arguements to the Hadoop program. Hadoop then executes these functions in the MapReduce framework. In fact, looking at the code examples throughout this assignment, we can see that the name of the mapper and reducer are passed as arguments to Hadoop Streaming.\n",
    "\n",
    "Below is a short example leveraging functional programming in Python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate Function\n",
    "def calculate( fun, *args ):\n",
    "    return fun( *args )\n",
    "\n",
    "# Add Function\n",
    "def add(a=0, b=0):\n",
    "    return a + b\n",
    "\n",
    "# Multiply Function\n",
    "def multiply(a=0, b=0):\n",
    "    return a * b\n",
    "\n",
    "# Demonstrate passing a function as an arguement to another function\n",
    "print calculate( add, 2, 3 )\n",
    "print calculate( multiply, 5, 3 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:silver\"><b>HW2.1. </b> Sort in Hadoop MapReduce\n",
    "Given as input: Records of the form (integer, â€œNAâ€), where integer is any integer, and â€œNAâ€ is just the empty string.\n",
    "Output: sorted key value pairs of the form (integer, â€œNAâ€); what happens if you have multiple reducers? Do you need additional steps? Explain.</span>\n",
    "\n",
    "<span style=\"color:silver\">Write code to generate N  random records of the form (integer, â€œNAâ€). Let N = 10,000.\n",
    "Write the python Hadoop streaming map-reduce job to perform this sort.</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\"><b>Answer:</b></span>\n",
    "\n",
    "If we have multiple reducers, the results will be sorted within each reducer - however they will not be globally sorted. While all the results for a given key wind up at the same reducer, reducers are not guaranteed to be given consecutive keys in the sort order. To correct for this, we could either force our system to send keys to the reducers in sorted chunks, or we could post-process all the reducer outputs to re-sort."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:silver\"><b>HW2.2.</b> Using the Enron data from HW1 and Hadoop MapReduce streaming, write mapper/reducer pair that  will determine the number of occurrences of a single, user-specified word. Examine the word â€œassistanceâ€ and report your results.</span>\n",
    "\n",
    "\n",
    "   <span style=\"color:silver\">To do so, make sure that</span>\n",
    "   \n",
    "   - <span style=\"color:silver\">mapper.py counts all occurrences of a single word, and</span>\n",
    "   - <span style=\"color:silver\">reducer.py collates the counts of the single word.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "\n",
    "findword = sys.argv[1]\n",
    "\n",
    "# input comes from standard input\n",
    "for full_email in sys.stdin:\n",
    "\n",
    "    # Parse out email body for processing. Find body using \"tab spam/ham tab\"\n",
    "    # use regex to strip out non alpha-numeric. \"don't\" will become \"dont\" which is fine.\n",
    "    keyword = re.findall(\"\\t[0-1]\\t\",full_email)[0]\n",
    "    email_id, is_spam_tabbed, email_body = full_email.partition(keyword)\n",
    "    email_body = re.sub('[^A-Za-z0-9\\s]+', '', email_body)\n",
    "\n",
    "    for word in email_body.split():\n",
    "        if word == findword:\n",
    "            print '%s\\t%s' % (word, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "\n",
    "# input comes from standard input\n",
    "for line in sys.stdin:\n",
    "    # parse the input we got from mapper.py\n",
    "    word, count = line.strip().split('\\t', 1)\n",
    "    count = int(count)\n",
    "\n",
    "    # take advantage of sorted keys\n",
    "    if current_word == word:\n",
    "        current_count += count\n",
    "    else:\n",
    "        if current_word:\n",
    "            # print result when word changes\n",
    "            print \"{}\\t{}\".format(current_word, current_count)\n",
    "        current_count = count\n",
    "        current_word = word\n",
    "\n",
    "# print final word\n",
    "if current_word == word:\n",
    "    print \"{}\\t{}\".format(current_word, current_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Move input file to HDFS\n",
    "!hdfs dfs -put ./enronemail_1h.txt ./W261/In/HW2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# HW2.2: Execute a job using Hadoop Streaming to search the input file for a user-specified word\n",
    "def HW2_2(term=\"assistance\"):    \n",
    "    import re\n",
    "\n",
    "    !hadoop jar /usr/local/Cellar/hadoop/2.6.0/libexec/share/hadoop/tools/lib/hadoop-streaming-2.6.0.jar \\\n",
    "    -Dmapreduce.job.maps=10 \\\n",
    "    -Dmapreduce.job.reduces=1 \\\n",
    "    -files ./mapper.py,./reducer.py \\\n",
    "    -mapper './mapper.py {term}' \\\n",
    "    -reducer ./reducer.py \\\n",
    "    -input ./W261/In/HW2/enronemail_1h.txt -output ./W261/Out/HW2_2\n",
    "    \n",
    "    print\n",
    "    print \"Display head of file to prove run worked:\"\n",
    "    !hadoop fs -cat ./W261/Out/HW2_2/part-00000 | head -n15\n",
    "\n",
    "    # Crosscheck results (data is small enough to use RE in python)\n",
    "    print \"Running Crosscheck...\"\n",
    "    with open (\"enronemail_1h.txt\", \"r\") as myfile:\n",
    "        print \"Check Result:\", len(re.findall(term,myfile.read()))\n",
    "        \n",
    "HW2_2(term=\"assistance\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:silver\"><b>HW2.3.</b> Using the Enron data from HW1 and Hadoop MapReduce, write  a mapper/reducer pair that\n",
    "   will classify the email messages by a single, user-specified word. Examine the word â€œassistanceâ€ and report your results. To do so, make sure that</span>\n",
    "   \n",
    "   - <span style=\"color:silver\">mapper.py</span>\n",
    "   - <span style=\"color:silver\">reducer.py </span>\n",
    "\n",
    "   <span style=\"color:silver\">performs a single word multinomial Naive Bayes classification.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "\n",
    "# Get search term(s)\n",
    "findword = sys.argv[1]\n",
    "\n",
    "for full_email in sys.stdin:\n",
    "#with open (findfile, \"r\") as myfile:\n",
    "#    for full_email in myfile:\n",
    "    # Empty dictionary\n",
    "    term_hits = {}\n",
    "\n",
    "    # Spam classification\n",
    "    is_spam = re.findall(\"\\t([0-1])\\t\",full_email)[0]\n",
    "\n",
    "    # Parse out email body for processing. Find body using \"tab spam/ham tab\"\n",
    "    # use regex to strip out non alpha-numeric. \"don't\" will become \"dont\" which is fine for classifying.\n",
    "    keyword = re.findall(\"\\t[0-1]\\t\",full_email)[0]\n",
    "    email_id, is_spam_tabbed, email_body = full_email.partition(keyword)\n",
    "    email_body = re.sub('[^A-Za-z0-9\\s]+', '', email_body)\n",
    "    # Must process search query and email bodies the same\n",
    "    findword = re.sub('[^A-Za-z0-9\\s]+', '', findword)\n",
    "    email_len = len(email_body.split())\n",
    "\n",
    "    # Build counts of term words. \n",
    "    for word in list(set(email_body.split())):\n",
    "        term_hits[word] = len(re.findall(word,email_body))\n",
    "\n",
    "    # Print as tuple with unique splitter \"|||\"\n",
    "    print \"{}\\t{} ||| {} ||| {}\".format(email_id, is_spam, email_len, term_hits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import ast\n",
    "import math\n",
    "\n",
    "# Get search term(s)\n",
    "findword = sys.argv[1]\n",
    "search_terms = findword.split()\n",
    "\n",
    "# Parse all mapper results into a list so we can loop through again to predict after \n",
    "# looping through to train the model\n",
    "mapper_results = []\n",
    "for line in sys.stdin:\n",
    "    mapper_results.append(line)\n",
    "\n",
    "spam_term_counts = {}\n",
    "ham_term_counts = {}\n",
    "word_prob_given_spam = {}\n",
    "word_prob_given_ham = {}\n",
    "spam_count = 0\n",
    "ham_count = 0\n",
    "spam_len = 0\n",
    "ham_len = 0\n",
    "distinct_term_list = []\n",
    "\n",
    "# Open each file and build Multinomial Naive Bayes model\n",
    "for processed_email in mapper_results:\n",
    "    # Read in tuples created by mapper\n",
    "    email_id, processed_email = processed_email.split(\"\\t\")\n",
    "    processed_email = processed_email.split(\" ||| \")\n",
    "    is_spam = int(processed_email[0])\n",
    "    email_len = int(processed_email[1])\n",
    "    count_dict = ast.literal_eval(processed_email[2])\n",
    "\n",
    "    # Build counts for spam and ham definitions.\n",
    "    if is_spam:\n",
    "        for key, value in count_dict.iteritems():\n",
    "            spam_term_counts[key] = spam_term_counts.get(key, 0) + value\n",
    "        spam_count += 1\n",
    "        spam_len += email_len\n",
    "    else:\n",
    "        for key, value in count_dict.iteritems():\n",
    "            ham_term_counts[key] = ham_term_counts.get(key, 0) + value\n",
    "        ham_count += 1\n",
    "        ham_len += email_len\n",
    "\n",
    "    distinct_term_list = list(set(distinct_term_list + count_dict.keys()))\n",
    "\n",
    "# Calculate our priors based on the overall ratio of spam to ham\n",
    "spam_prior = float(spam_count) / (spam_count + ham_count)\n",
    "ham_prior = 1 - spam_prior\n",
    "spam_prior = math.log10(spam_prior)\n",
    "ham_prior = math.log10(ham_prior)\n",
    "\n",
    "# Calculate our conditional probabilites for the search term using MNB formula\n",
    "#     term_given_spam = (spam_term_count + 1 for smoothing) / (total count of spam words + total distinct vocab size)\n",
    "distinct_term_count = len(distinct_term_list)\n",
    "\n",
    "for term in search_terms:\n",
    "    word_prob_given_spam[term] = math.log10((spam_term_counts.get(term,0) + 1.0) / (float(spam_len) + distinct_term_count))\n",
    "    word_prob_given_ham[term] = math.log10((ham_term_counts.get(term,0) + 1.0) / (float(ham_len) + distinct_term_count))\n",
    "\n",
    "# Now let's predict!\n",
    "accuracy = []\n",
    "for processed_email in mapper_results:\n",
    "    # Defaults\n",
    "    pred_spam = 0\n",
    "    spam_prediction = spam_prior\n",
    "    ham_prediction = ham_prior\n",
    "\n",
    "    # Read in tuples created by mapper\n",
    "    email_id, processed_email = processed_email.split(\"\\t\")\n",
    "    processed_email = processed_email.split(\" ||| \")\n",
    "    is_spam = int(processed_email[0])\n",
    "    count_dict = ast.literal_eval(processed_email[2])\n",
    "\n",
    "    # Read in counts to use in prediction\n",
    "    for term in word_prob_given_spam.keys():\n",
    "        # Calculate the probability for each class\n",
    "        spam_prediction += (word_prob_given_spam[term] * count_dict.get(term, 0))\n",
    "        ham_prediction += (word_prob_given_ham[term] * count_dict.get(term, 0))\n",
    "\n",
    "    # Pick the higher probability\n",
    "    if spam_prediction > ham_prediction: \n",
    "        pred_spam = 1\n",
    "\n",
    "    # Store accuracy in a list\n",
    "    accuracy.append(1*(pred_spam==is_spam))\n",
    "\n",
    "    # Print predictions to results file\n",
    "    print '{}\\t{}\\t{}'.format(email_id, is_spam, pred_spam)\n",
    "\n",
    "# Print accuracy\n",
    "sys.stderr.write(\"\\nSpam Probs: {}\\n\".format(word_prob_given_spam))\n",
    "sys.stderr.write(\"Ham Probs: {}\\n\".format(word_prob_given_ham))\n",
    "sys.stderr.write(\"Accuracy = {:.2f}\\n\".format(float(sum(accuracy))/len(accuracy)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# HW2.3: Predict via MNBusing Hadoop Streaming for a user-specified word\n",
    "def HW2_3(term=\"assistance\"):\n",
    "    !hadoop jar /usr/local/Cellar/hadoop/2.6.0/libexec/share/hadoop/tools/lib/hadoop-streaming-2.6.0.jar \\\n",
    "    -Dmapreduce.job.maps=10 \\\n",
    "    -Dmapreduce.job.reduces=1 \\\n",
    "    -files ./mapper.py,./reducer.py \\\n",
    "    -mapper './mapper.py {term}' \\\n",
    "    -reducer './reducer.py {term}' \\\n",
    "    -input ./W261/In/HW2/enronemail_1h.txt -output ./W261/Out/HW2_3\n",
    "    \n",
    "    print\n",
    "    print \"Display head of file to prove run worked:\"\n",
    "    !hadoop fs -cat ./W261/Out/HW2_3/part-00000 | head -n15\n",
    "        \n",
    "HW2_3(term=\"assistance\")\n",
    "\n",
    "# Note - output shows same accuracy of 60% that we saw in HW1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:silver\"><b>HW2.4.</b> Using the Enron data from HW1 and in the Hadoop MapReduce framework, write  a mapper/reducer pair that\n",
    "   will classify the email messages using multinomial Naive Bayes Classifier using a list of one or more user-specified words. Examine the words â€œassistanceâ€, â€œvaliumâ€, and â€œenlargementWithATypoâ€ and report your results\n",
    "   To do so, make sure that</span>\n",
    "\n",
    "   - <span style=\"color:silver\">mapper.py </span>\n",
    "   - <span style=\"color:silver\">reducer.py </span>\n",
    "\n",
    "   <span style=\"color:silver\">performs the multiple-word multinomial Naive Bayes classification via the chosen list.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# HW2.4: Predict via MNBusing Hadoop Streaming for multiple user-specified words\n",
    "# Note - the solution program to HW2.3 can already do this. We just need to give it more terms.\n",
    "def HW2_4(term=\"assistance valium enlargementWithATypo\"):\n",
    "    !hadoop jar /usr/local/Cellar/hadoop/2.6.0/libexec/share/hadoop/tools/lib/hadoop-streaming-2.6.0.jar \\\n",
    "    -Dmapreduce.job.maps=10 \\\n",
    "    -Dmapreduce.job.reduces=1 \\\n",
    "    -files ./mapper.py,./reducer.py \\\n",
    "    -mapper './mapper.py \"{term}\"' \\\n",
    "    -reducer './reducer.py \"{term}\"' \\\n",
    "    -input ./W261/In/HW2/enronemail_1h.txt -output ./W261/Out/HW2_4\n",
    "    \n",
    "    print\n",
    "    print \"Display head of file to prove run worked:\"\n",
    "    !hadoop fs -cat ./W261/Out/HW2_4/part-00000 | head -n15\n",
    "        \n",
    "HW2_4(term=\"assistance valium enlargementWithATypo\")\n",
    "\n",
    "# Note - output shows same accuracy of 63% that we saw in HW1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:silver\"><b>HW2.5.</b> Using the Enron data from HW1 an in the  Hadoop MapReduce framework, write  a mapper/reducer for a multinomial Naive Bayes Classifier that\n",
    "   will classify the email messages using  words present. Also drop words with a frequency of less than three (3). How does it affect the misclassifcation error of learnt naive multinomial Bayesian Classifiers on the training dataset:</span>\n",
    "\n",
    "\n",
    "<span style=\"color:green\"><b>Answer:</b></span>\n",
    "The full word (>=3) classifier drastically increases accuracy from 0.63 to 0.90. This is a misclassification error rate of 10%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import ast\n",
    "import math\n",
    "\n",
    "# Get search term(s)\n",
    "findword = sys.argv[1]\n",
    "search_terms = findword.split()\n",
    "\n",
    "# Parse all mapper results into a list so we can loop through again to predict after \n",
    "# looping through to train the model\n",
    "mapper_results = []\n",
    "for line in sys.stdin:\n",
    "    mapper_results.append(line)\n",
    "\n",
    "spam_term_counts = {}\n",
    "ham_term_counts = {}\n",
    "word_prob_given_spam = {}\n",
    "word_prob_given_ham = {}\n",
    "spam_count = 0\n",
    "ham_count = 0\n",
    "spam_len = 0\n",
    "ham_len = 0\n",
    "distinct_term_list = []\n",
    "\n",
    "# Open each file and build Multinomial Naive Bayes model\n",
    "for processed_email in mapper_results:\n",
    "    # Read in tuples created by mapper\n",
    "    email_id, processed_email = processed_email.split(\"\\t\")\n",
    "    processed_email = processed_email.split(\" ||| \")\n",
    "    is_spam = int(processed_email[0])\n",
    "    email_len = int(processed_email[1])\n",
    "    count_dict = ast.literal_eval(processed_email[2])\n",
    "\n",
    "    # Build counts for spam and ham definitions.\n",
    "    if is_spam:\n",
    "        for key, value in count_dict.iteritems():\n",
    "            spam_term_counts[key] = spam_term_counts.get(key, 0) + value\n",
    "        spam_count += 1\n",
    "        spam_len += email_len\n",
    "    else:\n",
    "        for key, value in count_dict.iteritems():\n",
    "            ham_term_counts[key] = ham_term_counts.get(key, 0) + value\n",
    "        ham_count += 1\n",
    "        ham_len += email_len\n",
    "\n",
    "    distinct_term_list = list(set(distinct_term_list + count_dict.keys()))\n",
    "\n",
    "# Calculate our priors based on the overall ratio of spam to ham\n",
    "spam_prior = float(spam_count) / (spam_count + ham_count)\n",
    "ham_prior = 1 - spam_prior\n",
    "spam_prior = math.log10(spam_prior)\n",
    "ham_prior = math.log10(ham_prior)\n",
    "\n",
    "# Calculate our conditional probabilites for the search term using MNB formula\n",
    "#     term_given_spam = (spam_term_count + 1 for smoothing) / (total count of spam words + total distinct vocab size)\n",
    "distinct_term_count = len(distinct_term_list)\n",
    "\n",
    "# Added logic for this problem - replace search_terms with all terms if we were given a \"*\"\n",
    "if search_terms[0] == \"*\":\n",
    "    search_terms = distinct_term_list\n",
    "\n",
    "for term in search_terms:\n",
    "    if (spam_term_counts.get(term,0) + ham_term_counts.get(term,0)) >= 3:\n",
    "        word_prob_given_spam[term] = math.log10((spam_term_counts.get(term,0) + 1.0) / (float(spam_len) + distinct_term_count))\n",
    "        word_prob_given_ham[term] = math.log10((ham_term_counts.get(term,0) + 1.0) / (float(ham_len) + distinct_term_count))\n",
    "\n",
    "# Now let's predict!\n",
    "accuracy = []\n",
    "for processed_email in mapper_results:\n",
    "    # Defaults\n",
    "    pred_spam = 0\n",
    "    spam_prediction = spam_prior\n",
    "    ham_prediction = ham_prior\n",
    "\n",
    "    # Read in tuples created by mapper\n",
    "    email_id, processed_email = processed_email.split(\"\\t\")\n",
    "    processed_email = processed_email.split(\" ||| \")\n",
    "    is_spam = int(processed_email[0])\n",
    "    count_dict = ast.literal_eval(processed_email[2])\n",
    "\n",
    "    # Read in counts to use in prediction\n",
    "    for term in word_prob_given_spam.keys():\n",
    "        # Calculate the probability for each class\n",
    "        spam_prediction += (word_prob_given_spam[term] * count_dict.get(term, 0))\n",
    "        ham_prediction += (word_prob_given_ham[term] * count_dict.get(term, 0))\n",
    "\n",
    "    # Pick the higher probability\n",
    "    if spam_prediction > ham_prediction: \n",
    "        pred_spam = 1\n",
    "\n",
    "    # Store accuracy in a list\n",
    "    accuracy.append(1*(pred_spam==is_spam))\n",
    "\n",
    "    # Print predictions to results file\n",
    "    print '{}\\t{}\\t{}'.format(email_id, is_spam, pred_spam)\n",
    "\n",
    "# Print accuracy\n",
    "sys.stderr.write(\"Accuracy = {:.2f}\\n\".format(float(sum(accuracy))/len(accuracy)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# HW2.5: Predict via MNBusing Hadoop Streaming for multiple user-specified words\n",
    "# Note - the mapper program to HW2.3 can already do this, so we only created a new reducer\n",
    "def HW2_5(term=\"*\"):\n",
    "    !hadoop jar /usr/local/Cellar/hadoop/2.6.0/libexec/share/hadoop/tools/lib/hadoop-streaming-2.6.0.jar \\\n",
    "    -Dmapreduce.job.maps=10 \\\n",
    "    -Dmapreduce.job.reduces=1 \\\n",
    "    -files ./mapper.py,./reducer.py \\\n",
    "    -mapper './mapper.py \"{term}\"' \\\n",
    "    -reducer './reducer.py \"{term}\"' \\\n",
    "    -input ./W261/In/HW2/enronemail_1h.txt -output ./W261/Out/HW2_5\n",
    "    \n",
    "    print\n",
    "    print \"Display head of file to prove run worked:\"\n",
    "    !hadoop fs -cat ./W261/Out/HW2_5/part-00000 | head -n15\n",
    "        \n",
    "HW2_5(term=\"*\")\n",
    "\n",
    "# Note - output shows 90% accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This cell can be used to delete old output to allow re-run of any Hadoop script.\n",
    "#!hadoop fs -rm -r ./W261/Out/HW2_1\n",
    "#!hadoop fs -rm -r ./W261/Out/HW2_2\n",
    "#!hadoop fs -rm -r ./W261/Out/HW2_3\n",
    "#!hadoop fs -rm -r ./W261/Out/HW2_4\n",
    "#!hadoop fs -rm -r ./W261/Out/HW2_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This concludes HW 2.0. Thanks for reading!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "item_inventory = {}\n",
    "\n",
    "with open(\"ProductPurchaseData.txt\") as infile:\n",
    "    for line in infile:\n",
    "        for item in line.rstrip('\\n').split():\n",
    "            item_inventory[item] = item_inventory.get(item, 0) + 1\n",
    "            \n",
    "x = 1\n",
    "for item, inventory in item_inventory.iteritems():\n",
    "    while x < 5:\n",
    "        print \"{}\\t{}\".format(item, inventory)\n",
    "        x += 1\n",
    "        \n",
    "sample = [\"ELE39978\t2\",\"ELE39978\t4\",\"ELE39978\t5\",\"ELE39978\t2\",\"CJE39978\t2\",\"QJE39978\t7\",\"QJE39978\t12\"]\n",
    "\n",
    "unique_item_count = 0\n",
    "current_item_count = 0\n",
    "current_item = \"\"\n",
    "\n",
    "for line in sample:\n",
    "    line = line.rstrip('\\n').split()\n",
    "    if current_item == line[0]:\n",
    "        # If same item, add to count\n",
    "        current_item_count += int(line[1])\n",
    "    else:\n",
    "        # If new item, print, increment unique, restart count\n",
    "        if unique_item_count > 0:\n",
    "            print unique_item_count, current_item, current_item_count\n",
    "        unique_item_count += 1\n",
    "        current_item_count = int(line[1])\n",
    "        current_item = line[0]\n",
    "        \n",
    "# Print final row\n",
    "print unique_item_count, current_item, current_item_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "basket_inventory = {}\n",
    "\n",
    "with open(\"ProductPurchaseData.txt\") as infile:\n",
    "    for basket in infile:\n",
    "        basket = basket.rstrip('\\n')\n",
    "        basket_inventory[basket] = basket_inventory.get(basket, 0) + 1\n",
    "            \n",
    "#for basket, inventory in basket_inventory.iteritems():\n",
    "#    print \"{}\\t{}\".format(basket, inventory)\n",
    "#    print \"{}\\t{}\".format(basket.replace(\" \", \"_\"), inventory)\n",
    "\n",
    "unique_basket_count = 0\n",
    "current_basket_count = 0\n",
    "current_basket = \"\"\n",
    "largest_basket = []\n",
    "largest_basket_size = 0\n",
    "\n",
    "test = [\"FRO44973 ELE90882 \t1\", \"FRO44973 ELE90882 \t3\", \"GRO24246 ELE26032 DAI48891 ELE78169 \t1\"\n",
    "       , \"GRO24246 ELE26032 DAI48891 ELE78170 \t1\", \"GRO24246 ELE26032 DAI48891 ELE78170 \t1\"]\n",
    "#for line in sys.stdin:\n",
    "for line in test:\n",
    "    line = line.rstrip('\\n').split('\\t')\n",
    "    if current_basket == line[0]:\n",
    "        # If same item, add to count\n",
    "        current_basket_count += int(line[1])\n",
    "    else:\n",
    "        # If new item, print, increment unique, restart count\n",
    "        if unique_basket_count > 0:\n",
    "            print unique_basket_count, current_basket, current_basket_count\n",
    "        unique_basket_count += 1\n",
    "        current_basket_count = int(line[1])\n",
    "        current_basket = line[0]\n",
    "        \n",
    "    # Track the maximum basket size\n",
    "    if len(current_basket.split()) > largest_basket_size:\n",
    "        largest_basket_size = len(current_basket.split())\n",
    "        largest_basket = [current_basket]\n",
    "    elif len(current_basket.split()) == largest_basket_size:\n",
    "        largest_basket.append(current_basket)\n",
    "        \n",
    "print unique_item_count, current_item, current_item_count\n",
    "print \"The largest basket(s) have {} items: {}\".format(largest_basket_size, largest_basket)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
