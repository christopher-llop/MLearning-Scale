{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIDS W261 Machine Learning At Scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Christopher Llop | christopher.llop@ischool.berkeley.edu <br>\n",
    "Week 3 | Submission Date: 9/22/2015\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>HW3.0.</b>\n",
    "\n",
    "1. What is a merge sort? Where is it used in Hadoop?\n",
    "2. How is  a combiner function in the context of Hadoop? \n",
    "3. Give an example where it can be used and justify why it should be used in the context of this problem.\n",
    "4. What is the Hadoop shuffle?\n",
    "5. What is the Apriori algorithm? Describe an example use in your domain of expertise. \n",
    "6. Define confidence and lift."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<span style=\"color:green\"><b>Answer:</b></span>\n",
    "1. A merge sort occurs when we are combining several datasets that are already sorted. Because the data has been sorted, we can simply look at the \"top\" element of every dataset to see which one comes next in order. The combined dataset can easily be created, in sorted order, by repeating the process of evaluating the top sorted element of the datasets being merged. This can occurs in Hadoop when data is sent from multiple mappers to a single reducer. Ultimately, each reducer will get a sorted list of the keys assigned to that reducer. However, these keys come from multiple mappers. By using a merge sort, Hadoop can more quickly present each reducers with the correct keys in sorted order.\n",
    "\n",
    "2. A combiner in Hadoop occurs after the mapper but before the reducer. While it is useful to think of the combiner occuring directly after mapping, in some instances combiners can run reducer-side. The combiner will take records leaving the mapper and combine certain records together to decrease the load over the network. We are not assured that the combiner will run on any given data, so the combiner's output must be in the same format as the mappers output so that the reducer can input data from either a combiner or mapper seamlessly. Note that a different kind of combiner, called an in-memory combiner, can be set up <i>within</i> a mapper itself. In this instance, the mapper combines records in memory and then outputs periodically (perhaps when memory is running low).\n",
    "\n",
    "3. One example of a combiner is in the basic \"word count\" problem. If a mapper outputs a key-value pair for each word encountered, with a count of \"1\", the combiner can then sum these records together. For example, multiple counts of \"dog 1\" can be combined into \"dog 5\". Note that combiner functions must be communiative and associative in a way that enables the mathematics of the reducer to function correctly even though values have been aggregated. An average, for example, cannot be done in the combiner (without some work arounds) because the average of averages is not the same as the average of an entire series.\n",
    "\n",
    "4. The Hadoop shuffle refers to the many steps that occur between the mapper and reducer (and sometimes the combiner). The shuffle has been refered to as the \"heart and soul\" of Hadoop, because this is the process that takes the parallelized mapper output, groups by key, and presents the correct keys to each reducer. It is possible to optimize during this process - for example, by splitting the workload of reducers so that heavily-used keys are not sent to the same reducer, we can reduce stragglers. This entire process of optimization and network traffic is included in the phrase \"Hadoop shuffle\".\n",
    "\n",
    "5. The Apriori algorithem is a method of identifying groups of items that are popular in a basket of goods. These nouns can be generalized to any situation where we are finding groups of things similar to items that are subsets of things similar to baskets. In the Apriori algorithem, we are given a required level of support (say, N = 100) that we must have to consider a group of items meaningful. To save processing time, we first create a frequency count for any ONE item. For all items that occur in greater frequency than our threshold, we then cound PAIRS of items (both parts of the pair must have N > threshold). Because we are subsetting, we need to store data for many fewer pairs than we otherwise would. We can continue this algorytehm forward, building triples off of pairs that pass the threshold, etc.  Ultimately, we can then use our final set of items to calculate confidence scores. That is, given item X, what are the chances a customer would also want item Y? (Pr(Y|X)) or Given items X and Y, what are the chances of Z (Pr(Z|(X & Y))). For more information on this calculation, see the code implemented below. In my domain, this method could be generalized to investigating what groups of academic experts are used together in a lawsuit (though I'm sure many other uses exist).\n",
    "\n",
    "6. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:silver\"><b>HW3.1. </b></span>\n",
    "\n",
    "<span style=\"color:silver\">Product Recommendations: The action or practice of selling additional products or services to existing customers is called cross-selling. Giving product recommendation is one of the examples of cross-selling that are frequently used by online retailers. One simple method to give product recommendations is to recommend products that are frequently browsed together by the customers.</span>\n",
    "\n",
    "<span style=\"color:silver\">Suppose we want to recommend new products to the customer based on the products they have already browsed on the online website. Write a program using the A-priori algorithm to find products which are frequently browsed together. Fix the support to s = 100  (i.e. product pairs need to occur together at least 100 times to be considered frequent) and find itemsets of size 2 and 3. (Note - Jake told us not to do this via the Google Group).</span>\n",
    "\n",
    "<span style=\"color:silver\">Use the online browsing behavior dataset at: </span>\n",
    "\n",
    "https://www.dropbox.com/s/zlfyiwa70poqg74/ProductPurchaseData.txt?dl=0\n",
    "\n",
    "<span style=\"color:silver\">Each line in this dataset represents a browsing session of a customer. On each line, each string <br>\n",
    "of 8 characters represents the id of an item browsed during that session. The items are separated <br>\n",
    "by spaces.</span>\n",
    "\n",
    "<span style=\"color:silver\">Do some exploratory data analysis of this dataset. </span>\n",
    "\n",
    "<span style=\"color:silver\">Report your findings such as number of unique products; largest basket, etc. using Hadoop Map-Reduce.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\"><b>Answer:</b></span>\n",
    "We were asked to preform some EDA. I've decided to look at the number of unique products, the largest basket, the freqency of each basket size, and the frequency of each product. The code below solves each of these problems. There are two MapReduce runs. The first calculates the number of unique products and the product frequency. The second finds the largest basket (it turns out there is a tie) and counts the frequency of each basket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of unique products and product frequency can be solved together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "item_inventory = {}\n",
    "\n",
    "for line in sys.stdin:\n",
    "    for item in line.rstrip('\\n').split():\n",
    "        item_inventory[item] = item_inventory.get(item, 0) + 1\n",
    "            \n",
    "for item, inventory in item_inventory.iteritems():\n",
    "    print \"{}\\t{}\".format(item, inventory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "unique_item_count = 0\n",
    "current_item_count = 0\n",
    "current_item = \"\"\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.rstrip('\\n').split()\n",
    "    if current_item == line[0]:\n",
    "        # If same item, add to count\n",
    "        current_item_count += int(line[1])\n",
    "    else:\n",
    "        # If new item, print, increment unique, restart count\n",
    "        if unique_item_count > 0:\n",
    "            print current_item, current_item_count\n",
    "        unique_item_count += 1\n",
    "        current_item_count = int(line[1])\n",
    "        current_item = line[0]\n",
    "        \n",
    "# Print final row of counts\n",
    "print current_item, current_item_count\n",
    "\n",
    "# Finally, print the number of unique items (will be on last row of reducer output)\n",
    "print unique_item_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use chmod for permissions\n",
    "!chmod a+x mapper.py\n",
    "!chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Move files and make directory\n",
    "!hadoop fs -mkdir ./W261/In/HW3\n",
    "!hdfs dfs -put ./ProductPurchaseData.txt ./W261/In/HW3/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# HW3.1_a: Execute a job using Hadoop Streaming to generate 10,000 random integers and sort them.\n",
    "def HW3_1a():\n",
    "    !hadoop jar /usr/local/Cellar/hadoop/2.6.0/libexec/share/hadoop/tools/lib/hadoop-streaming-2.6.0.jar \\\n",
    "    -Dmapreduce.job.maps=10 \\\n",
    "    -Dmapreduce.job.reduces=1 \\\n",
    "    -files ./mapper.py,./reducer.py \\\n",
    "    -mapper ./mapper.py  \\\n",
    "    -reducer ./reducer.py \\\n",
    "    -input ./W261/In/HW3/ProductPurchaseData.txt -output ./W261/Out/HW3_1_a    \n",
    "    \n",
    "#HW3_1a()\n",
    "\n",
    "# Note - to clean up this python notebook, we ran HW3_1a and are replacing with its output\n",
    "print \"The first 5 results in the reducer output are:\"\n",
    "!hadoop fs -cat ./W261/Out/HW3_1_a/part-00000 | head -n5\n",
    "\n",
    "print\n",
    "print \"The total number of unique items is:\"\n",
    "!hadoop fs -cat ./W261/Out/HW3_1_a/part-00000 | tail -n1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Largest basket and frequency of basket counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "basket_inventory = {}\n",
    "\n",
    "for basket in sys.stdin:\n",
    "#    for basket in line:\n",
    "    basket = basket.rstrip('\\n')\n",
    "    basket_inventory[basket] = basket_inventory.get(basket, 0) + 1\n",
    "            \n",
    "# Note - this code assumes we can fit the ENTIRE document in memory. This isnt' best practice.\n",
    "#   I should really updated this code to check memory and emit whenever memory hits a certain point.\n",
    "for basket, inventory in basket_inventory.iteritems():\n",
    "    print \"{}\\t{}\".format(basket, inventory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "unique_basket_count = 0\n",
    "current_basket_count = 0\n",
    "current_basket = \"\"\n",
    "largest_basket = []\n",
    "largest_basket_size = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.rstrip('\\n').split('\\t')\n",
    "    if current_basket == line[0]:\n",
    "        # If same item, add to count\n",
    "        current_basket_count += int(line[1])\n",
    "    else:\n",
    "        # If new item, print, increment unique, restart count\n",
    "        if unique_basket_count > 0:\n",
    "            print current_basket.rstrip('\\n'), current_basket_count\n",
    "        unique_basket_count += 1\n",
    "        current_basket_count = int(line[1])\n",
    "        current_basket = line[0]\n",
    "        \n",
    "    # Track the maximum basket size\n",
    "    if len(current_basket.split()) > largest_basket_size:\n",
    "        largest_basket_size = len(current_basket.split())\n",
    "        largest_basket = [current_basket]\n",
    "    elif len(current_basket.split()) == largest_basket_size:\n",
    "        largest_basket.append(current_basket)\n",
    "        \n",
    "print current_basket.rstrip('\\n'), current_basket_count\n",
    "print \"The largest basket(s) have {} items. There are {} such baskets: {}\".format(\n",
    "    largest_basket_size, len(largest_basket), largest_basket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# HW3.1_b: Execute a job using Hadoop Streaming to generate 10,000 random integers and sort them.\n",
    "def HW3_1b():\n",
    "    !hadoop jar /usr/local/Cellar/hadoop/2.6.0/libexec/share/hadoop/tools/lib/hadoop-streaming-2.6.0.jar \\\n",
    "    -Dmapreduce.job.maps=10 \\\n",
    "    -Dmapreduce.job.reduces=1 \\\n",
    "    -files ./mapper.py,./reducer.py \\\n",
    "    -mapper ./mapper.py  \\\n",
    "    -reducer ./reducer.py \\\n",
    "    -input ./W261/In/HW3/ProductPurchaseData.txt -output ./W261/Out/HW3_1_b    \n",
    "    \n",
    "#HW3_1b()\n",
    "\n",
    "# Note - to clean up this python notebook, we ran HW3_1b and are replacing with its output\n",
    "print \"The first 5 results in the reducer output are:\"\n",
    "!hadoop fs -cat ./W261/Out/HW3_1_b/part-00000 | head -n5\n",
    "\n",
    "print\n",
    "print \"Printing largest baskets....\"\n",
    "!hadoop fs -cat ./W261/Out/HW3_1_b/part-00000 | tail -n1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:silver\"><b>HW3.2.</b> (Computationally prohibitive but then again Hadoop can handle this)</span>\n",
    "\n",
    "<span style=\"color:silver\">Note: for this part the writeup will require a specific rule ordering but the program need not sort the output.</span>\n",
    "\n",
    "<span style=\"color:silver\">List the top 5 rules with corresponding confidence scores in decreasing order of confidence score \n",
    "for frequent (100>count) itemsets of size 2. </span>\n",
    "<span style=\"color:silver\">A rule is of the form: </span>\n",
    "\n",
    "<span style=\"color:silver\">(item1) ⇒ item2.</span>\n",
    "\n",
    "<span style=\"color:silver\">Fix the ordering of the rule lexicographically (left to right), \n",
    "and break ties in confidence (between rules, if any exist) \n",
    "by taking the first ones in lexicographically increasing order. \n",
    "Use Hadoop MapReduce to complete this part of the assignment; \n",
    "use a single mapper and single reducer; use a combiner if you think it will help and justify. </span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\"><b>Answer:</b></span>\n",
    "I will solve this using the Apriori algorithm. To do so, we must first tabulate the counts for each item. Then, when checking item pairs, we first will make sure that each item is 'frequent' before checking to see if the item pair is 'frequent'. If either of the two items in the proposed pair is not frequent, we can skip checking to see if the pair itself is frequent.\n",
    "\n",
    "I will do this using inverted ordering. The mapper/combiner will output two sorts of records:\n",
    "\n",
    "The first type of record will automatically sort to the top and will allow us to calculate single-item frequency before processing pairs:\n",
    "\n",
    "\\* ITEM_ID COUNT\n",
    "\n",
    "The second type of record will count pairs. Because of the inverted ordering, we can be assured we can calculate the single-term frequency before processing the term pairs:\n",
    "\n",
    "*ITEM_ID ITEM_ID COUNT\n",
    "\n",
    "\n",
    "I will use the \"pairs\" approach, as explained in the code comments below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "# Formula is: I -> J = (I U J) / I\n",
    "\n",
    "# Pairs Approach | Note, I'm traveling to see a sick family member this weekend. While\n",
    "# stipes is faster and perhaps a better approach, I am making the design decision that, in \n",
    "# my case, time of the programmer is the resource that we need to take into account. You can \n",
    "# liken this to a business situation where a project cannot afford programing hours, but can\n",
    "# have a longer runtime :). In future weeks, I'll make different decisions to experiment\n",
    "# with stripes.\n",
    "\n",
    "# To gain some efficiecny back, I'll write a quick combiner to help make things better.\n",
    "# In this case, there is no way the combiner will hurt things. I have to write code similar \n",
    "# to the combiner for the reducer anyways, so I'll just let Hadoop decide if it has spare\n",
    "# bandwidth to run the combiner. Remember: Hadoop doesn't guarantee it will run the combiner\n",
    "# if it doesn't make sense to.\n",
    "\n",
    "basket_inventory = {}\n",
    "        \n",
    "for basket in sys.stdin:\n",
    "    # Get unique items in basket\n",
    "    basket = list(set(basket.rstrip('\\n').split()))\n",
    "\n",
    "    # Output a single record for each item so that we can use order inversion in our\n",
    "    # reduce side frequency count.\n",
    "    for item1 in basket:\n",
    "        for item2 in basket:\n",
    "            if item1 == item2:\n",
    "                print \"* {}\\t1\".format(item1)\n",
    "            else:\n",
    "                print \"{} {}\\t1\".format(item1, item2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting combiner.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile combiner.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "#itempairs = [\"* 55\\t1\",\"* 55\\t1\",\"* 66\\t1\",\"23 45\\t1\",\"23 45\\t1\",\"23 95\\t1\"]\n",
    "currentpair = \"\"\n",
    "currentcount = 0\n",
    "\n",
    "for itempair in sys.stdin:\n",
    "#for itempair in itempairs:\n",
    "    itempair = itempair.rstrip('\\n')\n",
    "    # If multiple of one key in a row, sum\n",
    "    if itempair.split('\\t')[0] == currentpair:\n",
    "        currentcount += int(itempair.split('\\t')[1])\n",
    "    else:\n",
    "        # Otherwise, print and reset counters. Note - a combiner must print in the same format\n",
    "        # as a mapper.\n",
    "        if currentcount > 0:\n",
    "            print \"{}\\t{}\".format(currentpair, currentcount)\n",
    "        currentpair = itempair.split('\\t')[0]\n",
    "        currentcount = int(itempair.split('\\t')[1])\n",
    "print \"{}\\t{}\".format(currentpair, currentcount)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "currentpair = \"\"\n",
    "currentcount = 0\n",
    "itemlist_freq = {}\n",
    "itempair_freq = {}\n",
    "frequent_cutoff = 100\n",
    "\n",
    "#itempairs = [\"* 55\\t1\",\"* 55\\t3\",\"* 45\\t51\",\"* 66\\t1\",\"23 95\\t1\",\"55 45\\t1\",\"55 45\\t1\"]\n",
    "\n",
    "#for itempair in itempairs:\n",
    "for itempair in sys.stdin:\n",
    "    itempair = itempair.rstrip('\\n')\n",
    "    itempair, count = itempair.split('\\t')\n",
    "    firstitem, seconditem = itempair.split(' ')\n",
    "    \n",
    "    # For all sorts of records, we are not assured that the combiner fully combined\n",
    "    # things. As such, we make sure the reducer finishes combining.\n",
    "    # If we have a repeated key, sum the count\n",
    "    if itempair == currentpair:\n",
    "        currentcount += int(count)\n",
    "    else:\n",
    "        # Otherwise, post process\n",
    "        if currentcount > 0:\n",
    "            # For \"*\" records, store into dictionary to use later\n",
    "            if currentpair.split()[0] == \"*\":\n",
    "                itemlist_freq[currentpair.split()[1]] = currentcount\n",
    "#                    print \"{}\\t{}\".format(currentpair, currentcount)\n",
    "            elif firstitem != \"*\":\n",
    "            # For pairs, first see if components pass the min value (100)\n",
    "            # Note - this step isn't technically needed since we already have the\n",
    "            # pair count, however, we implement it in the spirit of the Apriori Alg.\n",
    "                if itemlist_freq.get(currentpair.split()[0], 0) > frequent_cutoff and itemlist_freq.get(\n",
    "                    currentpair.split()[1], 0) > frequent_cutoff:\n",
    "                    # If both components pass, check to see if the full count passes\n",
    "                    if currentcount > frequent_cutoff:\n",
    "                        # In this case, we can calculate confidence score and output\n",
    "                        print \"{} => {} = {}\".format(currentpair.split()[0], currentpair.split(\n",
    "                            )[1], float(currentcount)/itemlist_freq[currentpair.split()[0]])\n",
    "        currentpair = itempair\n",
    "        currentcount = int(count)\n",
    "\n",
    "# Because of our loop structure, we'll need to check then output one last score\n",
    "if itemlist_freq.get(currentpair.split()[0], 0) > frequent_cutoff and itemlist_freq.get(\n",
    "    currentpair.split()[1], 0) > frequent_cutoff:\n",
    "    # If both components pass, check to see if the full count passes\n",
    "    if currentcount > frequent_cutoff:\n",
    "        # In this case, we can calculate confidence score and output\n",
    "        print \"{} => {} = {}\".format(currentpair.split()[0], currentpair.split(\n",
    "            )[1], float(currentcount)/itemlist_freq[currentpair.split()[0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 10 results in the reducer output are:\n",
      "15/09/21 21:59:25 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "DAI16732 => FRO78087 = 0.566844919786\t\n",
      "DAI18527 => SNA44451 = 0.380597014925\t\n",
      "DAI22177 => DAI31081 = 0.0780577750461\t\n",
      "DAI22177 => DAI62779 = 0.234787953288\t\n",
      "DAI22177 => DAI63921 = 0.0835894283958\t\n",
      "DAI22177 => DAI75645 = 0.0755992624462\t\n",
      "DAI22177 => DAI83733 = 0.0774431468961\t\n",
      "DAI22177 => DAI85309 = 0.105716041795\t\n",
      "DAI22177 => ELE17451 = 0.124769514444\t\n",
      "DAI22177 => ELE26917 = 0.0823601720959\t\n",
      "cat: Unable to write to output stream.\n"
     ]
    }
   ],
   "source": [
    "# HW3.2: Run MapReduce with mapper, reducer, and combiner\n",
    "def HW3_2():\n",
    "    !hadoop jar /usr/local/Cellar/hadoop/2.6.0/libexec/share/hadoop/tools/lib/hadoop-streaming-2.6.0.jar \\\n",
    "    -Dmapreduce.job.maps=1 \\\n",
    "    -Dmapreduce.job.reduces=1 \\\n",
    "    -mapper ./mapper.py  \\\n",
    "    -combiner ./combiner.py \\\n",
    "    -reducer ./reducer.py \\\n",
    "    -input ./W261/In/HW3/ProductPurchaseData.txt -output ./W261/Out/HW3_2    \n",
    "#    -combiner ./combiner.py  \\\n",
    "#HW3_2()\n",
    "\n",
    "# Note - to clean up this python notebook, we ran HW3_2 and are replacing with its output\n",
    "print \"The first 10 results in the reducer output are:\"\n",
    "!hadoop fs -cat ./W261/Out/HW3_2/part-00000 | head -n10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\"><b>Answer:</b></span>\n",
    "We were told we do not need to sort the top list in Hadoop. I will read them from file, after downloading the file from HDFS. The step below shows our final answer:\n",
    "\n",
    "- ('DAI93865', 'FRO40251', '1.0')\n",
    "- ('GRO85051', 'FRO40251', '0.999176276771')\n",
    "- ('GRO38636', 'FRO40251', '0.990654205607')\n",
    "- ('ELE12951', 'FRO40251', '0.990566037736')\n",
    "- ('DAI88079', 'FRO40251', '0.986725663717')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('DAI93865', 'FRO40251', '1.0')\n",
      "('GRO85051', 'FRO40251', '0.999176276771')\n",
      "('GRO38636', 'FRO40251', '0.990654205607')\n",
      "('ELE12951', 'FRO40251', '0.990566037736')\n",
      "('DAI88079', 'FRO40251', '0.986725663717')\n"
     ]
    }
   ],
   "source": [
    "results_list = []\n",
    "with open(\"part-00000\") as output:\n",
    "    for line in output:\n",
    "        results_list.append((line.strip('\\n').split()[0], line.strip('\\n').split()[2], line.strip('\\n').split()[4]))\n",
    "\n",
    "# Note - I confirmed there are not ties, so we do not need to worry about lex. order\n",
    "for ranked_result in sorted(results_list,key=lambda x: x[2], reverse=True)[:5]:\n",
    "    print ranked_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This cell can be used to delete old output to allow re-run of any Hadoop script.\n",
    "#!hadoop fs -rm -r ./W261/Out/HW3_1_a\n",
    "#!hadoop fs -rm -r ./W261/Out/HW3_1_b\n",
    "#!hadoop fs -rm -r ./W261/Out/HW3_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:silver\"><b>HW3.3 [Note - We were instructed to skip this problem]</b></span>\n",
    "\n",
    "<span style=\"color:silver\">Benchmark your results using the pyFIM implementation of the Apriori algorithm\n",
    "(Apriori - Association Rule Induction / Frequent Item Set Mining implemented by Christian Borgelt). </span>\n",
    "<span style=\"color:silver\">You can download pyFIM from here: </span>\n",
    "\n",
    "http://www.borgelt.net/pyfim.html\n",
    "\n",
    "<span style=\"color:silver\">Comment on the results from both implementations (your Hadoop MapReduce of apriori versus pyFIM) \n",
    "in terms of results and execution times.</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:silver\"><b>HW3.4</b> (Conceptual Exercise)</span>\n",
    "\n",
    "<span style=\"color:silver\">Suppose that you wished to perform the Apriori algorithm once again,\n",
    "though this time now with the goal of listing the top 5 rules with corresponding confidence scores \n",
    "in decreasing order of confidence score for itemsets of size 3 using Hadoop MapReduce.\n",
    "A rule is now of the form: </span>\n",
    "\n",
    "<span style=\"color:silver\">(item1, item2) ⇒ item3 </span>\n",
    "\n",
    "<span style=\"color:silver\">Recall that the Apriori algorithm is iterative for increasing itemset size,\n",
    "working off of the frequent itemsets of the previous size to explore \n",
    "ONLY the NECESSARY subset of a large combinatorial space. \n",
    "Describe how you might design a framework to perform this exercise.</span>\n",
    "\n",
    "<span style=\"color:silver\">In particular, focus on the following:</span>\n",
    "  — <span style=\"color:silver\">map-reduce steps required</span>\n",
    "  - <span style=\"color:silver\">enumeration of item sets and filtering for frequent candidates</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\"><b>Answer</b></span>:\n",
    "\n",
    "If we were still restricted to a single mapper/reducer pair, we would continue in the manner set forth above. We would need to violate teh Apriori algorithm by creating pairs (or stripes) for all possible 3-item sets, then in the reducer we would only process those 3-item sets comprised of passing 2-item sets. In that case, the data from the mapper would leverage an additional level of inverted ordering, outputting tuples as follows:\n",
    "\n",
    "- (**, **, a)\n",
    "- (**, a, b)\n",
    "- (a, b, c)\n",
    "\n",
    "From this, can count all the a, b terms via the pre-sorted *, a, b\n",
    "\n",
    "However, this thought exercise allows us to move beyond this simple limitation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This concludes HW 3.0. Thanks for reading!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
