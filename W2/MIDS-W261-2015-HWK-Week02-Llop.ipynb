{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIDS W261 Machine Learning At Scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Christopher Llop | christopher.llop@ischool.berkeley.edu <br>\n",
    "Week 2 | Submission Date:\n",
    "\n",
    "<span style=\"color:red\">[Placeholder for introduction to assignment]</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:black\"><b>HW2.0.</b> What is a race condition in the context of parallel computation? Give an example.\n",
    "What is MapReduce?\n",
    "How does it differ from Hadoop?\n",
    "Which programming paradigm is Hadoop based on? Explain and give a simple example in code and show the code running.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\"><b>Answer:</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:black\"><b>HW2.1. </b> Sort in Hadoop MapReduce\n",
    "\n",
    "\n",
    "Given as input: Records of the form (integer, “NA”), where integer is any integer, and “NA” is just the empty string.\n",
    "\n",
    "Output: sorted key value pairs of the form (integer, “NA”); what happens if you have multiple reducers? Do you need additional steps? Explain.\n",
    "\n",
    "Write code to generate N  random records of the form (integer, “NA”). Let N = 10,000.\n",
    "Write the python Hadoop streaming map-reduce job to perform this sort.</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\"><b>Answer:</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:black\"><b>HW2.2.</b></style> Using the Enron data from HW1 and Hadoop MapReduce streaming, write mapper/reducer pair that  will determine the number of occurrences of a single, user-specified word. Examine the word “assistance” and report your results.\n",
    "\n",
    "\n",
    "   To do so, make sure that\n",
    "   \n",
    "   - mapper.py counts all occurrences of a single word, and\n",
    "   - reducer.py collates the counts of the single word.\n",
    "\n",
    "CROSSCHECK: >grep assistance enronemail_1h.txt|cut -d$'\\t' -f4| grep assistance|wc -l\n",
    "       8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:black\"><b>HW2.3.</b></style> Using the Enron data from HW1 and Hadoop MapReduce, write  a mapper/reducer pair that\n",
    "   will classify the email messages by a single, user-specified word. Examine the word “assistance” and report your results. To do so, make sure that\n",
    "   \n",
    "   - mapper.py\n",
    "   - reducer.py \n",
    "\n",
    "   performs a single word multinomial Naive Bayes classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:black\"><b>HW2.4.</b></style> Using the Enron data from HW1 and in the Hadoop MapReduce framework, write  a mapper/reducer pair that\n",
    "   will classify the email messages using multinomial Naive Bayes Classifier using a list of one or more user-specified words. Examine the words “assistance”, “valium”, and “enlargementWithATypo” and report your results\n",
    "   To do so, make sure that\n",
    "\n",
    "   - mapper.py \n",
    "   - reducer.py \n",
    "\n",
    "   performs the multiple-word multinomial Naive Bayes classification via the chosen list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<span style=\"color:black\"><b>HW2.5.</b></style>\n",
    "\n",
    "Using the Enron data from HW1 an in the  Hadoop MapReduce framework, write  a mapper/reducer for a multinomial Naive Bayes Classifier that\n",
    "   will classify the email messages using  words present. Also drop words with a frequency of less than three (3). How does it affect the misclassifcation error of learnt naive multinomial Bayesian Classifiers on the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# HW 1.1: Read through pNaiveBayes and all comments to \n",
    "#         become comfortable with the code.\n",
    "\n",
    "def HW1_1():\n",
    "    print \"done\"\n",
    "\n",
    "HW1_1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:silver\"><b>HW1.2.</b> Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh will determine the number of occurrences of a single, user-specified word. Examine the word “assistance” and report your results.</span>\n",
    "\n",
    "   <span style=\"color:silver\">To do so, make sure that:</span>\n",
    "   - <span style=\"color:silver\">mapper.py counts all occurrences of a single word, and</span>\n",
    "   - <span style=\"color:silver\">reducer.py collates the counts of the single word.</span>\n",
    "   \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "\n",
    "# Get input parameters\n",
    "findword = sys.argv[2]\n",
    "findfile = sys.argv[1]\n",
    "term_hits = {}\n",
    "\n",
    "with open (findfile, \"r\") as myfile:\n",
    "    for full_email in myfile:\n",
    "        try:\n",
    "            # Spam classification\n",
    "            is_spam = re.findall(\"\\t([0-1])\\t\",full_email)[0]\n",
    "\n",
    "            # Parse out email body for processing. Find body using \"tab spam/ham tab\"\n",
    "            # use regex to strip out non alpha-numeric. \"don't\" will become \"dont\" which is fine.\n",
    "            keyword = re.findall(\"\\t[0-1]\\t\",full_email)[0]\n",
    "            email_id, is_spam_tabbed, email_body = full_email.partition(keyword)\n",
    "            email_body = re.sub('[^A-Za-z0-9\\s]+', '', email_body)\n",
    "            email_len = len(email_body.split())\n",
    "\n",
    "            # Number of hits for search word. Using a dictionary now will allow us to use\n",
    "            #    a similar Mapper output format when using multiple search terms later \n",
    "            #    in this problem set.\n",
    "            term_hits[findword] = len(re.findall(findword,email_body))\n",
    "            \n",
    "            # Print as tuple with unique splitter \"|||\"\n",
    "            print \"{} ||| {} ||| {}\".format(is_spam, email_len, term_hits)\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import ast\n",
    "\n",
    "# Get input parameters - list of file names\n",
    "filelist = sys.argv[1:]\n",
    "\n",
    "term_sum = {}\n",
    "\n",
    "# Open each map result\n",
    "for thisfile in filelist:\n",
    "    with open (thisfile, \"r\") as openfile:\n",
    "        # Process each emaul one at a time\n",
    "        for processed_email in openfile:\n",
    "            # Read data for email\n",
    "            processed_email = processed_email.split(\" ||| \")\n",
    "            count_dict = ast.literal_eval(processed_email[2])\n",
    "            \n",
    "            # Fold (sum) together the results from mapping\n",
    "            for key, value in count_dict.iteritems():\n",
    "                term_sum[key] = term_sum.get(key,0) + value\n",
    "\n",
    "# Print results\n",
    "for key, value in term_sum.iteritems():\n",
    "    print \"The word count for '{}' is {}\".format(key, value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use chmod for permissions - Only need to do this once per assignment since we're just overriding the file\n",
    "!chmod a+x mapper.py\n",
    "!chmod a+x reducer.py\n",
    "!chmod a+x pNaiveBayes.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: The word count for 'assistance' is 10\n",
      "\n",
      "Check Result: 10\n"
     ]
    }
   ],
   "source": [
    "# HW 1.2: Create map/reduce pair that determins occurances of a single word\n",
    "def HW1_2(word=\"assistance\"):\n",
    "    import re\n",
    "    \n",
    "    # Run pNaiveBayes.sh\n",
    "    !./pNaiveBayes.sh 10 {word}\n",
    "\n",
    "    # Print the output file contents to screen\n",
    "    with open (\"enronemail_1h.txt.output\", \"r\") as openfile:\n",
    "        print \"Result:\", openfile.read()\n",
    "\n",
    "    # Crosscheck results (data is small enough to use RE in python)\n",
    "    with open (\"enronemail_1h.txt\", \"r\") as myfile:\n",
    "        print \"Check Result:\", len(re.findall(word,myfile.read()))\n",
    "        \n",
    "HW1_2(word = \"assistance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:silver\"><b>HW1.3. </b>Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh\n",
    "   will classify the email messages by a single, user-specified word. Examine the word “assistance” and report your results. To do so, make sure that</span>\n",
    "   \n",
    "   - <span style=\"color:silver\">mapper.py is same as in part (2), and [Note - later instructions removed this criteria]</span>\n",
    "   - <span style=\"color:silver\">reducer.py performs a single word Naive Bayes classification.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "\n",
    "# Get input parameters\n",
    "findword = sys.argv[2]\n",
    "findfile = sys.argv[1]\n",
    "\n",
    "with open (findfile, \"r\") as myfile:\n",
    "    for full_email in myfile:\n",
    "        try:\n",
    "            # Empty dictionary\n",
    "            term_hits = {}\n",
    "            \n",
    "            # Spam classification\n",
    "            is_spam = re.findall(\"\\t([0-1])\\t\",full_email)[0]\n",
    "\n",
    "            # Parse out email body for processing. Find body using \"tab spam/ham tab\"\n",
    "            # use regex to strip out non alpha-numeric. \"don't\" will become \"dont\" which is fine for classifying.\n",
    "            keyword = re.findall(\"\\t[0-1]\\t\",full_email)[0]\n",
    "            email_id, is_spam_tabbed, email_body = full_email.partition(keyword)\n",
    "            email_body = re.sub('[^A-Za-z0-9\\s]+', '', email_body)\n",
    "            # Must process search query and email bodies the same\n",
    "            findword = re.sub('[^A-Za-z0-9\\s]+', '', findword)\n",
    "            email_len = len(email_body.split())\n",
    "\n",
    "            # Build counts of term words. Note, we need to get a count of distinct words\n",
    "            # regardless, so we might as well count all words here in preparation for the next problem 1.3.\n",
    "            # In an actual industry application, we would not store counts for all words since that would\n",
    "            # take up more space than required to solve the problem. Here, the focus is on learning MapReduce\n",
    "            # and Naive Bayes as opposed to this concern.\n",
    "            for word in list(set(email_body.split())):\n",
    "                term_hits[word] = len(re.findall(word,email_body))\n",
    "            \n",
    "            # Print as tuple with unique splitter \"|||\"\n",
    "            print \"{} ||| {} ||| {} ||| {} ||| {}\".format(re.findall(r'\\w+', findword), email_id, is_spam, email_len, term_hits)\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import ast\n",
    "import math\n",
    "\n",
    "filelist = sys.argv[1:]\n",
    "\n",
    "spam_term_counts = {}\n",
    "ham_term_counts = {}\n",
    "word_prob_given_spam = {}\n",
    "word_prob_given_ham = {}\n",
    "spam_count = 0\n",
    "ham_count = 0\n",
    "spam_len = 0\n",
    "ham_len = 0\n",
    "distinct_term_list = []\n",
    "\n",
    "# Open each file and build Multinomial Naive Bayes model\n",
    "for thisfile in filelist:\n",
    "    with open (thisfile, \"r\") as openfile:\n",
    "        for processed_email in openfile:\n",
    "            # Read in tuples created by mapper\n",
    "            processed_email = processed_email.split(\" ||| \")\n",
    "            search_terms = ast.literal_eval(processed_email[0])\n",
    "            email_id = processed_email[1]\n",
    "            is_spam = int(processed_email[2])\n",
    "            email_len = int(processed_email[3])\n",
    "            count_dict = ast.literal_eval(processed_email[4])\n",
    "            \n",
    "            # Build counts for spam and ham definitions.\n",
    "            if is_spam:\n",
    "                for key, value in count_dict.iteritems():\n",
    "                    spam_term_counts[key] = spam_term_counts.get(key, 0) + value\n",
    "                spam_count += 1\n",
    "                spam_len += email_len\n",
    "            else:\n",
    "                for key, value in count_dict.iteritems():\n",
    "                    ham_term_counts[key] = ham_term_counts.get(key, 0) + value\n",
    "                ham_count += 1\n",
    "                ham_len += email_len\n",
    "                \n",
    "            distinct_term_list = list(set(distinct_term_list + count_dict.keys()))\n",
    "\n",
    "# Calculate our priors based on the overall ratio of spam to ham\n",
    "spam_prior = float(spam_count) / (spam_count + ham_count)\n",
    "ham_prior = 1 - spam_prior\n",
    "spam_prior = math.log10(spam_prior)\n",
    "ham_prior = math.log10(ham_prior)\n",
    "\n",
    "# Calculate our conditional probabilites for the search term using MNB formula\n",
    "#     term_given_spam = (spam_term_count + 1 for smoothing) / (total count of spam words + total distinct vocab size)\n",
    "distinct_term_count = len(distinct_term_list)\n",
    "\n",
    "for term in search_terms:\n",
    "    word_prob_given_spam[term] = math.log10((spam_term_counts.get(term,0) + 1.0) / (float(spam_len) + distinct_term_count))\n",
    "    word_prob_given_ham[term] = math.log10((ham_term_counts.get(term,0) + 1.0) / (float(ham_len) + distinct_term_count))\n",
    "\n",
    "# Open each file and predict. Note - prediction is embaressingly parallel and could\n",
    "#   be done effectively via Mapping. However, the assignment asks us to solve the problem using\n",
    "#   the provided pNaiveBayes.sh.\n",
    "accuracy = []\n",
    "for thisfile in filelist:\n",
    "    with open (thisfile, \"r\") as openfile:\n",
    "        for processed_email in openfile:\n",
    "            # Defaults\n",
    "            pred_spam = 0\n",
    "            spam_prediction = spam_prior\n",
    "            ham_prediction = ham_prior\n",
    "\n",
    "            # Read in tuples created by mapper\n",
    "            processed_email = processed_email.split(\" ||| \")\n",
    "            email_id = processed_email[1]\n",
    "            is_spam = int(processed_email[2])\n",
    "            count_dict = ast.literal_eval(processed_email[4])\n",
    "            \n",
    "            # Read in counts to use in prediction\n",
    "            for term in word_prob_given_spam.keys():\n",
    "                # Calculate the probability for each class\n",
    "                spam_prediction += (word_prob_given_spam[term] * count_dict.get(term, 0))\n",
    "                ham_prediction += (word_prob_given_ham[term] * count_dict.get(term, 0))\n",
    "                \n",
    "            # Pick the higher probability\n",
    "            if spam_prediction > ham_prediction: \n",
    "                pred_spam = 1\n",
    "            \n",
    "            # Store accuracy in a list\n",
    "            accuracy.append(1*(pred_spam==is_spam))\n",
    "\n",
    "            # Print predictions to results file\n",
    "            print '{}\\t{}\\t{}'.format(email_id, is_spam, pred_spam)\n",
    "# Print accuracy\n",
    "sys.stderr.write(\"Spam Probs: {}\\n\".format(word_prob_given_spam))\n",
    "sys.stderr.write(\"Ham Probs: {}\\n\".format(word_prob_given_ham))\n",
    "sys.stderr.write(\"Accuracy = {:.2f}\\n\".format(float(sum(accuracy))/len(accuracy)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spam Probs: {'assistance': -3.4264028097516}\r\n",
      "Ham Probs: {'assistance': -3.799891684656865}\r\n",
      "Accuracy = 0.60\r\n"
     ]
    }
   ],
   "source": [
    "# HW 1.3: Create multinomial bayes map/reduce pair that predicts spam/ham using a single word\n",
    "def HW1_3(terms=\"assistance\"):\n",
    "    # Run pNaiveBayes.sh\n",
    "    !./pNaiveBayes.sh 10 \"{terms}\"\n",
    "        \n",
    "HW1_3(terms = \"assistance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:silver\"><b>HW1.4.</b> Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh\n",
    "   will classify the email messages by a list of one or more user-specified words. Examine the words “assistance”, “valium”, and “enlargementWithATypo” and report your results\n",
    "   To do so, make sure that</span>\n",
    " \n",
    "\n",
    "   - <span style=\"color:silver\">mapper.py counts all occurrences of a list of words, and</span>\n",
    "   - <span style=\"color:silver\">reducer.py performs the multiple-word Naive Bayes classification via the chosen list.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spam Probs: {'assistance': -3.4264028097516, 'enlargementWithATypo': -4.380645319190925, 'valium': -3.778585327862962}\r\n",
      "Ham Probs: {'assistance': -3.799891684656865, 'enlargementWithATypo': -4.277012939376528, 'valium': -4.277012939376528}\r\n",
      "Accuracy = 0.63\r\n"
     ]
    }
   ],
   "source": [
    "# HW 1.4: Our function for HW 1.3 can also classify multiple words, as requested by problem 1.4\n",
    "def HW1_4(words = \"assistance valium enlargementWithATypo\"):\n",
    "    # Run pNaiveBayes.sh\n",
    "    !./pNaiveBayes.sh 10 \"{words}\"\n",
    "            \n",
    "HW1_4(words = \"assistance valium enlargementWithATypo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:silver\"><b>HW1.5.</b> Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh will classify the email messages by all words present. To do so, make sure that</span>\n",
    "\n",
    "   - <span style=\"color:silver\">mapper.py counts all occurrences of all words, and</span>\n",
    "   - <span style=\"color:silver\">reducer.py performs a word-distribution-wide Naive Bayes classification.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "\n",
    "# Get input parameters\n",
    "findword = sys.argv[2]\n",
    "findfile = sys.argv[1]\n",
    "\n",
    "star_switch = 0\n",
    "if findword == \"*\":\n",
    "    star_switch = 1\n",
    "\n",
    "with open (findfile, \"r\") as myfile:\n",
    "    for full_email in myfile:\n",
    "        try:\n",
    "            # Empty dictionary\n",
    "            term_hits = {}\n",
    "\n",
    "            # Spam classification\n",
    "            is_spam = re.findall(\"\\t([0-1])\\t\",full_email)[0]\n",
    "\n",
    "            # Parse out email body for processing. Find body using \"tab spam/ham tab\"\n",
    "            # use regex to strip out non alpha-numeric. \"don't\" will become \"dont\" which is fine for classifying.\n",
    "            keyword = re.findall(\"\\t[0-1]\\t\",full_email)[0]\n",
    "            email_id, is_spam_tabbed, email_body = full_email.partition(keyword)\n",
    "            email_body = re.sub('[^A-Za-z0-9\\s]+', '', email_body)\n",
    "            # Must process search query and email bodies the same\n",
    "            # This is a good place to add logic to use all terms in body as search terms when \"*\" appears\n",
    "            if star_switch == 1:\n",
    "                findword = \" \".join(list(set(email_body.split())))\n",
    "            else:\n",
    "                findword = re.sub('[^A-Za-z0-9\\s]+', '', findword)\n",
    "            email_len = len(email_body.split())\n",
    "\n",
    "            # Build counts of term words.\n",
    "            for word in list(set(email_body.split())):\n",
    "                term_hits[word] = len(re.findall(word,email_body))\n",
    "\n",
    "            # Print as tuple with unique splitter \"|||\"\n",
    "            print \"{} ||| {} ||| {} ||| {} ||| {}\".format(re.findall(r'\\w+', findword), email_id, is_spam, email_len, term_hits)\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import ast\n",
    "import math\n",
    "\n",
    "filelist = sys.argv[1:]\n",
    "\n",
    "spam_term_counts = {}\n",
    "ham_term_counts = {}\n",
    "word_prob_given_spam = {}\n",
    "word_prob_given_ham = {}\n",
    "spam_count = 0\n",
    "ham_count = 0\n",
    "spam_len = 0\n",
    "ham_len = 0\n",
    "distinct_term_list = []\n",
    "\n",
    "# Open each file and build Multinomial Naive Bayes model\n",
    "for thisfile in filelist:\n",
    "    with open (thisfile, \"r\") as openfile:\n",
    "        for processed_email in openfile:\n",
    "            # Read in tuples created by mapper\n",
    "            processed_email = processed_email.split(\" ||| \")\n",
    "            search_terms = ast.literal_eval(processed_email[0])\n",
    "            email_id = processed_email[1]\n",
    "            is_spam = int(processed_email[2])\n",
    "            email_len = int(processed_email[3])\n",
    "            count_dict = ast.literal_eval(processed_email[4])\n",
    "            \n",
    "            # Build counts for spam and ham definitions.\n",
    "            if is_spam:\n",
    "                for key, value in count_dict.iteritems():\n",
    "                    spam_term_counts[key] = spam_term_counts.get(key, 0) + value\n",
    "                spam_count += 1\n",
    "                spam_len += email_len\n",
    "            else:\n",
    "                for key, value in count_dict.iteritems():\n",
    "                    ham_term_counts[key] = ham_term_counts.get(key, 0) + value\n",
    "                ham_count += 1\n",
    "                ham_len += email_len\n",
    "                \n",
    "            distinct_term_list = list(set(distinct_term_list + count_dict.keys()))\n",
    "\n",
    "# Calculate our priors based on the overall ratio of spam to ham\n",
    "spam_prior = float(spam_count) / (spam_count + ham_count)\n",
    "ham_prior = 1 - spam_prior\n",
    "spam_prior = math.log10(spam_prior)\n",
    "ham_prior = math.log10(ham_prior)\n",
    "\n",
    "# Calculate our conditional probabilites for the search term using MNB formula\n",
    "#     term_given_spam = (spam_term_count + 1 for smoothing) / (total count of spam words + total distinct vocab size)\n",
    "distinct_term_count = len(distinct_term_list)\n",
    "\n",
    "for term in distinct_term_list:\n",
    "    word_prob_given_spam[term] = math.log10((spam_term_counts.get(term,0) + 1.0) / (float(spam_len) + distinct_term_count))\n",
    "    word_prob_given_ham[term] = math.log10((ham_term_counts.get(term,0) + 1.0) / (float(ham_len) + distinct_term_count))\n",
    "\n",
    "# Open each file and predict. Note - prediction is embaressingly parallel and could\n",
    "#   be done effectively via Mapping. However, the assignment asks us to solve the problem using\n",
    "#   the provided pNaiveBayes.sh.\n",
    "accuracy = []\n",
    "for thisfile in filelist:\n",
    "    with open (thisfile, \"r\") as openfile:\n",
    "        for processed_email in openfile:\n",
    "            # Defaults\n",
    "            pred_spam = 0\n",
    "            spam_prediction = spam_prior\n",
    "            ham_prediction = ham_prior\n",
    "\n",
    "            # Read in tuples created by mapper\n",
    "            processed_email = processed_email.split(\" ||| \")\n",
    "            email_id = processed_email[1]\n",
    "            is_spam = int(processed_email[2])\n",
    "            count_dict = ast.literal_eval(processed_email[4])\n",
    "            \n",
    "            # Read in counts to use in prediction\n",
    "            for term in distinct_term_list:\n",
    "                # Calculate the probability for each class\n",
    "                spam_prediction += (word_prob_given_spam[term] * count_dict.get(term, 0))\n",
    "                ham_prediction += (word_prob_given_ham[term] * count_dict.get(term, 0))\n",
    "                \n",
    "            # Pick the higher probability\n",
    "            if spam_prediction > ham_prediction: \n",
    "                pred_spam = 1\n",
    "            \n",
    "            # Store accuracy in a list\n",
    "            accuracy.append(1*(pred_spam==is_spam))\n",
    "\n",
    "            # Print predictions to results file\n",
    "            print '{}\\t{}\\t{}'.format(email_id, is_spam, pred_spam)\n",
    "# Print accuracy\n",
    "sys.stderr.write(\"Accuracy = {:.2f}\\n\".format(float(sum(accuracy))/len(accuracy)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.96\r\n"
     ]
    }
   ],
   "source": [
    "# HW 1.5: Run on all words. \n",
    "def HW1_5():\n",
    "    # Run pNaiveBayes.sh\n",
    "    !./pNaiveBayes.sh 10 \"*\"\n",
    "            \n",
    "HW1_5()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '3', 'five']\n",
      "1 3 five\n"
     ]
    }
   ],
   "source": [
    "test = [\"1\",\"3\",\"five\"]\n",
    "print str(test)\n",
    "print \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "content = \"12313313\t1\tasdad13\t1\ta13\t0\ta\"\n",
    "print re.findall(\"\\t([0-1])\\t\",content)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12313313\n",
      "\t1\t\n",
      "asdad13\t1\ta13\t0\ta\n"
     ]
    }
   ],
   "source": [
    "content = \"12313313\t1\tasdad13\t1\ta13\t0\ta\"\n",
    "keyword = re.findall(\"\\t[0-1]\\t\",content)[0]\n",
    "email_id, is_spam, email_body = content.partition(keyword)\n",
    "print email_id\n",
    "print is_spam\n",
    "print email_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
