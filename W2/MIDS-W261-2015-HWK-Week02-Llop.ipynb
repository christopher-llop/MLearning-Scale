{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIDS W261 Machine Learning At Scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Christopher Llop | christopher.llop@ischool.berkeley.edu <br>\n",
    "Week 2 | Submission Date:\n",
    "\n",
    "<span style=\"color:red\">[Placeholder for introduction to assignment]</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:black\"><b>HW2.0.</b> What is a race condition in the context of parallel computation? Give an example.\n",
    "What is MapReduce?\n",
    "How does it differ from Hadoop?\n",
    "Which programming paradigm is Hadoop based on? Explain and give a simple example in code and show the code running.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\"><b>Answer:</b></span>\n",
    "\n",
    "A <b>race condition</b> is a condition where two threads must access the same data source. The programmer in this instance does not control which thread modifies the data first. As a result, it is possible that the ultimate end product of the code differs at random based on the order that the data source is accessed by threads.\n",
    "\n",
    "As an example, say the number \"3\" is stored on disk. Thread 1 wants to double the number, while Thread 2 wants to add 5 to the number. If Thread 1 acts first, the result is $(3 * 2) + 5 = 11$. If Thread 2 acts first, the result is $(3 + 5) * 2 = 16$. This sort of condition can cause all sorts of difficulties.\n",
    "\n",
    "<br>\n",
    "<b>MapReduce</b> is a problem solving framework/concept for embaressingly parallel data analysis. At its core, a problem is chunked and first processed in parallel by a number of mappers. Reducers then \"fold\" together the results of the mappers into a final output. <b>Hadoop</b> is a technical environment that, when combined with Hadoop File System allows a programmer to execute MapReduce jobs with ease. Hadoop programming is based on the MapReduce paradigm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:silver\"><b>HW2.1. </b> Sort in Hadoop MapReduce\n",
    "Given as input: Records of the form (integer, “NA”), where integer is any integer, and “NA” is just the empty string.\n",
    "Output: sorted key value pairs of the form (integer, “NA”); what happens if you have multiple reducers? Do you need additional steps? Explain.</span>\n",
    "\n",
    "<span style=\"color:silver\">Write code to generate N  random records of the form (integer, “NA”). Let N = 10,000.\n",
    "Write the python Hadoop streaming map-reduce job to perform this sort.</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\"><b>Answer:</b></span>\n",
    "\n",
    "If we have multiple reducers, the results will be sorted within each reducer - however they will not be globally sorted. While all the results for a given key wind up at the same reducer, reducers are not guaranteed to be given consecutive keys in the sort order. To correct for this, we could either force our system to send keys to the reducers in sorted chunks, or we could post-process all the reducer outputs to re-sort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "from random import randint\n",
    "\n",
    "for x in range(0,10000):\n",
    "    # Generate random integers\n",
    "    print \"{}\\t{}\".format(randint(0,10000),\"NA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    # Because the reducer is given the keys in sorted order, we can simply print\n",
    "    print line.rstrip('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use chmod for permissions\n",
    "!chmod a+x mapper.py\n",
    "!chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/12 19:25:42 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "put: `W261/In/HW2/dummy.txt': File exists\n"
     ]
    }
   ],
   "source": [
    "# We need a dummy text file to run streaming\n",
    "!echo nothing > dummy.txt\n",
    "!hadoop fs -mkdir ./W261/In/HW2\n",
    "!hdfs dfs -put ./dummy.txt ./W261/In/HW2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/12 19:26:25 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/12 19:26:26 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "15/09/12 19:26:26 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "15/09/12 19:26:26 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "15/09/12 19:26:27 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "15/09/12 19:26:27 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "15/09/12 19:26:27 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1950920983_0001\n",
      "15/09/12 19:26:27 INFO mapred.LocalDistributedCacheManager: Localized file:/Users/cjllop/Code/MIDS/W261/HW/W2/mapper.py as file:/usr/local/Cellar/hadoop/hdfs/tmp/mapred/local/1442100387402/mapper.py\n",
      "15/09/12 19:26:27 INFO mapred.LocalDistributedCacheManager: Localized file:/Users/cjllop/Code/MIDS/W261/HW/W2/reducer.py as file:/usr/local/Cellar/hadoop/hdfs/tmp/mapred/local/1442100387403/reducer.py\n",
      "15/09/12 19:26:28 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "15/09/12 19:26:28 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "15/09/12 19:26:28 INFO mapreduce.Job: Running job: job_local1950920983_0001\n",
      "15/09/12 19:26:28 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "15/09/12 19:26:28 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "15/09/12 19:26:28 INFO mapred.LocalJobRunner: Starting task: attempt_local1950920983_0001_m_000000_0\n",
      "15/09/12 19:26:28 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "15/09/12 19:26:28 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "15/09/12 19:26:28 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/cjllop/W261/In/HW2/dummy.txt:0+8\n",
      "15/09/12 19:26:28 INFO mapred.MapTask: numReduceTasks: 1\n",
      "15/09/12 19:26:28 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "15/09/12 19:26:28 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "15/09/12 19:26:28 INFO mapred.MapTask: soft limit at 83886080\n",
      "15/09/12 19:26:28 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "15/09/12 19:26:28 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "15/09/12 19:26:28 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "15/09/12 19:26:28 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/cjllop/Code/MIDS/W261/HW/W2/././mapper.py]\n",
      "15/09/12 19:26:28 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "15/09/12 19:26:28 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "15/09/12 19:26:28 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "15/09/12 19:26:28 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "15/09/12 19:26:28 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "15/09/12 19:26:28 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "15/09/12 19:26:28 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "15/09/12 19:26:28 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "15/09/12 19:26:28 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "15/09/12 19:26:28 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "15/09/12 19:26:28 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "15/09/12 19:26:28 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "15/09/12 19:26:28 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/12 19:26:28 INFO streaming.PipeMapRed: Records R/W=1/1\n",
      "15/09/12 19:26:28 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/09/12 19:26:28 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/09/12 19:26:28 INFO mapred.LocalJobRunner: \n",
      "15/09/12 19:26:28 INFO mapred.MapTask: Starting flush of map output\n",
      "15/09/12 19:26:28 INFO mapred.MapTask: Spilling map output\n",
      "15/09/12 19:26:28 INFO mapred.MapTask: bufstart = 0; bufend = 78900; bufvoid = 104857600\n",
      "15/09/12 19:26:28 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26174400(104697600); length = 39997/6553600\n",
      "15/09/12 19:26:28 INFO mapred.MapTask: Finished spill 0\n",
      "15/09/12 19:26:28 INFO mapred.Task: Task:attempt_local1950920983_0001_m_000000_0 is done. And is in the process of committing\n",
      "15/09/12 19:26:28 INFO mapred.LocalJobRunner: Records R/W=1/1\n",
      "15/09/12 19:26:28 INFO mapred.Task: Task 'attempt_local1950920983_0001_m_000000_0' done.\n",
      "15/09/12 19:26:28 INFO mapred.LocalJobRunner: Finishing task: attempt_local1950920983_0001_m_000000_0\n",
      "15/09/12 19:26:28 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "15/09/12 19:26:28 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "15/09/12 19:26:28 INFO mapred.LocalJobRunner: Starting task: attempt_local1950920983_0001_r_000000_0\n",
      "15/09/12 19:26:28 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "15/09/12 19:26:28 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "15/09/12 19:26:28 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@3df89785\n",
      "15/09/12 19:26:28 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=371130368, maxSingleShuffleLimit=92782592, mergeThreshold=244946048, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "15/09/12 19:26:28 INFO reduce.EventFetcher: attempt_local1950920983_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "15/09/12 19:26:29 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1950920983_0001_m_000000_0 decomp: 98902 len: 98906 to MEMORY\n",
      "15/09/12 19:26:29 INFO reduce.InMemoryMapOutput: Read 98902 bytes from map-output for attempt_local1950920983_0001_m_000000_0\n",
      "15/09/12 19:26:29 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 98902, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->98902\n",
      "15/09/12 19:26:29 INFO mapreduce.Job: Job job_local1950920983_0001 running in uber mode : false\n",
      "15/09/12 19:26:29 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "15/09/12 19:26:29 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/12 19:26:29 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "15/09/12 19:26:29 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "15/09/12 19:26:29 INFO mapred.Merger: Merging 1 sorted segments\n",
      "15/09/12 19:26:29 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 98898 bytes\n",
      "15/09/12 19:26:29 INFO reduce.MergeManagerImpl: Merged 1 segments, 98902 bytes to disk to satisfy reduce memory limit\n",
      "15/09/12 19:26:29 INFO reduce.MergeManagerImpl: Merging 1 files, 98906 bytes from disk\n",
      "15/09/12 19:26:29 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "15/09/12 19:26:29 INFO mapred.Merger: Merging 1 sorted segments\n",
      "15/09/12 19:26:29 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 98898 bytes\n",
      "15/09/12 19:26:29 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/12 19:26:29 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/cjllop/Code/MIDS/W261/HW/W2/././reducer.py]\n",
      "15/09/12 19:26:29 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "15/09/12 19:26:29 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "15/09/12 19:26:29 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/12 19:26:29 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/12 19:26:29 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/12 19:26:29 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/12 19:26:29 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/12 19:26:29 INFO streaming.PipeMapRed: Records R/W=10000/1\n",
      "15/09/12 19:26:29 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/09/12 19:26:29 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/09/12 19:26:29 INFO mapred.Task: Task:attempt_local1950920983_0001_r_000000_0 is done. And is in the process of committing\n",
      "15/09/12 19:26:29 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/12 19:26:29 INFO mapred.Task: Task attempt_local1950920983_0001_r_000000_0 is allowed to commit now\n",
      "15/09/12 19:26:29 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1950920983_0001_r_000000_0' to hdfs://localhost:9000/user/cjllop/W261/Out/HW2_1/_temporary/0/task_local1950920983_0001_r_000000\n",
      "15/09/12 19:26:29 INFO mapred.LocalJobRunner: Records R/W=10000/1 > reduce\n",
      "15/09/12 19:26:29 INFO mapred.Task: Task 'attempt_local1950920983_0001_r_000000_0' done.\n",
      "15/09/12 19:26:29 INFO mapred.LocalJobRunner: Finishing task: attempt_local1950920983_0001_r_000000_0\n",
      "15/09/12 19:26:29 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "15/09/12 19:26:30 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "15/09/12 19:26:30 INFO mapreduce.Job: Job job_local1950920983_0001 completed successfully\n",
      "15/09/12 19:26:30 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=408796\n",
      "\t\tFILE: Number of bytes written=1050282\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=16\n",
      "\t\tHDFS: Number of bytes written=78900\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=1\n",
      "\t\tMap output records=10000\n",
      "\t\tMap output bytes=78900\n",
      "\t\tMap output materialized bytes=98906\n",
      "\t\tInput split bytes=107\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=6346\n",
      "\t\tReduce shuffle bytes=98906\n",
      "\t\tReduce input records=10000\n",
      "\t\tReduce output records=10000\n",
      "\t\tSpilled Records=20000\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=4\n",
      "\t\tTotal committed heap usage (bytes)=397885440\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=8\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=78900\n",
      "15/09/12 19:26:30 INFO streaming.StreamJob: Output directory: ./W261/Out/HW2_1\n",
      "\n",
      "Display head of file to proove run worked:\n",
      "15/09/12 19:26:31 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "0\tNA\n",
      "0\tNA\n",
      "100\tNA\n",
      "1000\tNA\n",
      "1002\tNA\n",
      "1003\tNA\n",
      "1004\tNA\n",
      "1005\tNA\n",
      "1005\tNA\n",
      "1005\tNA\n",
      "1005\tNA\n",
      "1007\tNA\n",
      "1009\tNA\n",
      "1009\tNA\n",
      "101\tNA\n",
      "cat: Unable to write to output stream.\n"
     ]
    }
   ],
   "source": [
    "# HW2.1: Execute a job using Hadoop Streaming to generate 10,000 random integers and sort them.\n",
    "def HW2_1():\n",
    "    !hadoop jar /usr/local/Cellar/hadoop/2.6.0/libexec/share/hadoop/tools/lib/hadoop-streaming-2.6.0.jar \\\n",
    "    -Dmapreduce.job.maps=10 \\\n",
    "    -Dmapreduce.job.reduces=1 \\\n",
    "    -files ./mapper.py,./reducer.py \\\n",
    "    -mapper ./mapper.py  \\\n",
    "    -reducer ./reducer.py \\\n",
    "    -input ./W261/In/HW2/dummy.txt -output ./W261/Out/HW2_1\n",
    "    \n",
    "    print\n",
    "    print \"Display head of file to prove run worked:\"\n",
    "    !hadoop fs -cat ./W261/Out/HW2_1/part-00000 | head -n15\n",
    "\n",
    "HW2_1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:silver\"><b>HW2.2.</b> Using the Enron data from HW1 and Hadoop MapReduce streaming, write mapper/reducer pair that  will determine the number of occurrences of a single, user-specified word. Examine the word “assistance” and report your results.</style>\n",
    "\n",
    "\n",
    "   <span style=\"color:silver\">To do so, make sure that</span>\n",
    "   \n",
    "   - <span style=\"color:silver\">mapper.py counts all occurrences of a single word, and</span>\n",
    "   - <span style=\"color:silver\">reducer.py collates the counts of the single word.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "\n",
    "findword = sys.argv[1]\n",
    "\n",
    "# input comes from standard input\n",
    "for full_email in sys.stdin:\n",
    "\n",
    "    # Parse out email body for processing. Find body using \"tab spam/ham tab\"\n",
    "    # use regex to strip out non alpha-numeric. \"don't\" will become \"dont\" which is fine.\n",
    "    keyword = re.findall(\"\\t[0-1]\\t\",full_email)[0]\n",
    "    email_id, is_spam_tabbed, email_body = full_email.partition(keyword)\n",
    "    email_body = re.sub('[^A-Za-z0-9\\s]+', '', email_body)\n",
    "\n",
    "    for word in email_body.split():\n",
    "        if word == findword:\n",
    "            print '%s\\t%s' % (word, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "\n",
    "# input comes from standard input\n",
    "for line in sys.stdin:\n",
    "    # parse the input we got from mapper.py\n",
    "    word, count = line.strip().split('\\t', 1)\n",
    "    count = int(count)\n",
    "\n",
    "    # take advantage of sorted keys\n",
    "    if current_word == word:\n",
    "        current_count += count\n",
    "    else:\n",
    "        if current_word:\n",
    "            # print result when word changes\n",
    "            print \"{}\\t{}\".format(current_word, current_count)\n",
    "        current_count = count\n",
    "        current_word = word\n",
    "\n",
    "# print final word\n",
    "if current_word == word:\n",
    "    print \"{}\\t{}\".format(current_word, current_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/12 21:06:25 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "# Move input file to HDFS\n",
    "!hdfs dfs -put ./enronemail_1h.txt ./W261/In/HW2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/12 21:16:16 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/12 21:16:17 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "15/09/12 21:16:17 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "15/09/12 21:16:17 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "15/09/12 21:16:17 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "15/09/12 21:16:17 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "15/09/12 21:16:17 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local299837932_0001\n",
      "15/09/12 21:16:18 INFO mapred.LocalDistributedCacheManager: Localized file:/Users/cjllop/Code/MIDS/W261/HW/W2/mapper.py as file:/usr/local/Cellar/hadoop/hdfs/tmp/mapred/local/1442106978099/mapper.py\n",
      "15/09/12 21:16:18 INFO mapred.LocalDistributedCacheManager: Localized file:/Users/cjllop/Code/MIDS/W261/HW/W2/reducer.py as file:/usr/local/Cellar/hadoop/hdfs/tmp/mapred/local/1442106978100/reducer.py\n",
      "15/09/12 21:16:18 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "15/09/12 21:16:18 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "15/09/12 21:16:18 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "15/09/12 21:16:18 INFO mapreduce.Job: Running job: job_local299837932_0001\n",
      "15/09/12 21:16:18 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "15/09/12 21:16:18 INFO mapred.LocalJobRunner: Starting task: attempt_local299837932_0001_m_000000_0\n",
      "15/09/12 21:16:18 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "15/09/12 21:16:18 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "15/09/12 21:16:18 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/cjllop/W261/In/HW2/enronemail_1h.txt:0+203983\n",
      "15/09/12 21:16:18 INFO mapred.MapTask: numReduceTasks: 1\n",
      "15/09/12 21:16:19 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "15/09/12 21:16:19 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "15/09/12 21:16:19 INFO mapred.MapTask: soft limit at 83886080\n",
      "15/09/12 21:16:19 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "15/09/12 21:16:19 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "15/09/12 21:16:19 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "15/09/12 21:16:19 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/cjllop/Code/MIDS/W261/HW/W2/././mapper.py, assistance]\n",
      "15/09/12 21:16:19 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "15/09/12 21:16:19 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "15/09/12 21:16:19 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "15/09/12 21:16:19 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "15/09/12 21:16:19 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "15/09/12 21:16:19 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "15/09/12 21:16:19 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "15/09/12 21:16:19 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "15/09/12 21:16:19 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "15/09/12 21:16:19 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "15/09/12 21:16:19 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "15/09/12 21:16:19 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "15/09/12 21:16:19 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/12 21:16:19 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/12 21:16:19 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/12 21:16:19 INFO streaming.PipeMapRed: Records R/W=100/1\n",
      "15/09/12 21:16:19 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/09/12 21:16:19 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/09/12 21:16:19 INFO mapred.LocalJobRunner: \n",
      "15/09/12 21:16:19 INFO mapred.MapTask: Starting flush of map output\n",
      "15/09/12 21:16:19 INFO mapred.MapTask: Spilling map output\n",
      "15/09/12 21:16:19 INFO mapred.MapTask: bufstart = 0; bufend = 130; bufvoid = 104857600\n",
      "15/09/12 21:16:19 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214360(104857440); length = 37/6553600\n",
      "15/09/12 21:16:19 INFO mapred.MapTask: Finished spill 0\n",
      "15/09/12 21:16:19 INFO mapred.Task: Task:attempt_local299837932_0001_m_000000_0 is done. And is in the process of committing\n",
      "15/09/12 21:16:19 INFO mapred.LocalJobRunner: Records R/W=100/1\n",
      "15/09/12 21:16:19 INFO mapred.Task: Task 'attempt_local299837932_0001_m_000000_0' done.\n",
      "15/09/12 21:16:19 INFO mapred.LocalJobRunner: Finishing task: attempt_local299837932_0001_m_000000_0\n",
      "15/09/12 21:16:19 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "15/09/12 21:16:19 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "15/09/12 21:16:19 INFO mapred.LocalJobRunner: Starting task: attempt_local299837932_0001_r_000000_0\n",
      "15/09/12 21:16:19 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "15/09/12 21:16:19 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "15/09/12 21:16:19 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@17eda64e\n",
      "15/09/12 21:16:19 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=371130368, maxSingleShuffleLimit=92782592, mergeThreshold=244946048, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "15/09/12 21:16:19 INFO reduce.EventFetcher: attempt_local299837932_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "15/09/12 21:16:19 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local299837932_0001_m_000000_0 decomp: 152 len: 156 to MEMORY\n",
      "15/09/12 21:16:19 INFO reduce.InMemoryMapOutput: Read 152 bytes from map-output for attempt_local299837932_0001_m_000000_0\n",
      "15/09/12 21:16:19 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 152, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->152\n",
      "15/09/12 21:16:19 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "15/09/12 21:16:19 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/12 21:16:19 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "15/09/12 21:16:19 INFO mapred.Merger: Merging 1 sorted segments\n",
      "15/09/12 21:16:19 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 139 bytes\n",
      "15/09/12 21:16:19 INFO reduce.MergeManagerImpl: Merged 1 segments, 152 bytes to disk to satisfy reduce memory limit\n",
      "15/09/12 21:16:19 INFO reduce.MergeManagerImpl: Merging 1 files, 156 bytes from disk\n",
      "15/09/12 21:16:19 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "15/09/12 21:16:19 INFO mapred.Merger: Merging 1 sorted segments\n",
      "15/09/12 21:16:19 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 139 bytes\n",
      "15/09/12 21:16:19 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/12 21:16:19 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/cjllop/Code/MIDS/W261/HW/W2/././reducer.py]\n",
      "15/09/12 21:16:19 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "15/09/12 21:16:19 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "15/09/12 21:16:19 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/12 21:16:19 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/12 21:16:19 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/09/12 21:16:19 INFO streaming.PipeMapRed: Records R/W=10/1\n",
      "15/09/12 21:16:19 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/09/12 21:16:19 INFO mapreduce.Job: Job job_local299837932_0001 running in uber mode : false\n",
      "15/09/12 21:16:19 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "15/09/12 21:16:19 INFO mapred.Task: Task:attempt_local299837932_0001_r_000000_0 is done. And is in the process of committing\n",
      "15/09/12 21:16:19 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/12 21:16:19 INFO mapred.Task: Task attempt_local299837932_0001_r_000000_0 is allowed to commit now\n",
      "15/09/12 21:16:19 INFO output.FileOutputCommitter: Saved output of task 'attempt_local299837932_0001_r_000000_0' to hdfs://localhost:9000/user/cjllop/W261/Out/HW2_2/_temporary/0/task_local299837932_0001_r_000000\n",
      "15/09/12 21:16:19 INFO mapred.LocalJobRunner: Records R/W=10/1 > reduce\n",
      "15/09/12 21:16:19 INFO mapred.Task: Task 'attempt_local299837932_0001_r_000000_0' done.\n",
      "15/09/12 21:16:19 INFO mapred.LocalJobRunner: Finishing task: attempt_local299837932_0001_r_000000_0\n",
      "15/09/12 21:16:19 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "15/09/12 21:16:20 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "15/09/12 21:16:20 INFO mapreduce.Job: Job job_local299837932_0001 completed successfully\n",
      "15/09/12 21:16:20 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=213666\n",
      "\t\tFILE: Number of bytes written=753714\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=407966\n",
      "\t\tHDFS: Number of bytes written=14\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=10\n",
      "\t\tMap output bytes=130\n",
      "\t\tMap output materialized bytes=156\n",
      "\t\tInput split bytes=115\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce shuffle bytes=156\n",
      "\t\tReduce input records=10\n",
      "\t\tReduce output records=1\n",
      "\t\tSpilled Records=20\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=5\n",
      "\t\tTotal committed heap usage (bytes)=397893632\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=203983\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=14\n",
      "15/09/12 21:16:20 INFO streaming.StreamJob: Output directory: ./W261/Out/HW2_2\n",
      "\n",
      "Display head of file to proove run worked:\n",
      "15/09/12 21:16:22 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "assistance\t10\n",
      "Running Crosscheck...\n",
      "Check Result: 10\n"
     ]
    }
   ],
   "source": [
    "# HW2.2: Execute a job using Hadoop Streaming to search the input file for a user-specified word\n",
    "def HW2_2(term=\"assistance\"):\n",
    "    !hadoop jar /usr/local/Cellar/hadoop/2.6.0/libexec/share/hadoop/tools/lib/hadoop-streaming-2.6.0.jar \\\n",
    "    -Dmapreduce.job.maps=10 \\\n",
    "    -Dmapreduce.job.reduces=1 \\\n",
    "    -files ./mapper.py,./reducer.py \\\n",
    "    -mapper './mapper.py {term}' \\\n",
    "    -reducer ./reducer.py \\\n",
    "    -input ./W261/In/HW2/enronemail_1h.txt -output ./W261/Out/HW2_2\n",
    "    \n",
    "    print\n",
    "    print \"Display head of file to prove run worked:\"\n",
    "    !hadoop fs -cat ./W261/Out/HW2_2/part-00000 | head -n15\n",
    "\n",
    "    # Crosscheck results (data is small enough to use RE in python)\n",
    "    print \"Running Crosscheck...\"\n",
    "    with open (\"enronemail_1h.txt\", \"r\") as myfile:\n",
    "        print \"Check Result:\", len(re.findall(findword,myfile.read()))\n",
    "        \n",
    "HW2_2(term=\"assistance\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:silver\"><b>HW2.3.</b> Using the Enron data from HW1 and Hadoop MapReduce, write  a mapper/reducer pair that\n",
    "   will classify the email messages by a single, user-specified word. Examine the word “assistance” and report your results. To do so, make sure that</style>\n",
    "   \n",
    "   - <span style=\"color:silver\">mapper.py</span>\n",
    "   - <span style=\"color:silver\">reducer.py </span>\n",
    "\n",
    "   <span style=\"color:silver\">performs a single word multinomial Naive Bayes classification.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "\n",
    "# Get search term(s)\n",
    "findword = sys.argv[1]\n",
    "\n",
    "for full_email in sys.stdin:\n",
    "#with open (findfile, \"r\") as myfile:\n",
    "#    for full_email in myfile:\n",
    "    # Empty dictionary\n",
    "    term_hits = {}\n",
    "\n",
    "    # Spam classification\n",
    "    is_spam = re.findall(\"\\t([0-1])\\t\",full_email)[0]\n",
    "\n",
    "    # Parse out email body for processing. Find body using \"tab spam/ham tab\"\n",
    "    # use regex to strip out non alpha-numeric. \"don't\" will become \"dont\" which is fine for classifying.\n",
    "    keyword = re.findall(\"\\t[0-1]\\t\",full_email)[0]\n",
    "    email_id, is_spam_tabbed, email_body = full_email.partition(keyword)\n",
    "    email_body = re.sub('[^A-Za-z0-9\\s]+', '', email_body)\n",
    "    # Must process search query and email bodies the same\n",
    "    findword = re.sub('[^A-Za-z0-9\\s]+', '', findword)\n",
    "    email_len = len(email_body.split())\n",
    "\n",
    "    # Build counts of term words. \n",
    "    for word in list(set(email_body.split())):\n",
    "        term_hits[word] = len(re.findall(word,email_body))\n",
    "\n",
    "    # Print as tuple with unique splitter \"|||\"\n",
    "    print \"{}\\t{} ||| {} ||| {}\".format(email_id, is_spam, email_len, term_hits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import ast\n",
    "import math\n",
    "\n",
    "# Get search term(s)\n",
    "findword = sys.argv[1]\n",
    "search_terms = findword.split()\n",
    "\n",
    "# Parse all mapper results into a list so we can loop through again to predict after \n",
    "# looping through to train the model\n",
    "mapper_results = []\n",
    "for line in sys.stdin:\n",
    "    mapper_results.append(line)\n",
    "\n",
    "spam_term_counts = {}\n",
    "ham_term_counts = {}\n",
    "word_prob_given_spam = {}\n",
    "word_prob_given_ham = {}\n",
    "spam_count = 0\n",
    "ham_count = 0\n",
    "spam_len = 0\n",
    "ham_len = 0\n",
    "distinct_term_list = []\n",
    "\n",
    "# Open each file and build Multinomial Naive Bayes model\n",
    "for processed_email in mapper_results:\n",
    "    # Read in tuples created by mapper\n",
    "    email_id, processed_email = processed_email.split(\"\\t\")\n",
    "    processed_email = processed_email.split(\" ||| \")\n",
    "    is_spam = int(processed_email[0])\n",
    "    email_len = int(processed_email[1])\n",
    "    count_dict = ast.literal_eval(processed_email[2])\n",
    "\n",
    "    # Build counts for spam and ham definitions.\n",
    "    if is_spam:\n",
    "        for key, value in count_dict.iteritems():\n",
    "            spam_term_counts[key] = spam_term_counts.get(key, 0) + value\n",
    "        spam_count += 1\n",
    "        spam_len += email_len\n",
    "    else:\n",
    "        for key, value in count_dict.iteritems():\n",
    "            ham_term_counts[key] = ham_term_counts.get(key, 0) + value\n",
    "        ham_count += 1\n",
    "        ham_len += email_len\n",
    "\n",
    "    distinct_term_list = list(set(distinct_term_list + count_dict.keys()))\n",
    "\n",
    "# Calculate our priors based on the overall ratio of spam to ham\n",
    "spam_prior = float(spam_count) / (spam_count + ham_count)\n",
    "ham_prior = 1 - spam_prior\n",
    "spam_prior = math.log10(spam_prior)\n",
    "ham_prior = math.log10(ham_prior)\n",
    "\n",
    "# Calculate our conditional probabilites for the search term using MNB formula\n",
    "#     term_given_spam = (spam_term_count + 1 for smoothing) / (total count of spam words + total distinct vocab size)\n",
    "distinct_term_count = len(distinct_term_list)\n",
    "\n",
    "for term in search_terms:\n",
    "    word_prob_given_spam[term] = math.log10((spam_term_counts.get(term,0) + 1.0) / (float(spam_len) + distinct_term_count))\n",
    "    word_prob_given_ham[term] = math.log10((ham_term_counts.get(term,0) + 1.0) / (float(ham_len) + distinct_term_count))\n",
    "\n",
    "# Now let's predict!\n",
    "accuracy = []\n",
    "for processed_email in mapper_results:\n",
    "    # Defaults\n",
    "    pred_spam = 0\n",
    "    spam_prediction = spam_prior\n",
    "    ham_prediction = ham_prior\n",
    "\n",
    "    # Read in tuples created by mapper\n",
    "    email_id, processed_email = processed_email.split(\"\\t\")\n",
    "    processed_email = processed_email.split(\" ||| \")\n",
    "    is_spam = int(processed_email[0])\n",
    "    count_dict = ast.literal_eval(processed_email[2])\n",
    "\n",
    "    # Read in counts to use in prediction\n",
    "    for term in word_prob_given_spam.keys():\n",
    "        # Calculate the probability for each class\n",
    "        spam_prediction += (word_prob_given_spam[term] * count_dict.get(term, 0))\n",
    "        ham_prediction += (word_prob_given_ham[term] * count_dict.get(term, 0))\n",
    "\n",
    "    # Pick the higher probability\n",
    "    if spam_prediction > ham_prediction: \n",
    "        pred_spam = 1\n",
    "\n",
    "    # Store accuracy in a list\n",
    "    accuracy.append(1*(pred_spam==is_spam))\n",
    "\n",
    "    # Print predictions to results file\n",
    "    print '{}\\t{}\\t{}'.format(email_id, is_spam, pred_spam)\n",
    "\n",
    "# Print accuracy\n",
    "sys.stderr.write(\"\\nSpam Probs: {}\\n\".format(word_prob_given_spam))\n",
    "sys.stderr.write(\"Ham Probs: {}\\n\".format(word_prob_given_ham))\n",
    "sys.stderr.write(\"Accuracy = {:.2f}\\n\".format(float(sum(accuracy))/len(accuracy)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/12 21:57:31 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/12 21:57:32 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "15/09/12 21:57:32 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "15/09/12 21:57:32 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "15/09/12 21:57:33 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "15/09/12 21:57:33 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "15/09/12 21:57:33 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local540684409_0001\n",
      "15/09/12 21:57:34 INFO mapred.LocalDistributedCacheManager: Localized file:/Users/cjllop/Code/MIDS/W261/HW/W2/mapper.py as file:/usr/local/Cellar/hadoop/hdfs/tmp/mapred/local/1442109453919/mapper.py\n",
      "15/09/12 21:57:34 INFO mapred.LocalDistributedCacheManager: Localized file:/Users/cjllop/Code/MIDS/W261/HW/W2/reducer.py as file:/usr/local/Cellar/hadoop/hdfs/tmp/mapred/local/1442109453920/reducer.py\n",
      "15/09/12 21:57:34 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "15/09/12 21:57:34 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "15/09/12 21:57:34 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "15/09/12 21:57:34 INFO mapreduce.Job: Running job: job_local540684409_0001\n",
      "15/09/12 21:57:34 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "15/09/12 21:57:34 INFO mapred.LocalJobRunner: Starting task: attempt_local540684409_0001_m_000000_0\n",
      "15/09/12 21:57:34 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "15/09/12 21:57:34 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "15/09/12 21:57:34 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/cjllop/W261/In/HW2/enronemail_1h.txt:0+203983\n",
      "15/09/12 21:57:34 INFO mapred.MapTask: numReduceTasks: 1\n",
      "15/09/12 21:57:34 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "15/09/12 21:57:34 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "15/09/12 21:57:34 INFO mapred.MapTask: soft limit at 83886080\n",
      "15/09/12 21:57:34 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "15/09/12 21:57:34 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "15/09/12 21:57:34 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "15/09/12 21:57:34 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/cjllop/Code/MIDS/W261/HW/W2/././mapper.py, assistance]\n",
      "15/09/12 21:57:34 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "15/09/12 21:57:34 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "15/09/12 21:57:34 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "15/09/12 21:57:34 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "15/09/12 21:57:34 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "15/09/12 21:57:34 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "15/09/12 21:57:34 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "15/09/12 21:57:34 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "15/09/12 21:57:34 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "15/09/12 21:57:34 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "15/09/12 21:57:34 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "15/09/12 21:57:34 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "15/09/12 21:57:35 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/12 21:57:35 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/12 21:57:35 INFO mapreduce.Job: Job job_local540684409_0001 running in uber mode : false\n",
      "15/09/12 21:57:35 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "15/09/12 21:57:36 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:100=100/1 [rec/s] out:0=0/1 [rec/s]\n",
      "15/09/12 21:57:36 INFO streaming.PipeMapRed: Records R/W=100/1\n",
      "15/09/12 21:57:37 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/09/12 21:57:37 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/09/12 21:57:37 INFO mapred.LocalJobRunner: \n",
      "15/09/12 21:57:37 INFO mapred.MapTask: Starting flush of map output\n",
      "15/09/12 21:57:37 INFO mapred.MapTask: Spilling map output\n",
      "15/09/12 21:57:37 INFO mapred.MapTask: bufstart = 0; bufend = 199737; bufvoid = 104857600\n",
      "15/09/12 21:57:37 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214000(104856000); length = 397/6553600\n",
      "15/09/12 21:57:37 INFO mapred.MapTask: Finished spill 0\n",
      "15/09/12 21:57:37 INFO mapred.Task: Task:attempt_local540684409_0001_m_000000_0 is done. And is in the process of committing\n",
      "15/09/12 21:57:37 INFO mapred.LocalJobRunner: Records R/W=100/1\n",
      "15/09/12 21:57:37 INFO mapred.Task: Task 'attempt_local540684409_0001_m_000000_0' done.\n",
      "15/09/12 21:57:37 INFO mapred.LocalJobRunner: Finishing task: attempt_local540684409_0001_m_000000_0\n",
      "15/09/12 21:57:37 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "15/09/12 21:57:37 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "15/09/12 21:57:37 INFO mapred.LocalJobRunner: Starting task: attempt_local540684409_0001_r_000000_0\n",
      "15/09/12 21:57:37 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "15/09/12 21:57:37 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "15/09/12 21:57:37 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@25a9cf52\n",
      "15/09/12 21:57:37 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=371130368, maxSingleShuffleLimit=92782592, mergeThreshold=244946048, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "15/09/12 21:57:37 INFO reduce.EventFetcher: attempt_local540684409_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "15/09/12 21:57:37 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local540684409_0001_m_000000_0 decomp: 200128 len: 200132 to MEMORY\n",
      "15/09/12 21:57:37 INFO reduce.InMemoryMapOutput: Read 200128 bytes from map-output for attempt_local540684409_0001_m_000000_0\n",
      "15/09/12 21:57:37 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 200128, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->200128\n",
      "15/09/12 21:57:37 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "15/09/12 21:57:37 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/12 21:57:37 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "15/09/12 21:57:37 INFO mapred.Merger: Merging 1 sorted segments\n",
      "15/09/12 21:57:37 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 200103 bytes\n",
      "15/09/12 21:57:37 INFO reduce.MergeManagerImpl: Merged 1 segments, 200128 bytes to disk to satisfy reduce memory limit\n",
      "15/09/12 21:57:37 INFO reduce.MergeManagerImpl: Merging 1 files, 200132 bytes from disk\n",
      "15/09/12 21:57:37 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "15/09/12 21:57:37 INFO mapred.Merger: Merging 1 sorted segments\n",
      "15/09/12 21:57:37 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 200103 bytes\n",
      "15/09/12 21:57:37 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/12 21:57:37 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/cjllop/Code/MIDS/W261/HW/W2/././reducer.py, assistance]\n",
      "15/09/12 21:57:37 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "15/09/12 21:57:37 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "15/09/12 21:57:37 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/12 21:57:37 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/12 21:57:37 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "15/09/12 21:57:37 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "\n",
      "Spam Probs: {'assistance': -3.4264028097516}\n",
      "Ham Probs: {'assistance': -3.799891684656865}\n",
      "Accuracy = 0.60\n",
      "15/09/12 21:57:37 INFO streaming.PipeMapRed: Records R/W=100/1\n",
      "15/09/12 21:57:37 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/09/12 21:57:37 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/09/12 21:57:38 INFO mapred.Task: Task:attempt_local540684409_0001_r_000000_0 is done. And is in the process of committing\n",
      "15/09/12 21:57:38 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/12 21:57:38 INFO mapred.Task: Task attempt_local540684409_0001_r_000000_0 is allowed to commit now\n",
      "15/09/12 21:57:38 INFO output.FileOutputCommitter: Saved output of task 'attempt_local540684409_0001_r_000000_0' to hdfs://localhost:9000/user/cjllop/W261/Out/HW2_3/_temporary/0/task_local540684409_0001_r_000000\n",
      "15/09/12 21:57:38 INFO mapred.LocalJobRunner: Records R/W=100/1 > reduce\n",
      "15/09/12 21:57:38 INFO mapred.Task: Task 'attempt_local540684409_0001_r_000000_0' done.\n",
      "15/09/12 21:57:38 INFO mapred.LocalJobRunner: Finishing task: attempt_local540684409_0001_r_000000_0\n",
      "15/09/12 21:57:38 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "15/09/12 21:57:38 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "15/09/12 21:57:38 INFO mapreduce.Job: Job job_local540684409_0001 completed successfully\n",
      "15/09/12 21:57:38 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=619490\n",
      "\t\tFILE: Number of bytes written=1359614\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=407966\n",
      "\t\tHDFS: Number of bytes written=2672\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=100\n",
      "\t\tMap output bytes=199737\n",
      "\t\tMap output materialized bytes=200132\n",
      "\t\tInput split bytes=115\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=100\n",
      "\t\tReduce shuffle bytes=200132\n",
      "\t\tReduce input records=100\n",
      "\t\tReduce output records=100\n",
      "\t\tSpilled Records=200\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=4\n",
      "\t\tTotal committed heap usage (bytes)=397893632\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=203983\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2672\n",
      "15/09/12 21:57:38 INFO streaming.StreamJob: Output directory: ./W261/Out/HW2_3\n",
      "\n",
      "Display head of file to proove run worked:\n",
      "15/09/12 21:57:40 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "0001.1999-12-10.farmer\t0\t0\n",
      "0001.1999-12-10.kaminski\t0\t0\n",
      "0001.2000-01-17.beck\t0\t0\n",
      "0001.2000-06-06.lokay\t0\t0\n",
      "0001.2001-02-07.kitchen\t0\t0\n",
      "0001.2001-04-02.williams\t0\t0\n",
      "0002.1999-12-13.farmer\t0\t0\n",
      "0002.2001-02-07.kitchen\t0\t0\n",
      "0002.2001-05-25.SA_and_HP\t1\t0\n",
      "0002.2003-12-18.GP\t1\t0\n",
      "0002.2004-08-01.BG\t1\t1\n",
      "0003.1999-12-10.kaminski\t0\t0\n",
      "0003.1999-12-14.farmer\t0\t0\n",
      "0003.2000-01-17.beck\t0\t0\n",
      "0003.2001-02-08.kitchen\t0\t0\n"
     ]
    }
   ],
   "source": [
    "# HW2.3: Predict via MNBusing Hadoop Streaming for a user-specified word\n",
    "def HW2_3(term=\"assistance\"):\n",
    "    !hadoop jar /usr/local/Cellar/hadoop/2.6.0/libexec/share/hadoop/tools/lib/hadoop-streaming-2.6.0.jar \\\n",
    "    -Dmapreduce.job.maps=10 \\\n",
    "    -Dmapreduce.job.reduces=1 \\\n",
    "    -files ./mapper.py,./reducer.py \\\n",
    "    -mapper './mapper.py {term}' \\\n",
    "    -reducer './reducer.py {term}' \\\n",
    "    -input ./W261/In/HW2/enronemail_1h.txt -output ./W261/Out/HW2_3\n",
    "    \n",
    "    print\n",
    "    print \"Display head of file to prove run worked:\"\n",
    "    !hadoop fs -cat ./W261/Out/HW2_3/part-00000 | head -n15\n",
    "        \n",
    "HW2_3(term=\"assistance\")\n",
    "\n",
    "# Note - output shows same accuracy of 60% that we saw in HW1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:black\"><b>HW2.4.</b></style> Using the Enron data from HW1 and in the Hadoop MapReduce framework, write  a mapper/reducer pair that\n",
    "   will classify the email messages using multinomial Naive Bayes Classifier using a list of one or more user-specified words. Examine the words “assistance”, “valium”, and “enlargementWithATypo” and report your results\n",
    "   To do so, make sure that\n",
    "\n",
    "   - mapper.py \n",
    "   - reducer.py \n",
    "\n",
    "   performs the multiple-word multinomial Naive Bayes classification via the chosen list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/12 22:07:07 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/12 22:07:08 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "15/09/12 22:07:08 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "15/09/12 22:07:08 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "15/09/12 22:07:08 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "15/09/12 22:07:08 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "15/09/12 22:07:09 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local864627615_0001\n",
      "15/09/12 22:07:09 INFO mapred.LocalDistributedCacheManager: Localized file:/Users/cjllop/Code/MIDS/W261/HW/W2/mapper.py as file:/usr/local/Cellar/hadoop/hdfs/tmp/mapred/local/1442110029290/mapper.py\n",
      "15/09/12 22:07:09 INFO mapred.LocalDistributedCacheManager: Localized file:/Users/cjllop/Code/MIDS/W261/HW/W2/reducer.py as file:/usr/local/Cellar/hadoop/hdfs/tmp/mapred/local/1442110029291/reducer.py\n",
      "15/09/12 22:07:09 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "15/09/12 22:07:09 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "15/09/12 22:07:09 INFO mapreduce.Job: Running job: job_local864627615_0001\n",
      "15/09/12 22:07:09 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "15/09/12 22:07:09 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "15/09/12 22:07:09 INFO mapred.LocalJobRunner: Starting task: attempt_local864627615_0001_m_000000_0\n",
      "15/09/12 22:07:09 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "15/09/12 22:07:09 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "15/09/12 22:07:09 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/cjllop/W261/In/HW2/enronemail_1h.txt:0+203983\n",
      "15/09/12 22:07:09 INFO mapred.MapTask: numReduceTasks: 1\n",
      "15/09/12 22:07:10 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "15/09/12 22:07:10 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "15/09/12 22:07:10 INFO mapred.MapTask: soft limit at 83886080\n",
      "15/09/12 22:07:10 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "15/09/12 22:07:10 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "15/09/12 22:07:10 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "15/09/12 22:07:10 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/cjllop/Code/MIDS/W261/HW/W2/././mapper.py, assistance valium enlargementWithATypo]\n",
      "15/09/12 22:07:10 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "15/09/12 22:07:10 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "15/09/12 22:07:10 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "15/09/12 22:07:10 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "15/09/12 22:07:10 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "15/09/12 22:07:10 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "15/09/12 22:07:10 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "15/09/12 22:07:10 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "15/09/12 22:07:10 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "15/09/12 22:07:10 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "15/09/12 22:07:10 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "15/09/12 22:07:10 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "15/09/12 22:07:10 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/12 22:07:10 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/12 22:07:10 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/12 22:07:10 INFO streaming.PipeMapRed: Records R/W=100/1\n",
      "15/09/12 22:07:10 INFO mapreduce.Job: Job job_local864627615_0001 running in uber mode : false\n",
      "15/09/12 22:07:10 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "15/09/12 22:07:11 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/09/12 22:07:11 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/09/12 22:07:11 INFO mapred.LocalJobRunner: \n",
      "15/09/12 22:07:11 INFO mapred.MapTask: Starting flush of map output\n",
      "15/09/12 22:07:11 INFO mapred.MapTask: Spilling map output\n",
      "15/09/12 22:07:11 INFO mapred.MapTask: bufstart = 0; bufend = 199737; bufvoid = 104857600\n",
      "15/09/12 22:07:11 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214000(104856000); length = 397/6553600\n",
      "15/09/12 22:07:11 INFO mapred.MapTask: Finished spill 0\n",
      "15/09/12 22:07:11 INFO mapred.Task: Task:attempt_local864627615_0001_m_000000_0 is done. And is in the process of committing\n",
      "15/09/12 22:07:11 INFO mapred.LocalJobRunner: Records R/W=100/1\n",
      "15/09/12 22:07:11 INFO mapred.Task: Task 'attempt_local864627615_0001_m_000000_0' done.\n",
      "15/09/12 22:07:11 INFO mapred.LocalJobRunner: Finishing task: attempt_local864627615_0001_m_000000_0\n",
      "15/09/12 22:07:11 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "15/09/12 22:07:11 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "15/09/12 22:07:11 INFO mapred.LocalJobRunner: Starting task: attempt_local864627615_0001_r_000000_0\n",
      "15/09/12 22:07:11 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "15/09/12 22:07:11 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "15/09/12 22:07:11 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@74c12978\n",
      "15/09/12 22:07:11 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=371130368, maxSingleShuffleLimit=92782592, mergeThreshold=244946048, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "15/09/12 22:07:11 INFO reduce.EventFetcher: attempt_local864627615_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "15/09/12 22:07:11 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local864627615_0001_m_000000_0 decomp: 200128 len: 200132 to MEMORY\n",
      "15/09/12 22:07:11 INFO reduce.InMemoryMapOutput: Read 200128 bytes from map-output for attempt_local864627615_0001_m_000000_0\n",
      "15/09/12 22:07:11 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 200128, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->200128\n",
      "15/09/12 22:07:11 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "15/09/12 22:07:11 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/12 22:07:11 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "15/09/12 22:07:11 INFO mapred.Merger: Merging 1 sorted segments\n",
      "15/09/12 22:07:11 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 200103 bytes\n",
      "15/09/12 22:07:11 INFO reduce.MergeManagerImpl: Merged 1 segments, 200128 bytes to disk to satisfy reduce memory limit\n",
      "15/09/12 22:07:11 INFO reduce.MergeManagerImpl: Merging 1 files, 200132 bytes from disk\n",
      "15/09/12 22:07:11 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "15/09/12 22:07:11 INFO mapred.Merger: Merging 1 sorted segments\n",
      "15/09/12 22:07:11 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 200103 bytes\n",
      "15/09/12 22:07:11 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/12 22:07:11 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/cjllop/Code/MIDS/W261/HW/W2/././reducer.py, assistance valium enlargementWithATypo]\n",
      "15/09/12 22:07:11 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "15/09/12 22:07:11 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "15/09/12 22:07:11 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/12 22:07:11 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/12 22:07:11 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/12 22:07:11 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "\n",
      "Spam Probs: {'assistance': -3.4264028097516, 'enlargementWithATypo': -4.380645319190925, 'valium': -3.778585327862962}\n",
      "Ham Probs: {'assistance': -3.799891684656865, 'enlargementWithATypo': -4.277012939376528, 'valium': -4.277012939376528}\n",
      "Accuracy = 0.63\n",
      "15/09/12 22:07:12 INFO streaming.PipeMapRed: Records R/W=100/1\n",
      "15/09/12 22:07:12 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/09/12 22:07:12 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/09/12 22:07:12 INFO mapred.Task: Task:attempt_local864627615_0001_r_000000_0 is done. And is in the process of committing\n",
      "15/09/12 22:07:12 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/12 22:07:12 INFO mapred.Task: Task attempt_local864627615_0001_r_000000_0 is allowed to commit now\n",
      "15/09/12 22:07:12 INFO output.FileOutputCommitter: Saved output of task 'attempt_local864627615_0001_r_000000_0' to hdfs://localhost:9000/user/cjllop/W261/Out/HW2_4/_temporary/0/task_local864627615_0001_r_000000\n",
      "15/09/12 22:07:12 INFO mapred.LocalJobRunner: Records R/W=100/1 > reduce\n",
      "15/09/12 22:07:12 INFO mapred.Task: Task 'attempt_local864627615_0001_r_000000_0' done.\n",
      "15/09/12 22:07:12 INFO mapred.LocalJobRunner: Finishing task: attempt_local864627615_0001_r_000000_0\n",
      "15/09/12 22:07:12 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "15/09/12 22:07:12 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "15/09/12 22:07:12 INFO mapreduce.Job: Job job_local864627615_0001 completed successfully\n",
      "15/09/12 22:07:12 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=619490\n",
      "\t\tFILE: Number of bytes written=1359870\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=407966\n",
      "\t\tHDFS: Number of bytes written=2672\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=100\n",
      "\t\tMap output bytes=199737\n",
      "\t\tMap output materialized bytes=200132\n",
      "\t\tInput split bytes=115\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=100\n",
      "\t\tReduce shuffle bytes=200132\n",
      "\t\tReduce input records=100\n",
      "\t\tReduce output records=100\n",
      "\t\tSpilled Records=200\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=5\n",
      "\t\tTotal committed heap usage (bytes)=397893632\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=203983\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2672\n",
      "15/09/12 22:07:12 INFO streaming.StreamJob: Output directory: ./W261/Out/HW2_4\n",
      "\n",
      "Display head of file to prove run worked:\n",
      "15/09/12 22:07:14 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "0001.1999-12-10.farmer\t0\t0\n",
      "0001.1999-12-10.kaminski\t0\t0\n",
      "0001.2000-01-17.beck\t0\t0\n",
      "0001.2000-06-06.lokay\t0\t0\n",
      "0001.2001-02-07.kitchen\t0\t0\n",
      "0001.2001-04-02.williams\t0\t0\n",
      "0002.1999-12-13.farmer\t0\t0\n",
      "0002.2001-02-07.kitchen\t0\t0\n",
      "0002.2001-05-25.SA_and_HP\t1\t0\n",
      "0002.2003-12-18.GP\t1\t0\n",
      "0002.2004-08-01.BG\t1\t1\n",
      "0003.1999-12-10.kaminski\t0\t0\n",
      "0003.1999-12-14.farmer\t0\t0\n",
      "0003.2000-01-17.beck\t0\t0\n",
      "0003.2001-02-08.kitchen\t0\t0\n"
     ]
    }
   ],
   "source": [
    "# HW2.4: Predict via MNBusing Hadoop Streaming for multiple user-specified words\n",
    "# Note - the solution program to HW2.3 can already do this. We just need to give it more terms.\n",
    "def HW2_4(term=\"assistance valium enlargementWithATypo\"):\n",
    "    !hadoop jar /usr/local/Cellar/hadoop/2.6.0/libexec/share/hadoop/tools/lib/hadoop-streaming-2.6.0.jar \\\n",
    "    -Dmapreduce.job.maps=10 \\\n",
    "    -Dmapreduce.job.reduces=1 \\\n",
    "    -files ./mapper.py,./reducer.py \\\n",
    "    -mapper './mapper.py \"{term}\"' \\\n",
    "    -reducer './reducer.py \"{term}\"' \\\n",
    "    -input ./W261/In/HW2/enronemail_1h.txt -output ./W261/Out/HW2_4\n",
    "    \n",
    "    print\n",
    "    print \"Display head of file to prove run worked:\"\n",
    "    !hadoop fs -cat ./W261/Out/HW2_4/part-00000 | head -n15\n",
    "        \n",
    "HW2_4(term=\"assistance valium enlargementWithATypo\")\n",
    "\n",
    "# Note - output shows same accuracy of 63% that we saw in HW1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/12 22:07:01 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/12 22:07:01 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted W261/Out/HW2_4\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -rm -r ./W261/Out/HW2_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:black\"><b>HW2.5.</b></style> Using the Enron data from HW1 an in the  Hadoop MapReduce framework, write  a mapper/reducer for a multinomial Naive Bayes Classifier that\n",
    "   will classify the email messages using  words present. Also drop words with a frequency of less than three (3). How does it affect the misclassifcation error of learnt naive multinomial Bayesian Classifiers on the training dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import ast\n",
    "import math\n",
    "\n",
    "filelist = sys.argv[1:]\n",
    "\n",
    "spam_term_counts = {}\n",
    "ham_term_counts = {}\n",
    "word_prob_given_spam = {}\n",
    "word_prob_given_ham = {}\n",
    "spam_count = 0\n",
    "ham_count = 0\n",
    "spam_len = 0\n",
    "ham_len = 0\n",
    "distinct_term_list = []\n",
    "\n",
    "# Open each file and build Multinomial Naive Bayes model\n",
    "for thisfile in filelist:\n",
    "    with open (thisfile, \"r\") as openfile:\n",
    "        for processed_email in openfile:\n",
    "            # Read in tuples created by mapper\n",
    "            processed_email = processed_email.split(\" ||| \")\n",
    "            search_terms = ast.literal_eval(processed_email[0])\n",
    "            email_id = processed_email[1]\n",
    "            is_spam = int(processed_email[2])\n",
    "            email_len = int(processed_email[3])\n",
    "            count_dict = ast.literal_eval(processed_email[4])\n",
    "            \n",
    "            # Build counts for spam and ham definitions.\n",
    "            if is_spam:\n",
    "                for key, value in count_dict.iteritems():\n",
    "                    spam_term_counts[key] = spam_term_counts.get(key, 0) + value\n",
    "                spam_count += 1\n",
    "                spam_len += email_len\n",
    "            else:\n",
    "                for key, value in count_dict.iteritems():\n",
    "                    ham_term_counts[key] = ham_term_counts.get(key, 0) + value\n",
    "                ham_count += 1\n",
    "                ham_len += email_len\n",
    "                \n",
    "            distinct_term_list = list(set(distinct_term_list + count_dict.keys()))\n",
    "\n",
    "# Calculate our priors based on the overall ratio of spam to ham\n",
    "spam_prior = float(spam_count) / (spam_count + ham_count)\n",
    "ham_prior = 1 - spam_prior\n",
    "spam_prior = math.log10(spam_prior)\n",
    "ham_prior = math.log10(ham_prior)\n",
    "\n",
    "# Calculate our conditional probabilites for the search term using MNB formula\n",
    "#     term_given_spam = (spam_term_count + 1 for smoothing) / (total count of spam words + total distinct vocab size)\n",
    "distinct_term_count = len(distinct_term_list)\n",
    "\n",
    "for term in search_terms:\n",
    "    word_prob_given_spam[term] = math.log10((spam_term_counts.get(term,0) + 1.0) / (float(spam_len) + distinct_term_count))\n",
    "    word_prob_given_ham[term] = math.log10((ham_term_counts.get(term,0) + 1.0) / (float(ham_len) + distinct_term_count))\n",
    "\n",
    "# Open each file and predict. Note - prediction is embaressingly parallel and could\n",
    "#   be done effectively via Mapping. However, the assignment asks us to solve the problem using\n",
    "#   the provided pNaiveBayes.sh.\n",
    "accuracy = []\n",
    "for thisfile in filelist:\n",
    "    with open (thisfile, \"r\") as openfile:\n",
    "        for processed_email in openfile:\n",
    "            # Defaults\n",
    "            pred_spam = 0\n",
    "            spam_prediction = spam_prior\n",
    "            ham_prediction = ham_prior\n",
    "\n",
    "            # Read in tuples created by mapper\n",
    "            processed_email = processed_email.split(\" ||| \")\n",
    "            email_id = processed_email[1]\n",
    "            is_spam = int(processed_email[2])\n",
    "            count_dict = ast.literal_eval(processed_email[4])\n",
    "            \n",
    "            # Read in counts to use in prediction\n",
    "            for term in word_prob_given_spam.keys():\n",
    "                # Calculate the probability for each class\n",
    "                spam_prediction += (word_prob_given_spam[term] * count_dict.get(term, 0))\n",
    "                ham_prediction += (word_prob_given_ham[term] * count_dict.get(term, 0))\n",
    "                \n",
    "            # Pick the higher probability\n",
    "            if spam_prediction > ham_prediction: \n",
    "                pred_spam = 1\n",
    "            \n",
    "            # Store accuracy in a list\n",
    "            accuracy.append(1*(pred_spam==is_spam))\n",
    "\n",
    "            # Print predictions to results file\n",
    "            print '{}\\t{}\\t{}'.format(email_id, is_spam, pred_spam)\n",
    "# Print accuracy\n",
    "sys.stderr.write(\"Spam Probs: {}\\n\".format(word_prob_given_spam))\n",
    "sys.stderr.write(\"Ham Probs: {}\\n\".format(word_prob_given_ham))\n",
    "sys.stderr.write(\"Accuracy = {:.2f}\\n\".format(float(sum(accuracy))/len(accuracy)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spam Probs: {'assistance': -3.4264028097516}\r\n",
      "Ham Probs: {'assistance': -3.799891684656865}\r\n",
      "Accuracy = 0.60\r\n"
     ]
    }
   ],
   "source": [
    "# HW 1.3: Create multinomial bayes map/reduce pair that predicts spam/ham using a single word\n",
    "def HW1_3(terms=\"assistance\"):\n",
    "    # Run pNaiveBayes.sh\n",
    "    !./pNaiveBayes.sh 10 \"{terms}\"\n",
    "        \n",
    "HW1_3(terms = \"assistance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:silver\"><b>HW1.4.</b> Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh\n",
    "   will classify the email messages by a list of one or more user-specified words. Examine the words “assistance”, “valium”, and “enlargementWithATypo” and report your results\n",
    "   To do so, make sure that</span>\n",
    " \n",
    "\n",
    "   - <span style=\"color:silver\">mapper.py counts all occurrences of a list of words, and</span>\n",
    "   - <span style=\"color:silver\">reducer.py performs the multiple-word Naive Bayes classification via the chosen list.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spam Probs: {'assistance': -3.4264028097516, 'enlargementWithATypo': -4.380645319190925, 'valium': -3.778585327862962}\r\n",
      "Ham Probs: {'assistance': -3.799891684656865, 'enlargementWithATypo': -4.277012939376528, 'valium': -4.277012939376528}\r\n",
      "Accuracy = 0.63\r\n"
     ]
    }
   ],
   "source": [
    "# HW 1.4: Our function for HW 1.3 can also classify multiple words, as requested by problem 1.4\n",
    "def HW1_4(words = \"assistance valium enlargementWithATypo\"):\n",
    "    # Run pNaiveBayes.sh\n",
    "    !./pNaiveBayes.sh 10 \"{words}\"\n",
    "            \n",
    "HW1_4(words = \"assistance valium enlargementWithATypo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:silver\"><b>HW1.5.</b> Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh will classify the email messages by all words present. To do so, make sure that</span>\n",
    "\n",
    "   - <span style=\"color:silver\">mapper.py counts all occurrences of all words, and</span>\n",
    "   - <span style=\"color:silver\">reducer.py performs a word-distribution-wide Naive Bayes classification.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "\n",
    "# Get input parameters\n",
    "findword = sys.argv[2]\n",
    "findfile = sys.argv[1]\n",
    "\n",
    "star_switch = 0\n",
    "if findword == \"*\":\n",
    "    star_switch = 1\n",
    "\n",
    "with open (findfile, \"r\") as myfile:\n",
    "    for full_email in myfile:\n",
    "        try:\n",
    "            # Empty dictionary\n",
    "            term_hits = {}\n",
    "\n",
    "            # Spam classification\n",
    "            is_spam = re.findall(\"\\t([0-1])\\t\",full_email)[0]\n",
    "\n",
    "            # Parse out email body for processing. Find body using \"tab spam/ham tab\"\n",
    "            # use regex to strip out non alpha-numeric. \"don't\" will become \"dont\" which is fine for classifying.\n",
    "            keyword = re.findall(\"\\t[0-1]\\t\",full_email)[0]\n",
    "            email_id, is_spam_tabbed, email_body = full_email.partition(keyword)\n",
    "            email_body = re.sub('[^A-Za-z0-9\\s]+', '', email_body)\n",
    "            # Must process search query and email bodies the same\n",
    "            # This is a good place to add logic to use all terms in body as search terms when \"*\" appears\n",
    "            if star_switch == 1:\n",
    "                findword = \" \".join(list(set(email_body.split())))\n",
    "            else:\n",
    "                findword = re.sub('[^A-Za-z0-9\\s]+', '', findword)\n",
    "            email_len = len(email_body.split())\n",
    "\n",
    "            # Build counts of term words.\n",
    "            for word in list(set(email_body.split())):\n",
    "                term_hits[word] = len(re.findall(word,email_body))\n",
    "\n",
    "            # Print as tuple with unique splitter \"|||\"\n",
    "            print \"{} ||| {} ||| {} ||| {} ||| {}\".format(re.findall(r'\\w+', findword), email_id, is_spam, email_len, term_hits)\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import ast\n",
    "import math\n",
    "\n",
    "filelist = sys.argv[1:]\n",
    "\n",
    "spam_term_counts = {}\n",
    "ham_term_counts = {}\n",
    "word_prob_given_spam = {}\n",
    "word_prob_given_ham = {}\n",
    "spam_count = 0\n",
    "ham_count = 0\n",
    "spam_len = 0\n",
    "ham_len = 0\n",
    "distinct_term_list = []\n",
    "\n",
    "# Open each file and build Multinomial Naive Bayes model\n",
    "for thisfile in filelist:\n",
    "    with open (thisfile, \"r\") as openfile:\n",
    "        for processed_email in openfile:\n",
    "            # Read in tuples created by mapper\n",
    "            processed_email = processed_email.split(\" ||| \")\n",
    "            search_terms = ast.literal_eval(processed_email[0])\n",
    "            email_id = processed_email[1]\n",
    "            is_spam = int(processed_email[2])\n",
    "            email_len = int(processed_email[3])\n",
    "            count_dict = ast.literal_eval(processed_email[4])\n",
    "            \n",
    "            # Build counts for spam and ham definitions.\n",
    "            if is_spam:\n",
    "                for key, value in count_dict.iteritems():\n",
    "                    spam_term_counts[key] = spam_term_counts.get(key, 0) + value\n",
    "                spam_count += 1\n",
    "                spam_len += email_len\n",
    "            else:\n",
    "                for key, value in count_dict.iteritems():\n",
    "                    ham_term_counts[key] = ham_term_counts.get(key, 0) + value\n",
    "                ham_count += 1\n",
    "                ham_len += email_len\n",
    "                \n",
    "            distinct_term_list = list(set(distinct_term_list + count_dict.keys()))\n",
    "\n",
    "# Calculate our priors based on the overall ratio of spam to ham\n",
    "spam_prior = float(spam_count) / (spam_count + ham_count)\n",
    "ham_prior = 1 - spam_prior\n",
    "spam_prior = math.log10(spam_prior)\n",
    "ham_prior = math.log10(ham_prior)\n",
    "\n",
    "# Calculate our conditional probabilites for the search term using MNB formula\n",
    "#     term_given_spam = (spam_term_count + 1 for smoothing) / (total count of spam words + total distinct vocab size)\n",
    "distinct_term_count = len(distinct_term_list)\n",
    "\n",
    "for term in distinct_term_list:\n",
    "    word_prob_given_spam[term] = math.log10((spam_term_counts.get(term,0) + 1.0) / (float(spam_len) + distinct_term_count))\n",
    "    word_prob_given_ham[term] = math.log10((ham_term_counts.get(term,0) + 1.0) / (float(ham_len) + distinct_term_count))\n",
    "\n",
    "# Open each file and predict. Note - prediction is embaressingly parallel and could\n",
    "#   be done effectively via Mapping. However, the assignment asks us to solve the problem using\n",
    "#   the provided pNaiveBayes.sh.\n",
    "accuracy = []\n",
    "for thisfile in filelist:\n",
    "    with open (thisfile, \"r\") as openfile:\n",
    "        for processed_email in openfile:\n",
    "            # Defaults\n",
    "            pred_spam = 0\n",
    "            spam_prediction = spam_prior\n",
    "            ham_prediction = ham_prior\n",
    "\n",
    "            # Read in tuples created by mapper\n",
    "            processed_email = processed_email.split(\" ||| \")\n",
    "            email_id = processed_email[1]\n",
    "            is_spam = int(processed_email[2])\n",
    "            count_dict = ast.literal_eval(processed_email[4])\n",
    "            \n",
    "            # Read in counts to use in prediction\n",
    "            for term in distinct_term_list:\n",
    "                # Calculate the probability for each class\n",
    "                spam_prediction += (word_prob_given_spam[term] * count_dict.get(term, 0))\n",
    "                ham_prediction += (word_prob_given_ham[term] * count_dict.get(term, 0))\n",
    "                \n",
    "            # Pick the higher probability\n",
    "            if spam_prediction > ham_prediction: \n",
    "                pred_spam = 1\n",
    "            \n",
    "            # Store accuracy in a list\n",
    "            accuracy.append(1*(pred_spam==is_spam))\n",
    "\n",
    "            # Print predictions to results file\n",
    "            print '{}\\t{}\\t{}'.format(email_id, is_spam, pred_spam)\n",
    "# Print accuracy\n",
    "sys.stderr.write(\"Accuracy = {:.2f}\\n\".format(float(sum(accuracy))/len(accuracy)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.96\r\n"
     ]
    }
   ],
   "source": [
    "# HW 1.5: Run on all words. \n",
    "def HW1_5():\n",
    "    # Run pNaiveBayes.sh\n",
    "    !./pNaiveBayes.sh 10 \"*\"\n",
    "            \n",
    "HW1_5()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '3', 'five']\n",
      "1 3 five\n"
     ]
    }
   ],
   "source": [
    "test = [\"1\",\"3\",\"five\"]\n",
    "print str(test)\n",
    "print \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "content = \"12313313\t1\tasdad13\t1\ta13\t0\ta\"\n",
    "print re.findall(\"\\t([0-1])\\t\",content)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12313313\n",
      "\t1\t\n",
      "asdad13\t1\ta13\t0\ta\n"
     ]
    }
   ],
   "source": [
    "content = \"12313313\t1\tasdad13\t1\ta13\t0\ta\"\n",
    "keyword = re.findall(\"\\t[0-1]\\t\",content)[0]\n",
    "email_id, is_spam, email_body = content.partition(keyword)\n",
    "print email_id\n",
    "print is_spam\n",
    "print email_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
